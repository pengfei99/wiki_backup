
<h1 class="sectionedit1" id="lesson1handling_structured_data">Lesson1: Handling structured data</h1>
<div class="level1">

<p>
In this Lesson, we will use sqoop to extract a database which is stored in a mysql server. Sqoop will inject this database into hive data warehouse. Then we will do some simple analysis.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Lesson1: Handling structured data&quot;,&quot;hid&quot;:&quot;lesson1handling_structured_data&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:1,&quot;range&quot;:&quot;1-238&quot;} -->
<h2 class="sectionedit2" id="connect_to_the_mysql_database">1. Connect to the mysql database</h2>
<div class="level2">
<pre class="code"># View database (retail_db) in the mysql database (pwd:hadoop)
mysql -u root -p retail_db

# check database tables 
show tables;

# show 10 distinct customers
select distinct * from customers limit 10;</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;1. Connect to the mysql database&quot;,&quot;hid&quot;:&quot;connect_to_the_mysql_database&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:2,&quot;range&quot;:&quot;239-502&quot;} -->
<h2 class="sectionedit3" id="use_sqoop_to_import_database">2. Use sqoop to import database</h2>
<div class="level2">

<p>
For more information about sqoop on import data, see <a href="/doku.php?id=employes:pengfei.liu:data_science:sqoop:import_data" class="wikilink1" title="employes:pengfei.liu:data_science:sqoop:import_data">Sqoop import data from DB</a>
</p>
<pre class="code"># 1. view the data in mysql server

# 2. create db in hive
$ create database retail_db

# 3. import data via sqoop
$ sqoop import-all-tables --connect jdbc:mysql://127.0.0.1:3306/retail_db --username=root -P --compression-codec=snappy --hive-overwrite --hive-import --hive-database retail_db 

# In our case, we can&#039;t use --direct, becasue mysqldump is not installed on the server. Note that with different DB, the --direct means different things. In mysql/mariadb, postgres, it means use mysqldump/pgdump to bypass jdbc. In oracle

# 4. login to beeline and view the imported data
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2. Use sqoop to import database&quot;,&quot;hid&quot;:&quot;use_sqoop_to_import_database&quot;,&quot;codeblockOffset&quot;:1,&quot;secid&quot;:3,&quot;range&quot;:&quot;503-1283&quot;} -->
<h2 class="sectionedit4" id="use_hive_to_analyse_data">3. Use hive to analyse data</h2>
<div class="level2">
<pre class="code"># 1. Top ten best seller
select c.category_name as category_name, count(order_item_quantity) as product_number
from order_items oi
inner join products p on oi.order_item_product_id = p.product_id
inner join categories c on c.category_id = p.product_category_id
group by c.category_name
order by product_number desc
limit 10;

# 2. top 10 revenue generating products, if you want more precision, you can change int to float
select p.product_name as product_name, r.revenue as revenue
from products p inner join
(select oi.order_item_product_id, sum(cast(oi.order_item_subtotal as int)) as revenue
from order_items oi inner join orders o
on oi.order_item_order_id = o.order_id
where o.order_status &lt;&gt; &#039;CANCELED&#039;
and o.order_status &lt;&gt; &#039;SUSPECTED_FRAUD&#039;
group by order_item_product_id) r
on p.product_id = r.order_item_product_id
order by r.revenue desc
limit 10;
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;3. Use hive to analyse data&quot;,&quot;hid&quot;:&quot;use_hive_to_analyse_data&quot;,&quot;codeblockOffset&quot;:2,&quot;secid&quot;:4,&quot;range&quot;:&quot;1284-2201&quot;} -->
<h2 class="sectionedit5" id="save_result_to_hive">4. Save result to hive</h2>
<div class="level2">
<pre class="code"># save the best seller result to a new table
CREATE TABLE retail_db.best_seller STORED AS ORC AS select c.category_name as category_name, count(order_item_quantity) as product_number
from order_items oi
inner join products p on oi.order_item_product_id = p.product_id
inner join categories c on c.category_id = p.product_category_id
group by c.category_name
order by product_number desc;

# save the revenue value generated by each products
CREATE TABLE retail_db.products_revenue STORED AS ORC AS select p.product_name as product_name, r.revenue as revenue
from products p inner join
(select oi.order_item_product_id, sum(cast(oi.order_item_subtotal as float)) as revenue
from order_items oi inner join orders o
on oi.order_item_order_id = o.order_id
where o.order_status &lt;&gt; &#039;CANCELED&#039;
and o.order_status &lt;&gt; &#039;SUSPECTED_FRAUD&#039;
group by order_item_product_id) r
on p.product_id = r.order_item_product_id
order by r.revenue desc</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;4. Save result to hive&quot;,&quot;hid&quot;:&quot;save_result_to_hive&quot;,&quot;codeblockOffset&quot;:3,&quot;secid&quot;:5,&quot;range&quot;:&quot;2202-3182&quot;} -->
<h2 class="sectionedit6" id="common_problems">Common problems</h2>
<div class="level2">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Common problems&quot;,&quot;hid&quot;:&quot;common_problems&quot;,&quot;codeblockOffset&quot;:4,&quot;secid&quot;:6,&quot;range&quot;:&quot;3183-3211&quot;} -->
<h3 class="sectionedit7" id="transactional_tables_with_parquet_format_are_not_supported_by_hive">1. Transactional tables with Parquet format are not supported by Hive</h3>
<div class="level3">

<p>
In HDP 3, managed Hive tables must be transactional (hive.strict.managed.tables=true). Transactional tables with Parquet format are not supported by Hive. Hive imports with –as-parquetfile must use external tables by specifying –external-table-dir.
</p>

<p>
Associated error message
</p>

<p>
Table db.table failed strict managed table checks due to the
following reason: Table is marked as a managed table but is not
transactional. 
Workaround
</p>

<p>
When using –hive-import with –as-parquetfile, users must also provide –external-table-dir with a fully qualified location of the table:
</p>
<pre class="code">sqoop import ... --hive-import
                 --as-parquetfile 
                 --external-table-dir hdfs:///path/to/table</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;1. Transactional tables with Parquet format are not supported by Hive&quot;,&quot;hid&quot;:&quot;transactional_tables_with_parquet_format_are_not_supported_by_hive&quot;,&quot;codeblockOffset&quot;:4,&quot;secid&quot;:7,&quot;range&quot;:&quot;3212-&quot;} -->