
<h1 class="sectionedit1" id="spark_read_and_write_parquet_in_file_system">Spark read and write parquet in file system</h1>
<div class="level1">

<p>
Apache Parquet is implemented using the record-shredding and assembly algorithm, which accommodates the complex data structures that can be used to store the data. The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:
</p>
<ul>
<li class="level1"><div class="li"> Column-wise compression is efficient and saves storage space</div>
</li>
<li class="level1"><div class="li"> Compression techniques specific to a type can be applied as the column values tend to be of the same type</div>
</li>
<li class="level1"><div class="li"> Queries that fetch specific column values need not read the entire row data thus improving performance</div>
</li>
<li class="level1"><div class="li"> Different encoding techniques can be applied to different columns</div>
</li>
</ul>

<p>
Moreover, Apache Parquet is implemented using the Apache Thrift framework which increases its flexibility; it can work with a number of programming languages like C++, Java, Python, PHP, etc.
</p>

<p>
As of August 2015, Parquet supports the big-data-processing frameworks including Apache Hive, Apache Drill, Apache Impala, Apache Crunch, Apache Pig, Cascading and Apache Spark.
</p>

</div>
<!-- EDIT1 SECTION "Spark read and write parquet in file system" [1-1089] -->
<h2 class="sectionedit2" id="write_data_to_parquet_file">Write data to parquet file</h2>
<div class="level2">

<p>
We re use the data example of <a href="/doku.php?id=employes:pengfei.liu:big_data:spark:spark_usecase:sf_client_satisfaction" class="wikilink1" title="employes:pengfei.liu:big_data:spark:spark_usecase:sf_client_satisfaction">Happy customers</a>
</p>

<p>
We suppose the name of dataframe is df.
</p>
<pre class="code">import spark.implicits._

# one way to do it
df.write.format(&quot;parquet&quot;).save(&quot;file:///tmp/happyClient.parquet&quot;)

#another way to do it
scala&gt; df.write.parquet(&quot;file:///tmp/happyClient.parquet&quot;)

#the content of happyClient.parquet
cd /tmp/happyClient.parquet
[hadoop@localhost happyClient.parquet]$ ls
part-00000-43ff1107-22f3-4de5-9d07-25f3a38a8cf7-c000.snappy.parquet  _SUCCESS
# part is actual data, _SUCCESS is the output of spark job</pre>

</div>
<!-- EDIT2 SECTION "Write data to parquet file" [1090-1750] -->
<h2 class="sectionedit3" id="read_data_from_parquet_file">Read data from parquet file</h2>
<div class="level2">

<p>
To read parquet file generated by spark, you only need to give the folder path.
</p>
<pre class="code">#open a new spark shell consol

scala&gt; import spark.implicits._
import spark.implicits._

#
scala&gt; val df= spark.read.parquet(&quot;file:///tmp/happyClient.parquet&quot;)
df: org.apache.spark.sql.DataFrame = [manager_name: string, client_name: string ... 4 more fields]

scala&gt; df.show
+---------------+-------------+-------------+----------+-------------+------------------+
|   manager_name|  client_name|client_gender|client_age|response_time|satisfaction_level|
+---------------+-------------+-------------+----------+-------------+------------------+
|    Arjun Kumar|  Rehan Nigam|         male|        30|          4.0|               0.5|
|     Kabir Vish| Abhinav Neel|         male|        28|         12.0|               0.1|
|    Arjun Kumar|    Sam Mehta|         male|        27|          3.0|               0.7|
|    Arjun Kumar|    Ira Pawan|       female|        40|          2.5|               0.6|
|Rohit Srivastav| Vihaan Sahni|         male|        38|          6.0|               0.5|
|     Kabir Vish|Daivik Saxena|         male|        45|         16.0|               0.2|
|Rohit Srivastav|   Lera Uddin|       female|        20|          8.0|               0.4|
|Rohit Srivastav|   Aaran Puri|         male|        34|          7.5|               0.3|
|     Kabir Vish|   Rudra Mati|         male|        50|         20.0|               0.1|
+---------------+-------------+-------------+----------+-------------+------------------+
</pre>

</div>
<!-- EDIT3 SECTION "Read data from parquet file" [1751-] -->