
<h1 class="sectionedit1" id="lesson02installing_and_configuring_kafka">Lesson02: Installing and configuring kafka</h1>
<div class="level1">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Lesson02: Installing and configuring kafka&quot;,&quot;hid&quot;:&quot;lesson02installing_and_configuring_kafka&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:1,&quot;range&quot;:&quot;2-61&quot;} -->
<h2 class="sectionedit2" id="prerequise">2.0 Prerequise</h2>
<div class="level2">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.0 Prerequise&quot;,&quot;hid&quot;:&quot;prerequise&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:2,&quot;range&quot;:&quot;62-89&quot;} -->
<h3 class="sectionedit3" id="install_jdk">2.0.1 Install JDK</h3>
<div class="level3">

<p>
Kafka needs JDK or JRE to run. So the first step is to install jdk.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.0.1 Install JDK&quot;,&quot;hid&quot;:&quot;install_jdk&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:3,&quot;range&quot;:&quot;90-187&quot;} -->
<h3 class="sectionedit4" id="install_zookeeper">2.0.2 Install zookeeper</h3>
<div class="level3">

<p>
<strong>A critical dependency of Apache Kafka is Apache Zookeeper</strong>, which is a distributed configuration and synchronization service. Kafka uses ZooKeeper for:
</p>
<ul>
<li class="level1"><div class="li"> leadership election of Kafka Broker to elect <strong>Controller</strong>: The first arrived broker creates /kafka/controller znode in zk. The other broker will fail because it already exists. </div>
</li>
<li class="level1"><div class="li"> partition replication leader/follower election</div>
</li>
<li class="level1"><div class="li"> discover new coming brokers</div>
</li>
<li class="level1"><div class="li"> stores basic metadata in Zookeeper such as information about topics(topic-partition pairs), brokers, consumer offsets (queue readers) and so on.</div>
</li>
<li class="level1"><div class="li"> ETc.</div>
</li>
</ul>

<p>
Since all the critical information is stored in the Zookeeper and it normally replicates this data across its ensemble, failure of Kafka broker / Zookeeper does not affect the state of the Kafka cluster. Kafka will restore the state, once the Zookeeper restarts. This gives zero downtime for Kafka. The leader election between the Kafka broker is also done by using Zookeeper in the event of leader failure.
</p>

<p>
To learn more about ZooKeeper: <a href="/doku.php?id=employes:pengfei.liu:data_science:zookeeper:start" class="wikilink1" title="employes:pengfei.liu:data_science:zookeeper:start">ZooKeeper docs</a>
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.0.2 Install zookeeper&quot;,&quot;hid&quot;:&quot;install_zookeeper&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:4,&quot;range&quot;:&quot;188-1313&quot;} -->
<h3 class="sectionedit5" id="hardware_requirement">2.0.3 Hardware requirement</h3>
<div class="level3">

<p>
At least 4GB of RAM on the server. Installations without this amount of RAM may cause the Kafka service to fail, with the Java virtual machine (JVM) throwing an “Out Of Memory” exception during startup.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.0.3 Hardware requirement&quot;,&quot;hid&quot;:&quot;hardware_requirement&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:5,&quot;range&quot;:&quot;1314-1560&quot;} -->
<h2 class="sectionedit6" id="creating_a_user_for_kafka">2.1 Creating a User for Kafka</h2>
<div class="level2">
<pre class="code"># create a system user Kafka with a home dir
useradd -r kafka -m

# add Kafka to sudoer group
usermod -aG wheel kafka

# login as kafka
su -l kafka</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.1 Creating a User for Kafka&quot;,&quot;hid&quot;:&quot;creating_a_user_for_kafka&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:6,&quot;range&quot;:&quot;1561-1765&quot;} -->
<h2 class="sectionedit7" id="downloading_and_extracting_the_kafka_binaries">2.2 Downloading and Extracting the Kafka Binaries</h2>
<div class="level2">

<p>
Download the kafka binary from <a href="http://kafka.apache.org/downloads.html" class="urlextern" title="http://kafka.apache.org/downloads.html" rel="nofollow">http://kafka.apache.org/downloads.html</a>
</p>

<p>
Note that Kafka uses Scala, and it&#039;s binary is compiled based on certain version of Scala. For example, kafka_2.11-1.0.0 means this Kafka version 1.0.0 is compiled based on scala 2.11; kafka_2.12-2.5.0 means this kafka of version 2.5.0 is compiled based on scala 2.12. So before you chose your kafka binary version, you need to check the scala version which runs on your server.
</p>

<p>
Then place the binary in your server. I put it under “/opt/kafka”
</p>
<pre class="code">mkdir /opt/kafka

cd /opt/kafka

# extract the binary
tar -xzvf kafka_2.12-2.5.0.tgz 

# change the name
mv kafka_2.12-2.5.0 kafka-2.5.0

# check the result
[root@localhost kafka-2.5.0]# pwd
/opt/kafka/kafka-2.5.0</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.2 Downloading and Extracting the Kafka Binaries&quot;,&quot;hid&quot;:&quot;downloading_and_extracting_the_kafka_binaries&quot;,&quot;codeblockOffset&quot;:1,&quot;secid&quot;:7,&quot;range&quot;:&quot;1766-2576&quot;} -->
<h2 class="sectionedit8" id="configuring_the_kafka_server">2.3 Configuring the Kafka Server</h2>
<div class="level2">

<p>
The main config file of a Kafka broker is on the <strong>server.properties</strong> file. The default config is enough to run a standalone Kafka broker as a test server. But I still want to highlight some key configuration attributes.
</p>
<pre class="code">【broker.id】
每个broker都必须自己设置的一个唯一id，可以在0~255之间. A good guideline is to set this value to something intrinsic to the host. For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.), that is a good choice for the broker.id value.

【log.dirs】
这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。ps：多个目录用英文逗号分隔. If more than one path is specified(e.g. log.dirs=path1,path2), the broker will store partitions on them in a “least-used” fashion with one partition’s log segments stored within the same path. Note that the broker will place a new partition in the path that has the least number of partitions currently stored in it, not the least amount of disk space used in the following situations

【zookeeper.connect】
连接kafka底层的zookeeper集群的. The default configuration is localhost:2181.

【Listeners】
broker监听客户端发起请求的ip,端口号，默认是9092. It will get the value returned from ** java.net.InetAddress.getCanonicalHostName() if not configured**. The port number can be set to any available port by changing the port configuration parameter. Keep in mind that if a port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.
</pre>
<pre class="code"># open the config file
cd /opt/kafka/kafka_2.11-1.0.0/config
vim server.properties</pre>

<p>
The following is an example of minimum configuration
</p>
<pre class="code file serverproperties">#in a single server with default zookeeper
broker.id=0
log.dirs=/tmp/kafka-logs
zookeeper.connect=localhost:2181
&nbsp;
#in a cluster environment with multiple zookeeper instances
zookeeper.connect=hadoop-nn.pengfei.org:2181,hadoop-dn1.pengfei.org:2181,hadoop-dn2.pengfei.org:2181
broker.id=1 (2,3 in other two machines)
log.dirs=fs_on_disk1,fs_on_disk2</pre>

<p>
The above config is the minimun config to run kafka, you may need to dig more.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.3 Configuring the Kafka Server&quot;,&quot;hid&quot;:&quot;configuring_the_kafka_server&quot;,&quot;codeblockOffset&quot;:2,&quot;secid&quot;:8,&quot;range&quot;:&quot;2577-5149&quot;} -->
<h2 class="sectionedit9" id="creating_systemd_unit_files_and_starting_the_kafka_server">2.4 Creating Systemd Unit Files and Starting the Kafka Server</h2>
<div class="level2">

<p>
We need to create the systemd service file for Kafka. As Kafka requires ZooKeeper to manage its cluster state and configuration. We need to make sure we have a systemd service file for Zookeeper first. Suppose we have it in /etc/systemd/system/zk.service
</p>

<p>
Our Kafka systemd file looks like this:
</p>
<pre class="code">[Unit]
Requires=zk.service
After=zk.service

[Service]
Type=forking
WorkingDirectory=/opt/kafka/kafka-2.5.0
User=kafka
Group=kafka
ExecStart=/opt/kafka/kafka-2.5.0/bin/kafka-server-start.sh /opt/kafka/kafka-2.5.0/config/server.properties &gt; /var/log/kafka/kafka.log 2&gt;&amp;1
ExecStop=/opt/kafka/kafka-2.5.0/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target</pre>

<p>
The Unit part will ensure that zookeeper gets started automatically when the kafa service starts.
</p>

<p>
Run the daemon
</p>
<pre class="code">sudo systemctl start/stop/status kafka

# run daemon at system startup
sudo systemctl enable kafka</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.4 Creating Systemd Unit Files and Starting the Kafka Server&quot;,&quot;hid&quot;:&quot;creating_systemd_unit_files_and_starting_the_kafka_server&quot;,&quot;codeblockOffset&quot;:5,&quot;secid&quot;:9,&quot;range&quot;:&quot;5150-6149&quot;} -->
<h2 class="sectionedit10" id="testing_the_installation">2.5 Testing the Installation</h2>
<div class="level2">

<p>
To test the kafka server (with one broker). Make sure the zookeeper is running before you run this command.
</p>
<pre class="code">sh bin/kafka-server-start.sh config/server.properties</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.5 Testing the Installation&quot;,&quot;hid&quot;:&quot;testing_the_installation&quot;,&quot;codeblockOffset&quot;:7,&quot;secid&quot;:10,&quot;range&quot;:&quot;6150-6369&quot;} -->
<h3 class="sectionedit11" id="create_a_topic">2.5.1 Create a topic</h3>
<div class="level3">

<p>
Create a topic with one partition and three replication
</p>
<pre class="code">#create a topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper hadoop-nn.bioaster.org --replication-factor 3 --partitions 1 --topic test-topic
Created topic &quot;test-topic&quot;.

#check status of a topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1</pre>

<p>
Create a topic with three partitions and three replication
</p>
<pre class="code">[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic Hello-Kafka

Created topic &quot;Hello-Kafka&quot;.

#get status of the topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic Hello-Kafka
Topic:Hello-Kafka	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: Hello-Kafka	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1
	Topic: Hello-Kafka	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
	Topic: Hello-Kafka	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2
</pre>

<p>
List all topic in one broker
</p>
<pre class="code">[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.5.1 Create a topic&quot;,&quot;hid&quot;:&quot;create_a_topic&quot;,&quot;codeblockOffset&quot;:8,&quot;secid&quot;:11,&quot;range&quot;:&quot;6370-7767&quot;} -->
<h3 class="sectionedit12" id="start_producer_to_send_messages">2.5.2 Start producer to send messages</h3>
<div class="level3">
<pre class="code"># In the broker-list, you could put one or more kafka brokers
# In this example, we use only one, the name of the broker is defined in server.properties  
[hadoop@CCLinDataWHD01 bin]$ sh kafka-console-producer.sh --broker-list hadoop-nn.bioaster.org:9092 --topic Hello-Kafka
#now you are in producer consol, all the text you enter below will be published in topic Hello-Kafka
&gt;hello
&gt;my first message
&gt;my second message
&gt;my third message
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.5.2 Start producer to send messages&quot;,&quot;hid&quot;:&quot;start_producer_to_send_messages&quot;,&quot;codeblockOffset&quot;:11,&quot;secid&quot;:12,&quot;range&quot;:&quot;7768-8270&quot;} -->
<h3 class="sectionedit13" id="start_two_consumers_to_receive_messages">2.5.3 Start two consumers to receive messages</h3>
<div class="level3">
<pre class="code">###in server hadoop-dn1.bioaster.org
hadoop@CCLinDataWHD02 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my first message
my second message
my third message
my fouth message
my fifth message


###in server hadoop-dn2.bioaster.org
[hadoop@CCLinDataWHD03-0 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my third message
my first message
my second message
my fouth message
my fifth message


##You could notice, if your consumer connecter after many message has been published
##the order may not be correct. But the latest message will be correct
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.5.3 Start two consumers to receive messages&quot;,&quot;hid&quot;:&quot;start_two_consumers_to_receive_messages&quot;,&quot;codeblockOffset&quot;:12,&quot;secid&quot;:13,&quot;range&quot;:&quot;8271-9403&quot;} -->
<h3 class="sectionedit14" id="close_the_test">2.5.4 Close the test</h3>
<div class="level3">

<p>
When you are done testing, press CTRL+C to stop the producer and consumer scripts.
</p>

<p>
To delete the topic use the following command
</p>
<pre class="code">[root@localhost kafka-2.5.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic pengfeiTest
Topic pengfeiTest is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.5.4 Close the test&quot;,&quot;hid&quot;:&quot;close_the_test&quot;,&quot;codeblockOffset&quot;:13,&quot;secid&quot;:14,&quot;range&quot;:&quot;9404-9804&quot;} -->
<h2 class="sectionedit15" id="setting_up_a_multi-node_cluster">2.6 Setting Up a Multi-Node Cluster</h2>
<div class="level2">

<p>
There are three possible scenarios:
</p>
<ul>
<li class="level1"><div class="li"> Multi brokers on single servers</div>
</li>
<li class="level1"><div class="li"> Single brokers on multi-servers</div>
</li>
<li class="level1"><div class="li"> Multi brokers on multi-servers</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.6 Setting Up a Multi-Node Cluster&quot;,&quot;hid&quot;:&quot;setting_up_a_multi-node_cluster&quot;,&quot;codeblockOffset&quot;:14,&quot;secid&quot;:15,&quot;range&quot;:&quot;9805-9997&quot;} -->
<h3 class="sectionedit16" id="multi_brokers_on_single_servers">2.6.1 Multi brokers on single servers</h3>
<div class="level3">

<p>
For setting up multiple brokers on a single node, different server property files are required for each broker. Each property file will define unique, different values for the following properties:
</p>
<ul>
<li class="level1"><div class="li"> broker.id</div>
</li>
<li class="level1"><div class="li"> port</div>
</li>
<li class="level1"><div class="li"> log.dir</div>
</li>
</ul>

<p>
For example, if we want to run three brokers on a single server, we need to create three config files server1.properties, server2.properties, server3.properties. 
</p>
<pre class="code file server1properties">broker.id=1
listeners = PLAINTEXT://your.host.name:9093
log.dir=/tmp/kafka-logs-1</pre>
<pre class="code file server2properties">broker.id=2
listeners = PLAINTEXT://your.host.name:9094
log.dir=/tmp/kafka-logs-2</pre>
<pre class="code file server3properties">broker.id=3
listeners = PLAINTEXT://your.host.name:9095
log.dir=/tmp/kafka-logs-3</pre>

<p>
We can run the three instances of the broker with the following command
</p>
<pre class="code">#run broker1
bin/kafka-server-start.sh config/server1.properties

#run broker2
bin/kafka-server-start.sh config/server1.properties

#run broker3
bin/kafka-server-start.sh config/server1.properties</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.6.1 Multi brokers on single servers&quot;,&quot;hid&quot;:&quot;multi_brokers_on_single_servers&quot;,&quot;codeblockOffset&quot;:14,&quot;secid&quot;:16,&quot;range&quot;:&quot;9998-11080&quot;} -->
<h3 class="sectionedit17" id="single_broker_on_multi_nodes">2.6.2 Single broker on multi nodes</h3>
<div class="level3">

<p>
This is very simple, all these brokers can have the exact same configuration except the <strong>broker.id</strong>.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.6.2 Single broker on multi nodes&quot;,&quot;hid&quot;:&quot;single_broker_on_multi_nodes&quot;,&quot;codeblockOffset&quot;:18,&quot;secid&quot;:17,&quot;range&quot;:&quot;11081-11230&quot;} -->
<h3 class="sectionedit18" id="zookeeper_configuration">Zookeeper configuration</h3>
<div class="level3">

<p>
If you want your Kafka broker to connect to multiple zookeeper servers. In zookeeper.connect, you can use comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.
</p>

<p>
For example
</p>
<pre class="code">//Kafka uses three zk instances
zookeeper.connect=zk1.pengfei.org:2181,zk2.pengfei.org:2181,zk3.pengfei.org:2181</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Zookeeper configuration&quot;,&quot;hid&quot;:&quot;zookeeper_configuration&quot;,&quot;codeblockOffset&quot;:18,&quot;secid&quot;:18,&quot;range&quot;:&quot;11231-11611&quot;} -->
<h2 class="sectionedit19" id="restricting_the_kafka_user">2.7 Restricting the Kafka User</h2>
<div class="level2">
<pre class="code"># Remove the kafka user from the sudo group:
sudo gpasswd -d kafka wheel

# To further improve your Kafka server’s security, lock the kafka user’s password using the passwd command. This makes sure that nobody can directly log into the server using this account:
sudo passwd kafka -l

# At this point, only root or a sudo user can log in as kafka by typing in the following command:
sudo su - Kafka

# In the future, if you want to unlock it, use passwd with the -u option:
sudo passwd kafka -u

</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.7 Restricting the Kafka User&quot;,&quot;hid&quot;:&quot;restricting_the_kafka_user&quot;,&quot;codeblockOffset&quot;:19,&quot;secid&quot;:19,&quot;range&quot;:&quot;11612-12172&quot;} -->
<h2 class="sectionedit20" id="basic_topic_operations_optional">2.8 Basic topic operations (Optional)</h2>
<div class="level2">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.8 Basic topic operations (Optional)&quot;,&quot;hid&quot;:&quot;basic_topic_operations_optional&quot;,&quot;codeblockOffset&quot;:20,&quot;secid&quot;:20,&quot;range&quot;:&quot;12173-12223&quot;} -->
<h3 class="sectionedit21" id="list_all_topics_and_show_details_of_a_topic">2.8.1 List all topics and show details of a topic</h3>
<div class="level3">
<pre class="code">
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic

#show detail of topic test-topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3

#we know that it has one partition and 3 replicas</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.8.1 List all topics and show details of a topic&quot;,&quot;hid&quot;:&quot;list_all_topics_and_show_details_of_a_topic&quot;,&quot;codeblockOffset&quot;:20,&quot;secid&quot;:21,&quot;range&quot;:&quot;12224-12755&quot;} -->
<h3 class="sectionedit22" id="change_the_partition_number_of_a_topic">2.8.2 Change the partition number of a topic</h3>
<div class="level3">
<pre class="code">#Now we want to change the partition from one to two
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --alter --topic test-topic --partitions 2
WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected
Adding partitions succeeded!

[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:2	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: test-topic	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.8.2 Change the partition number of a topic&quot;,&quot;hid&quot;:&quot;change_the_partition_number_of_a_topic&quot;,&quot;codeblockOffset&quot;:21,&quot;secid&quot;:22,&quot;range&quot;:&quot;12756-13489&quot;} -->
<h3 class="sectionedit23" id="delet_a_topic">2.8.3 Delet a topic</h3>
<div class="level3">
<pre class="code">[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --delete --topic test-topic
Topic test-topic is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.8.3 Delet a topic&quot;,&quot;hid&quot;:&quot;delet_a_topic&quot;,&quot;codeblockOffset&quot;:22,&quot;secid&quot;:23,&quot;range&quot;:&quot;13490-13874&quot;} -->
<h2 class="sectionedit24" id="kafka_runs_starts_and_runs_the_zookeeper_optional">2.9 Kafka runs starts and runs the zookeeper (Optional)</h2>
<div class="level2">

<p>
if your zookeeper cluster already running or controlled by zookeeper, you don&#039;t need to chage the config of /opt/kafka/kafka_2.11-1.0.0/config/zookeeper.properties
</p>

<p>
if you want to use kafka to control the zookeeper daemon, you need to edit the zookeeper.properties
</p>

<p>
Here is an example
</p>
<pre class="code"># the directory where the snapshot is stored.
dataDir=/opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0

# The number of milliseconds of each tick
tickTime=2000

# The number of ticks that the initial synchronization phase can take
initLimit=10

# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5

#zoo servers
server.1=hadoop-nn.pengfei.org:2888:3888
server.2=hadoop-dn1.pengfei.org:2888:3888
server.3=hadoop-dn2.pengfei.org:2888:3888
</pre>

<p>
It&#039;s the same as the zoo.cfg in zookeeper
</p>
<pre class="code"># The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
#initLimit=10
initLimit=5
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
#syncLimit=5
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to &quot;0&quot; to disable auto purge feature
#autopurge.purgeInterval=1

server.1=hadoop-nn.pengfei.org:2888:3888
server.2=hadoop-dn1.pengfei.org:2888:3888
server.3=hadoop-dn2.pengfei.org:2888:3888
</pre>

<p>
To run the zookeeper daemon in kafak
</p>
<pre class="code">cd /opt/kafka/kafka_2.11-1.0.0
sh bin/zookeeper-server-start.sh config/zookeeper.properties</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2.9 Kafka runs starts and runs the zookeeper (Optional)&quot;,&quot;hid&quot;:&quot;kafka_runs_starts_and_runs_the_zookeeper_optional&quot;,&quot;codeblockOffset&quot;:23,&quot;secid&quot;:24,&quot;range&quot;:&quot;13875-&quot;} -->