a:83:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:4:"Yarn";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:21;}i:4;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:24:"Hadoop Yarn architecture";i:1;i:2;i:2;i:21;}i:2;i:21;}i:5;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:21;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:21;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:182:"Apache Yarn Framework consists of a master daemon known as “Resource Manager”, slave daemon called node manager (one per slave node) and Application Master (one per application).";}i:2;i:59;}i:8;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:241;}i:9;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:241;}i:10;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:52:":employes:pengfei.liu:big_data:yarn_architecture.gif";i:1;s:0:"";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:243;}i:11;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:304;}i:12;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:306;}i:13;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:16:"Resource manager";i:1;i:3;i:2;i:306;}i:2;i:306;}i:14;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:306;}i:15;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:306;}i:16;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:265:"It is the master daemon of Yarn. RM manages the global assignments of resources (CPU and memory) among all the applications. It arbitrates system resources between competing applications. follow Resource Manager guide to learn Yarn Resource manager in great detail.";}i:2;i:334;}i:17;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:599;}i:18;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:599;}i:19;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:40:"Resource Manager has two Main components";}i:2;i:601;}i:20;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:642;}i:21;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:642;}i:22;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:642;}i:23;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:642;}i:24;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" Scheduler ";}i:2;i:646;}i:25;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:657;}i:26;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:306:" The scheduler is responsible for allocating the resources to the running application. The scheduler is pure scheduler it means that it performs no monitoring no tracking for the application and even doesn’t guarantees about restarting failed tasks either due to application failure or hardware failures.";}i:2;i:659;}i:27;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:965;}i:28;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:965;}i:29;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:965;}i:30;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:965;}i:31;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:" Application manager ";}i:2;i:969;}i:32;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:990;}i:33;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:190:" It manages running Application Masters in the cluster, i.e., it is responsible for starting application masters and for monitoring and restarting them on different nodes in case of failures";}i:2;i:992;}i:34;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1182;}i:35;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1182;}i:36;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:1182;}i:37;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1184;}i:38;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:16:"Node manager(NM)";i:1;i:3;i:2;i:1184;}i:2;i:1184;}i:39;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1184;}i:40;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1184;}i:41;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:541:"It is the slave daemon of Yarn. NM is responsible for containers monitoring their resource usage and reporting the same to the ResourceManager. Manage the user process on that machine. Yarn NodeManager also tracks the health of the node on which it is running. The design also allows plugging long-running auxiliary services to the NM; these are application-specific services, specified as part of the configurations and loaded by the NM during startup. A shuffle is a typical auxiliary service by the NMs for MapReduce applications on YARN.";}i:2;i:1212;}i:42;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1753;}i:43;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1755;}i:44;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:23:"Application Master (AM)";i:1;i:3;i:2;i:1755;}i:2;i:1755;}i:45;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1755;}i:46;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1755;}i:47;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:166:"One application master runs per application. It negotiates resources from the resource manager and works with the node manager. It Manages the application life cycle.";}i:2;i:1790;}i:48;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1956;}i:49;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1956;}i:50;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:139:"The AM acquires containers from the RM’s Scheduler before contacting the corresponding NMs to start the application’s individual tasks.";}i:2;i:1958;}i:51;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2097;}i:52;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2100;}i:53;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:24:"Resource Manager Restart";i:1;i:2;i:2;i:2100;}i:2;i:2100;}i:54;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:2100;}i:55;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2100;}i:56;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:194:"Resource Manager is the central authority that manages resources and schedules applications running on YARN. Hence, it is potentially an SPOF (single point of failure) in an Apache YARN cluster.";}i:2;i:2138;}i:57;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2332;}i:58;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2332;}i:59;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:52:"There are two types of restart for Resource Manager:";}i:2;i:2334;}i:60;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2387;}i:61;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2387;}i:62;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2387;}i:63;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2387;}i:64;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:2391;}i:65;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:2392;}i:66;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:"Non-work-preserving RM restart";}i:2;i:2394;}i:67;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:2424;}i:68;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:523:"  This restart enhances RM to persist application/attempt state in a pluggable state-store. Resource Manager will reload the same info from state-store on the restart and re-kick the previously running apps. Users does not need to re-submit the applications. Node manager and clients during down time of RM will keep polling RM until RM comes up, when RM comes up, it will send a re-sync command to all the NM and AM it was talking to via heartbeats. The NMs will kill all its manager’s containers and re-register with RM";}i:2;i:2426;}i:69;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2949;}i:70;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2949;}i:71;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2949;}i:72;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2949;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:2953;}i:74;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:2954;}i:75;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:26:"Work-preserving RM restart";}i:2;i:2956;}i:76;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:2982;}i:77;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:661:"  This focuses on reconstructing the running state of RM by combining the container status from Node Managers and container requests from Application Masters on restart. The key difference from Non-work-preserving RM restart is that already running apps will not be stopped after master restarts, so applications will not lose its processed data because of RM/master outage. RM recovers its running state by taking advantage of container status which is sent from all the node managers. NM will not kill the containers when it re-syncs with the restarted RM. It continues managing the containers and sends the container status across to RM when it re-registers.";}i:2;i:2984;}i:78;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:3645;}i:79;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:3645;}i:80;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:3645;}i:81;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3647;}i:82;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:3647;}}