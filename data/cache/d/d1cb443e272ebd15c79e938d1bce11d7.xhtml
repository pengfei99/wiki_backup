
<h1 class="sectionedit1" id="kafka的生产集群部署">Kafka的生产集群部署</h1>
<div class="level1">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Kafka\u7684\u751f\u4ea7\u96c6\u7fa4\u90e8\u7f72&quot;,&quot;hid&quot;:&quot;kafka\u7684\u751f\u4ea7\u96c6\u7fa4\u90e8\u7f72&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:1,&quot;range&quot;:&quot;1-42&quot;} -->
<h2 class="sectionedit2" id="方案背景">1 方案背景</h2>
<div class="level2">

<p>
假设每天集群需要承载10亿数据。一天24小时，晚上12点到凌晨8点几乎没多少数据。 使用二八法则估计，也就是80%的数据（8亿）会在16个小时涌入，而且8亿的80%的数据（6.4亿）会在这16个小时的20%时间（3小时）涌入。
</p>

<p>
QPS计算公式：640000000 ÷ (3x60x60) = 60000，也就是说高峰期的时候Kafka集群要扛住每秒6万的并发。
</p>

<p>
磁盘空间计算，每天10亿数据，每条50kb，也就是46T的数据。保存2个副本（在上一篇中也提到过其实两个副本会比较好，因为follower需要去leader那里同步数据，同步数据的过程需要耗费网络，而且需要磁盘空间，但是这个需要根据实际情况考虑），46 * 2 = 92T，保留最近3天的数据。故需要 92 * 3 = 276T
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;1 \u65b9\u6848\u80cc\u666f&quot;,&quot;hid&quot;:&quot;\u65b9\u6848\u80cc\u666f&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:2,&quot;range&quot;:&quot;43-875&quot;} -->
<h2 class="sectionedit3" id="qps方面">2 QPS方面</h2>
<div class="level2">

<p>
部署Kafka，Hadoop，MySQL···等核心分布式系统，一般建议直接采用物理机，抛弃使用一些低配置的虚拟机的想法。
</p>

<p>
高并发这个东西，不可能是说，你需要支撑6万QPS，你的集群就刚好把这6万并发卡的死死的。某一天出一些活动让数据量疯狂上涨，那整个集群就会垮掉。但是，假如说你只要支撑6w QPS，单台物理机本身就能扛住4~5万的并发。所以这时2台物理机绝对绝对够了。
</p>

<p>
通常是建议，预算充足，尽量是让高峰QPS控制在集群能承载的总QPS的30%左右（也就是集群的处理能力是高峰期的3~4倍这个样子），所以我们搭建的kafka集群能承载的总QPS为20万~30万才是安全的。所以大体上来说，需要5~7台物理机来部署，基本上就很安全了，每台物理机要求吞吐量在每秒4~5万条数据就可以了，物理机的配置和性能也不需要特别高。
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;2 QPS\u65b9\u9762&quot;,&quot;hid&quot;:&quot;qps\u65b9\u9762&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:3,&quot;range&quot;:&quot;876-1852&quot;} -->
<h2 class="sectionedit4" id="磁盘方面">3 磁盘方面</h2>
<div class="level2">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;3 \u78c1\u76d8\u65b9\u9762&quot;,&quot;hid&quot;:&quot;\u78c1\u76d8\u65b9\u9762&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:4,&quot;range&quot;:&quot;1853-1880&quot;} -->
<h3 class="sectionedit5" id="磁盘数量">3.1 磁盘数量</h3>
<div class="level3">

<p>
需要5台物理机的情况，需要存储276T的数据，平均下来差不多一台56T的数据。这个具体看磁盘数和盘的大小
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;3.1 \u78c1\u76d8\u6570\u91cf&quot;,&quot;hid&quot;:&quot;\u78c1\u76d8\u6570\u91cf&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:5,&quot;range&quot;:&quot;1881-2050&quot;} -->
<h3 class="sectionedit6" id="sas还是ssd">3.2 SAS还是SSD</h3>
<div class="level3">

<p>
现在我们需要考虑一个问题：是需要SSD固态硬盘，还是普通机械硬盘？
SSD固态硬盘，比机械硬盘要快，那么到底是快在哪里呢？其实SSD的快主要是快在磁盘随机读写，就要对磁盘上的随机位置来读写的时候，SSD比机械硬盘要快。比如说MySQL这种就应该使用SSD了（MySQL需要随机读写）。比如说我们在规划和部署线上系统的MySQL集群的时候，一般来说必须用SSD，性能可以提高很多，这样MySQL可以承载的并发请求量也会高很多，而且SQL语句执行的性能也会提高很多。
</p>

<p>
<strong>因为写磁盘的时候kafka是顺序写的。机械硬盘顺序写的性能机会跟内存读写的性能是差不多的，所以对于Kafka集群来说其实使用机械硬盘就可以了。</strong>
</p>

<p>
如果是需要自己创业或者是在公司成本不足的情况下，经费是能够缩减就尽量缩减的。
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;3.2 SAS\u8fd8\u662fSSD&quot;,&quot;hid&quot;:&quot;sas\u8fd8\u662fssd&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:6,&quot;range&quot;:&quot;2051-3004&quot;} -->
<h2 class="sectionedit7" id="内存角度">4 内存角度</h2>
<div class="level2">

<p>
JVM非常怕出现full gc的情况。kafka自身的jvm是用不了过多堆内存的，因为kafka设计就是规避掉用jvm对象来保存数据，避免频繁full gc导致的问题，所以一般kafka自身的jvm堆内存，分配个10G左右就够了，剩下的内存全部留给os cache。
</p>

<p>
那服务器需要多少内存呢。我们估算一下，大概有100个topic，所以要保证有100个topic的<strong>leader partition的数据在操作系统的内存里</strong>。100个topic，一个topic有5个partition。那么总共会有500个partition。每个partition的大小是1G（在上一篇中的日志分段存储中规定了.log文件不能超过1个G），我们有2个副本，也就是说要把100个topic的leader partition数据都驻留在内存里需要1000G的内存。
</p>

<p>
我们现在有5台服务器，所以平均下来每天服务器需要200G的内存，但是其实partition的数据我们没必要所有的都要驻留在内存里面，只需要25%的数据在内存就行，200G * 0.25 = 50G就可以了（因为在集群中的生产者和消费者几乎也算是实时的，基本不会出现消息积压太多的情况）。所以一共需要60G（附带上刚刚的10G Kafka服务）的内存，故我们可以挑选64G内存的服务器也行，大不了partition的数据再少一点在内存，当然如果能够提供128G内存那就更好。
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;4 \u5185\u5b58\u89d2\u5ea6&quot;,&quot;hid&quot;:&quot;\u5185\u5b58\u89d2\u5ea6&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:7,&quot;range&quot;:&quot;3005-4403&quot;} -->
<h2 class="sectionedit8" id="cpu_core">5 CPU core</h2>
<div class="level2">

<p>
CPU规划，主要是看你的这个进程里会有多少个线程，线程主要是依托多核CPU来执行的，如果你的线程特别多，但是CPU核很少，就会导致你的CPU负载很高，会导致整体工作线程执行的效率不太高，上一篇的Kafka的网络设计中讲过Kafka的Broker的模型。acceptor线程负责去接入客户端的连接请求，但是他接入了之后其实就会把连接分配给多个processor，默认是3个，但是一般生产环境建议大家还是多加几个，整体可以提升kafka的吞吐量比如说你可以增加到6个，或者是9个。另外就是负责处理请求的线程，是一个线程池，默认是8个线程，在生产集群里，建议大家可以把这块的线程数量稍微多加个2倍~3倍，其实都正常，比如说搞个16个工作线程，24个工作线程。
</p>

<p>
后台会有很多的其他的一些线程，比如说定期清理7天前数据的线程，Controller负责感知和管控整个集群的线程，副本同步拉取数据的线程，这样算下来每个broker起码会有上百个线程。根据经验4个cpu core，一般来说几十个线程，在高峰期CPU几乎都快打满了。8个cpu core，也就能够比较宽裕的支撑几十个线程繁忙的工作。所以Kafka的服务器一般是建议16核，基本上可以hold住一两百线程的工作。当然如果可以给到32 cpu core那就最好不过了。
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;5 CPU core&quot;,&quot;hid&quot;:&quot;cpu_core&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:8,&quot;range&quot;:&quot;4404-5859&quot;} -->
<h2 class="sectionedit9" id="网卡">6 网卡</h2>
<div class="level2">

<p>
现在的网基本就是千兆网卡（1GB / s），还有万兆网卡（10GB / s）。kafka集群之间，broker和broker之间是会做数据同步的，因为leader要同步数据到follower上去，他们是在不同的broker机器上的，broker机器之间会进行频繁的数据同步，传输大量的数据。那每秒两台broker机器之间大概会传输多大的数据量？
</p>

<p>
高峰期每秒大概会涌入6万条数据，约每天处理10000个请求，每个请求50kb，故每秒约进来488M数据，我们还有副本同步数据，故高峰期的时候需要488M * 2 = 976M/s的网络带宽，所以在高峰期的时候，使用千兆带宽，网络还是非常有压力的。
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;6 \u7f51\u5361&quot;,&quot;hid&quot;:&quot;\u7f51\u5361&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:9,&quot;range&quot;:&quot;5860-6580&quot;} -->
<h2 class="sectionedit10" id="choosing_file_system">7 Choosing file system</h2>
<div class="level2">

<p>
There are many different filesystems available, but the most common choices for local filesystems are either EXT4 (fourth extended file system) or Extents File System (XFS).
</p>

<p>
XFS outperforms EXT4 for most workloads with minimal tuning required. EXT4 can perform well, but it requires using tuning parameters that are considered less safe. For example, by setting the commit interval to a longer time than the default of five to force less frequent flushes. EXT4 also introduced delayed allocation of blocks, which brings with it a greater chance of data loss and filesystem corruption in the case of a system failure.
</p>

<p>
XFS also has better performance for Kafka’s workload without requiring tuning beyond the automatic tuning performed by the filesystem. It is also more efficient when batching disk writes, all of which combine to give better overall
I/O throughput.
</p>

<p>
Regardless of which filesystem is chosen, it is advisable to set the <strong>noatime mount option for the mount point</strong>. File metadata contains three timestamps:
</p>
<ul>
<li class="level1"><div class="li"> creation time (ctime): </div>
</li>
<li class="level1"><div class="li"> last modified time (mtime):</div>
</li>
<li class="level1"><div class="li"> last access time (atime): It&#039;s updated every time a file is read. This generates a large number of disk writes.</div>
</li>
</ul>

<p>
The <strong>atime is not used by Kafka at all</strong>, so disabling it is safe to do. Setting noatime on the mount will prevent these timestamp updates from happening, but <strong>will not affect the proper handling of the ctime and mtime attributes</strong>.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;7 Choosing file system&quot;,&quot;hid&quot;:&quot;choosing_file_system&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:10,&quot;range&quot;:&quot;6581-8058&quot;} -->
<h2 class="sectionedit11" id="jvm_setup">8. JVM setup</h2>
<div class="level2">

<p>
As we use JDK 8 for running Kafka. The best GC to collect garbage is G1. The Garbage First (or G1) garbage collector is designed to automatically adjust to different workloads and provide consistent pause times for garbage collection over the lifetime of the application. It also handles large heap sizes with ease by segmenting the heap into smaller zones and not collecting over the entire heap in each pause.
</p>

<p>
There are two configuration options for G1 used to adjust its performance:
</p>
<ul>
<li class="level1"><div class="li"> <strong>MaxGCPauseMillis</strong>: This option specifies the preferred pause time for each garbage-collection cycle. This value defaults to 200 milliseconds. This means that G1 will attempt to schedule the frequency of GC cycles, as well as the number of zones that are collected in each cycle, such that each cycle will take approximately 200ms(can be exceeded).</div>
</li>
<li class="level1"><div class="li"> <strong>InitiatingHeapOccupancyPercent</strong>: This option specifies the percentage of the total heap that may be in use before G1 will start a collection cycle. The default value is 45. This means that G1 will not start a collection cycle until after 45% of the heap is in use. This includes both the new (Eden) and old zone usage in total.</div>
</li>
</ul>

<p>
<strong>The GC tuning options have been tested for a server with 64 <abbr title="Gigabyte">GB</abbr> of memory. Running Kafka in a 5GB heap, MaxGCPauseMillis = 20 ms(less application pause time than default), and InitiatingHeapOccupancyPercent = 35(garbage collection to run slightly earlier than with the default value)</strong> can do the job. If you want to give more heap to make it more stable, you can go up to 10GB.
</p>

<p>
By default, Kafka uses <abbr title="Content Management System">CMS</abbr> as default GC, not G1. To modify it, you need to set the following parameter.
</p>
<pre class="code"># set java home
export JAVA_HOME=/usr/java/jdk1.8.0_51

# set environment var KAFKA_JVM_PERFORMANCE_OPTS, this can be changed with different version of Kafka
export KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:+UseG1GC
-XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35
-XX:+DisableExplicitGC -Djava.awt.headless=true&quot;

# 
bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties</pre>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;8. JVM setup&quot;,&quot;hid&quot;:&quot;jvm_setup&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:11,&quot;range&quot;:&quot;8059-10171&quot;} -->
<h2 class="sectionedit12" id="data_center_layout">9 Data Center layout</h2>
<div class="level2">

<p>
If you deploy the Kafka Cluster in a data center which has multiple physical racks, or multiple availability zones (if running in a cloud service like AWS). <strong>It is recommended to have each Kafka broker in a cluster installed in a different rack</strong>, or at the very least not share single points of failure for infrastructure services such as
power and network. This typically means at least deploying the servers that will run brokers with dual power connections (to two different circuits) and dual network switches (with a bonded interface on the servers themselves to failover seamlessly). Even with dual connections, there is a benefit to having brokers in completely separate racks. You can maintain your physical racks without stoping the Kafka cluster.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;9 Data Center layout&quot;,&quot;hid&quot;:&quot;data_center_layout&quot;,&quot;codeblockOffset&quot;:1,&quot;secid&quot;:12,&quot;range&quot;:&quot;10172-10965&quot;} -->
<h2 class="sectionedit13" id="综上描述">综上描述</h2>
<div class="level2">
<ul>
<li class="level1"><div class="li"> 10亿数据，60k message(50kb)/sec 的吞吐量，276T的数据(7 day retention)，5台物理机</div>
</li>
<li class="level1"><div class="li"> 硬盘：11（SAS） * 7T，7200转</div>
</li>
<li class="level1"><div class="li"> 内存：64GB/128GB，JVM分配10G，剩余的给os cache</div>
</li>
<li class="level1"><div class="li"> CPU：16核/32核</div>
</li>
<li class="level1"><div class="li"> 网络：千兆网卡，万兆更好</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;\u7efc\u4e0a\u63cf\u8ff0&quot;,&quot;hid&quot;:&quot;\u7efc\u4e0a\u63cf\u8ff0&quot;,&quot;codeblockOffset&quot;:1,&quot;secid&quot;:13,&quot;range&quot;:&quot;10966-&quot;} -->