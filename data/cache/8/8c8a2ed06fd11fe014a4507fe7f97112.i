a:14:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:47:"Integrating Apache spark 2.x jobs with NiFi 1.7";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:62:"There are many ways to integrate Apache NiFi and Apache Spark.";}i:2;i:65;}i:5;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:127;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:127;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:295:"We can call Apache Spark Streaming via S2S (Apache NiFi's Site to Site) or Kafka. If you want to execute a regular Apache Spark job, you can do that via Apache Livy which is included in HDP 2.6+. This is how Apache Zeppelin integrates with Apache Spark, so it's secure and a reasonable approach.";}i:2;i:129;}i:8;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:424;}i:9;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:424;}i:10;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:112:"I use this approach when I want to use Spark to process part of my process in the middle of an Apache NiFi flow.";}i:2;i:426;}i:11;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:538;}i:12;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:538;}i:13;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:538;}}