a:296:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:42:"Use Big data techs to analyze taobao sales";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:52:"The data set that I use can be downloaded from here ";}i:2;i:59;}i:5;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:31:"https://pan.baidu.com/s/1cs02Nc";i:1;N;}i:2;i:111;}i:6;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:" .";}i:2;i:142;}i:7;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:144;}i:8;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:144;}i:9;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:40:"In the data_format.zip, we have 3 files:";}i:2;i:146;}i:10;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:186;}i:11;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:186;}i:12;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:186;}i:13;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:186;}i:14;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" user_log.csv ";}i:2;i:190;}i:15;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:204;}i:16;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:" it logs the actions of users.";}i:2;i:206;}i:17;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:236;}i:18;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:236;}i:19;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:236;}i:20;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:236;}i:21;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" train.csv ";}i:2;i:240;}i:22;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:251;}i:23;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:62:" fidel client (client use taobao many times) training data set";}i:2;i:253;}i:24;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:315;}i:25;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:315;}i:26;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:315;}i:27;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:315;}i:28;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:" test.csv ";}i:2;i:319;}i:29;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:329;}i:30;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:61:" fidel client (client use taobao many times) testing data set";}i:2;i:331;}i:31;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:392;}i:32;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:392;}i:33;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:392;}i:34;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:394;}i:35;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:20:"Data set description";i:1;i:2;i:2;i:394;}i:2;i:394;}i:36;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:394;}i:37;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:332:"
user_id,item_id,cat_id,merchant_id,brand_id,month,day,action,age_range,gender,province
328862,323294,833,2882,2661,08,29,0,0,1,GuangDong
328862,844400,1271,2882,2661,08,29,0,1,1,Xian
328862,575153,1271,2882,2661,08,29,0,2,1,JiangSu
328862,996875,1271,2882,2661,08,29,0,1,1,LiaoNing
328862,1086186,1271,1253,1049,08,29,0,0,2,Taiwan
";i:1;s:3:"csv";i:2;s:12:"user_log.csv";}i:2;i:433;}i:38;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:150:"
user_id,age_range,gender,merchant_id,label
34176,6,0,944,-1
34176,6,0,412,-1
34176,6,0,1945,-1
34176,6,0,4752,-1
34176,6,0,643,-1
34176,6,0,2828,-1

";i:1;s:3:"csv";i:2;s:9:"train.csv";}i:2;i:797;}i:39;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:970;}i:40;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"User_log.csv";i:1;i:3;i:2;i:970;}i:2;i:970;}i:41;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:970;}i:42;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:11;i:1;i:2;i:2;i:994;}i:2;i:993;}i:43;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:993;}i:44;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:993;}i:45;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:993;}i:46;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"user_id";}i:2;i:995;}i:47;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1002;}i:48;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1002;}i:49;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"item_id";}i:2;i:1003;}i:50;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1010;}i:51;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1010;}i:52;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"cat_id";}i:2;i:1011;}i:53;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1017;}i:54;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1017;}i:55;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"merchant_id";}i:2;i:1018;}i:56;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1029;}i:57;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1029;}i:58;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"brand_id";}i:2;i:1030;}i:59;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1038;}i:60;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1038;}i:61;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"month";}i:2;i:1039;}i:62;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1044;}i:63;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1044;}i:64;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"day";}i:2;i:1045;}i:65;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1048;}i:66;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1048;}i:67;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"action";}i:2;i:1049;}i:68;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1055;}i:69;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1055;}i:70;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"age_range";}i:2;i:1056;}i:71;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1065;}i:72;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1065;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"gender";}i:2;i:1066;}i:74;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1072;}i:75;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1072;}i:76;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"province";}i:2;i:1073;}i:77;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1081;}i:78;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:1082;}i:79;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:1082;}i:80;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:1082;}i:81;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1082;}i:82;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Id of buyer";}i:2;i:1084;}i:83;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1095;}i:84;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1095;}i:85;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:"Id of product";}i:2;i:1096;}i:86;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1109;}i:87;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1109;}i:88;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:"Id of product categories";}i:2;i:1110;}i:89;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1134;}i:90;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1134;}i:91;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:"Id of seller ";}i:2;i:1135;}i:92;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1148;}i:93;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1148;}i:94;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Id of brand";}i:2;i:1149;}i:95;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1160;}i:96;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1160;}i:97;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:"transaction date in month";}i:2;i:1161;}i:98;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1186;}i:99;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1186;}i:100;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"transaction date in day";}i:2;i:1187;}i:101;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1210;}i:102;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1210;}i:103;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:124:"Behavior of buyer, 0 means click on link, 1 means add to shopping chart, 2 means buy product, 3 means keep an eye on product";}i:2;i:1211;}i:104;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1335;}i:105;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1335;}i:106;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:"Age range: 1";}i:2;i:1336;}i:107;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1348;}i:108;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:" [0,18], 2";}i:2;i:1350;}i:109;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1360;}i:110;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" [18,24], 3";}i:2;i:1362;}i:111;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1373;}i:112;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" [25,29], 4";}i:2;i:1375;}i:113;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1386;}i:114;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[30,34], 5";}i:2;i:1388;}i:115;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1398;}i:116;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[35,39], 6";}i:2;i:1400;}i:117;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1410;}i:118;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[40,49], 7";}i:2;i:1412;}i:119;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1422;}i:120;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"[50,200], 8";}i:2;i:1424;}i:121;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1435;}i:122;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"[Null]";}i:2;i:1437;}i:123;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1443;}i:124;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1443;}i:125;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:"Gender of the buyer 0";}i:2;i:1444;}i:126;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1465;}i:127;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"femal, 1";}i:2;i:1467;}i:128;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1475;}i:129;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"male, 2";}i:2;i:1477;}i:130;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1484;}i:131;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"NULL";}i:2;i:1486;}i:132;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1490;}i:133;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1490;}i:134;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:27:"Province Name of the buyer ";}i:2;i:1491;}i:135;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1518;}i:136;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:1519;}i:137;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:1519;}i:2;i:1519;}i:138;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1520;}i:139;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:22:"train.csv and test.csv";i:1;i:3;i:2;i:1520;}i:2;i:1520;}i:140;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1520;}i:141;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:5;i:1;i:2;i:2;i:1553;}i:2;i:1552;}i:142;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:1552;}i:143;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:1552;}i:144;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1552;}i:145;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"user_id";}i:2;i:1554;}i:146;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1561;}i:147;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1561;}i:148;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"age_range";}i:2;i:1562;}i:149;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1571;}i:150;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1571;}i:151;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"gender";}i:2;i:1572;}i:152;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1578;}i:153;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1578;}i:154;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"merchant_id";}i:2;i:1579;}i:155;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1590;}i:156;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1590;}i:157;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"label";}i:2;i:1591;}i:158;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:1596;}i:159;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:1597;}i:160;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:1597;}i:161;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:1597;}i:162;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1597;}i:163;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Id of buyer";}i:2;i:1599;}i:164;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1610;}i:165;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1610;}i:166;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:"Age range: 1";}i:2;i:1611;}i:167;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1623;}i:168;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:" [0,18], 2";}i:2;i:1625;}i:169;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1635;}i:170;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" [18,24], 3";}i:2;i:1637;}i:171;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1648;}i:172;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:" [25,29], 4";}i:2;i:1650;}i:173;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1661;}i:174;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[30,34], 5";}i:2;i:1663;}i:175;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1673;}i:176;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[35,39], 6";}i:2;i:1675;}i:177;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1685;}i:178;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"[40,49], 7";}i:2;i:1687;}i:179;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1697;}i:180;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"[50,200], 8";}i:2;i:1699;}i:181;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1710;}i:182;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"[Null]";}i:2;i:1712;}i:183;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1718;}i:184;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1718;}i:185;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:"Gender of the buyer 0";}i:2;i:1719;}i:186;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1740;}i:187;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"femal, 1";}i:2;i:1742;}i:188;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1750;}i:189;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"male, 2";}i:2;i:1752;}i:190;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1759;}i:191;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"NULL";}i:2;i:1761;}i:192;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1765;}i:193;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1765;}i:194;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:"Id of seller ";}i:2;i:1766;}i:195;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1779;}i:196;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:1779;}i:197;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:54:" This label means will this client use taobao again. 1";}i:2;i:1780;}i:198;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1834;}i:199;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"yes, 0";}i:2;i:1836;}i:200;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1842;}i:201;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"No, -1";}i:2;i:1844;}i:202;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1850;}i:203;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:20:"not considered, Null";}i:2;i:1852;}i:204;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:1872;}i:205;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"only exists in test.csv";}i:2;i:1874;}i:206;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:1897;}i:207;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:1900;}i:208;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:1900;}i:2;i:1900;}i:209;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1903;}i:210;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:19:"Import data to hive";i:1;i:2;i:2;i:1903;}i:2;i:1903;}i:211;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:1903;}i:212;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1903;}i:213;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:74:"To make this tutorial, we will only load the small_user_log.csv into hive.";}i:2;i:1936;}i:214;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2010;}i:215;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2010;}i:216;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:51:"Follow this link if you don't have hive installed. ";}i:2;i:2012;}i:217;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:49:"employes:pengfei.liu:big_data:hive:install_config";i:1;s:26:"Install and configure Hive";}i:2;i:2063;}i:218;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2143;}i:219;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2143;}i:220;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:"First tload the data into hdfs";}i:2;i:2146;}i:221;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2176;}i:222;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:251:"
#load from local file system to hdfs 
hdfs dfs -put small_user_log.csv /test_data/
#check the file
[hadoop@localhost taobao_data_set]$ hdfs dfs -ls /test_data
-rw-r--r--   1 hadoop supergroup     473395 2018-02-14 14:14 /test_data/small_user_log.csv
";i:1;N;i:2;N;}i:2;i:2183;}i:223;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2183;}i:224;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:110:"In hive, we create a external table. Please see this, the difference between internal and external hive table.";}i:2;i:2444;}i:225;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:58:"employes:pengfei.liu:big_data:hive:internal_external_table";i:1;s:33:"Hive Internal and External tables";}i:2;i:2554;}i:226;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2650;}i:227;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:360:"
hive>  CREATE EXTERNAL TABLE dbtaobao.user_log(user_id INT,item_id INT,cat_id INT,merchant_id INT,brand_id INT,month STRING,day STRING,action INT,age_range INT,gender INT,province STRING) COMMENT 'store taobao user action log in table dbtaobao.user_log!' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION '/dbtaobao/dataset/user_log';
";i:1;N;i:2;N;}i:2;i:2657;}i:228;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2657;}i:229;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"Test the table content ";}i:2;i:3027;}i:230;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3050;}i:231;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:291:"
hive> select * from user_log limit 10;
OK
328862	406349	1280	2700	5476	11	11	0	0	1	ShiChuan
328862	406349	1280	2700	5476	11	11	0	7	1	ChongQing
328862	807126	1181	1963	6109	11	11	0	1	0	ShangHai
328862	406349	1280	2700	5476	11	11	2	6	0	Taiwan
328862	406349	1280	2700	5476	11	11	0	6	2	GanShu

";i:1;N;i:2;N;}i:2;i:3057;}i:232;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3057;}i:233;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:29:"Get column name and type list";}i:2;i:3358;}i:234;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3393;}i:235;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:764:"
hive> desc user_log;
OK
user_id             	int                 	                    
item_id             	int                 	                    
cat_id              	int                 	                    
merchant_id         	int                 	                    
brand_id            	int                 	                    
month               	string              	                    
day                 	string              	                    
action              	int                 	                    
age_range           	int                 	                    
gender              	int                 	                    
province            	string              	                    
Time taken: 0.142 seconds, Fetched: 11 row(s)
";i:1;N;i:2;N;}i:2;i:3393;}i:236;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3393;}i:237;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:"Check table information with details;";}i:2;i:4167;}i:238;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4204;}i:239;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:805:"
hive> show create table user_log;
OK
CREATE EXTERNAL TABLE `user_log`(
  `user_id` int, 
  `item_id` int, 
  `cat_id` int, 
  `merchant_id` int, 
  `brand_id` int, 
  `month` string, 
  `day` string, 
  `action` int, 
  `age_range` int, 
  `gender` int, 
  `province` string)
COMMENT 'store taobao user action log in table dbtaobao.user_log!'
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
WITH SERDEPROPERTIES ( 
  'field.delim'=',', 
  'serialization.format'=',') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://localhost:9000/dbtaobao/dataset/user_log'
TBLPROPERTIES (
  'transient_lastDdlTime'='1518623485')
Time taken: 0.242 seconds, Fetched: 26 row(s)

";i:1;N;i:2;N;}i:2;i:4211;}i:240;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4211;}i:241;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:85:"As hive runs on top of map reduce, all the sql job will be transform to mapreduce job";}i:2;i:5026;}i:242;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5111;}i:243;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1411:"
# count all the lines in table user_log
hive> select count(*) from user_log;

# count distinct user 
hive> select count(distinct user_id) from user_log;

# get the province which has most users
hive> select count(user_id) as num, province from user_log group by province order by num desc limit 10;


# eliminate duplicate user orders
# we could say two user orders are the same, if the following attributes (user_id, item_id, cat_id, merchant_id, brand_id, month, day, action) of two orders are the same
# the following sql query distinc orders
hive> select count(*) from (select user_id,item_id,cat_id,merchant_id,brand_id,month,day,action from user_log group by user_id,item_id,cat_id,merchant_id,brand_id,month,day,action having count(*)=1)a;

#check how many people buy an item at 11/11.
hive> select count(distinct user_id) from user_log where action='2';

# get the female buyer numbers
hive> select count(distinct user_id) from user_log where action='2' and gender=0;

# get the product numbers which male buyer bought
hive> select count(*) from user_log where gender=1 and action='2';

# get user id of users which have bought more than 5 products
hive> select user_id from user_log where action='2' group by user_id having count(action='2')>5;

# get the top 5 buyer
hive> select user_id, count(action='2') as buy_num from user_log where action='2' group by user_id order by buy_num desc limit 5;



";i:1;N;i:2;N;}i:2;i:5119;}i:244;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5119;}i:245;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:56:"WE can also create new table based on the existing table";}i:2;i:6540;}i:246;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6596;}i:247;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:457:"
# create a table with 2 columns
hive> create table scan(brand_id INT,scan INT) COMMENT 'This is the search of bigdatataobao' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
OK
Time taken: 0.272 seconds

# load data of brand visite number
hive> insert overwrite table scan select brand_id,count(action) from user_log where action='2' group by brand_id;

# check new table
hive> select * from scan limit 5;
OK
NULL	8
60	3
69	1
82	11
99	3

";i:1;N;i:2;N;}i:2;i:6603;}i:248;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:7070;}i:249;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:37:"Export data from hive to sql database";i:1;i:2;i:2;i:7070;}i:2;i:7070;}i:250;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:7070;}i:251;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7070;}i:252;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:132:"In this tutorial, we will export a hive table into a postgreql database. We will use login pliu to connect to the postgresql server.";}i:2;i:7121;}i:253;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7253;}i:254;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:7255;}i:255;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:17:"Prepare postgreql";i:1;i:3;i:2;i:7255;}i:2;i:7255;}i:256;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:7255;}i:257;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:719:"
#run the postgresql server and login as postgres (root)

postgres=# create database dbtaobao;
CREATE DATABASE
postgres=# grant all privileges on database dbtaobao to pliu;
GRANT

[pliu@localhost tmp]$ psql -h 127.0.0.1 -p 5432 -U pliu -d dbtaobao
Password for user pliu: 
psql (9.5.11)
Type "help" for help.

dbtaobao=> CREATE TABLE user_log (user_id varchar(20),item_id varchar(20),cat_id varchar(20),merchant_id varchar(20),brand_id varchar(20), month varchar(6),day varchar(6),action varchar(6),age_range varchar(6),gender varchar(6),province varchar(10));

dbtaobao=> \dt
         List of relations
 Schema |   Name   | Type  | Owner 
--------+----------+-------+-------
 public | user_log | table | pliu
(1 row)

";i:1;N;i:2;N;}i:2;i:7289;}i:258;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:8018;}i:259;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:29:"Prepare hive table for export";i:1;i:3;i:2;i:8018;}i:2;i:8018;}i:260;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:8018;}i:261;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8018;}i:262;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:132:"Sqoop does not support import export of hive external tables. So we need to create an interal hive table to make sqoop export works.";}i:2;i:8059;}i:263;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8191;}i:264;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:852:"
# create a hive internal table
hive> create table dbtaobao.inner_user_log(user_id INT,item_id INT,cat_id INT,merchant_id INT,brand_id INT,month STRING,day STRING,action INT,age_range INT,gender INT,province STRING) COMMENT 'Now create inner table inner_user_log' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

# Use sqoop to export hive to 
cd /opt/sqoop/sqoop-1.4.6/bin
./sqoop export --connect jdbc:postgresql://127.0.0.1:5432/dbtaobao --username pliu --password changeMe --table user_log --export-dir '/user/hive/warehouse/dbtaobao.db/inner_user_log' --fields-terminated-by ',';

# last two lines of the output, we export 10000 records to postgreql
18/02/19 11:44:26 INFO mapreduce.ExportJobBase: Transferred 479.0508 KB in 52.9949 seconds (9.0396 KB/sec)
18/02/19 11:44:26 INFO mapreduce.ExportJobBase: Exported 10000 records.
";i:1;N;i:2;N;}i:2;i:8198;}i:265;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8198;}i:266;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:"Sqoop command explained :";}i:2;i:9060;}i:267;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9085;}i:268;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:375:"
# postgresql part: 
# connect to database dbtaobao table user_log with user name pliu and password
--connect jdbc:postgresql://127.0.0.1:5432/dbtaobao --username pliu --password changeMe --table user_log 

# hive part:
# give the path of hive table file
--export-dir '/user/hive/warehouse/dbtaobao.db/inner_user_log' 

# hive table file separator
--fields-terminated-by ','
";i:1;N;i:2;N;}i:2;i:9093;}i:269;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:9478;}i:270;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:34:"Use spark to analyze this data set";i:1;i:2;i:2;i:9478;}i:2;i:9478;}i:271;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:9478;}i:272;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9478;}i:273;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:85:"We will redo the data analytics which we do in hive, but this time we do it in spark.";}i:2;i:9527;}i:274;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9612;}i:275;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:9614;}i:276;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:20:"Import data to spark";i:1;i:3;i:2;i:9614;}i:2;i:9614;}i:277;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:9614;}i:278;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9614;}i:279;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:132:"As we export the data into postgresql server, in the following example. we will import data from the csv file and postgresql server.";}i:2;i:9646;}i:280;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9778;}i:281;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:770:"
<properties>
        <spark.version>2.2.0</spark.version>
        <scala.version>2.11</scala.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

</properties>

<dependencies>
<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
</dependency>

<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
</dependency>

<dependency>
            <groupId>org.postgresql</groupId>
            <artifactId>postgresql</artifactId>
            <version>9.4-1200-jdbc41</version>
</dependency>
</dependencies>
";i:1;s:3:"xml";i:2;s:7:"pom.xml";}i:2;i:9787;}i:282;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9787;}i:283;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:140:"The following scala script read data from csv or postgresql db and transform it into a dataframe, then write dataframe in a parquet file.   ";}i:2;i:10579;}i:284;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10719;}i:285;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:1830:"
package org.pengfei.spark.application.example

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, SparkSession}

object TaobaoSales {

def main(args:Array[String]): Unit ={
  Logger.getLogger("org").setLevel(Level.OFF)
  Logger.getLogger("akka").setLevel(Level.OFF)

  val spark = SparkSession.builder().
    master("local").
    appName("TaobaoSales").
    getOrCreate()

  val userLogDF=getDFFromDB(spark)
  /*val filePath="file:///DATA/data_set/spark/taobao_data_set/small_user_log.csv"
  val userLogDF=getDFFromCSV(spark,filePath)*/
  //userLogDF.show(5)
  userLogDF.write.format("parquet").save("file:///tmp/taobao.parquet")
  
}

  def getDFFromDB(spark : SparkSession): DataFrame ={
    val userLogDF=spark.read.format("jdbc").option("url", "jdbc:postgresql://127.0.0.1:5432/dbtaobao").option("driver","org.postgresql.Driver").option("dbtable", "user_log").option("user", "pliu").option("password", "Liua1983").load()
    return userLogDF
  }

  def getDFFromCSV(spark:SparkSession,filePath:String):DataFrame ={
    val userLogSchema = StructType(Array(
      StructField("user_id",IntegerType,true),
      StructField("item_id",IntegerType,true),
      StructField("cat_id",IntegerType,true),
      StructField("merchant_id",IntegerType,true),
      StructField("brand_id",IntegerType,true),
      StructField("month",StringType,true),
      StructField("day",StringType,true),
      StructField("action",IntegerType,true),
      StructField("age_range",IntegerType,true),
      StructField("gender",IntegerType,true),
      StructField("province",StringType,true)
    ))
    val userLogDF= spark.read.format("csv").option("header","false").schema(userLogSchema).load(filePath)
    return userLogDF
  }

}

";i:1;s:5:"scala";i:2;s:17:"TaobaoSales.scala";}i:2;i:10727;}i:286;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10727;}i:287;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:57:"For analytic, we use spark shell to read the parquet file";}i:2;i:12591;}i:288;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:12648;}i:289;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:994:"
scala> val df=spark.read.parquet("file:///tmp/taobao.parquet")
df: org.apache.spark.sql.DataFrame = [user_id: string, item_id: string ... 9 more fields]

scala> df.show(5)
+-------+-------+------+-----------+--------+-----+---+------+---------+------+--------+
|user_id|item_id|cat_id|merchant_id|brand_id|month|day|action|age_range|gender|province|
+-------+-------+------+-----------+--------+-----+---+------+---------+------+--------+
| 188983| 678722|     4|        231|    6065|   11| 11|     0|        6|     0|      四川|
| 188983| 980237|  1023|       1595|    5800|   11| 11|     0|        0|     0|      河南|
| 188983|1024557|   868|        184|    1360|   11| 11|     0|        2|     2|      江苏|
| 188983|   3195|   868|       2786|    1360|   11| 11|     0|        7|     0|      四川|
| 188983| 280132|   407|       1595|    5800|   11| 11|     0|        3|     2|      澳门|
+-------+-------+------+-----------+--------+-----+---+------+---------+------+--------+
";i:1;N;i:2;N;}i:2;i:12655;}i:290;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:12655;}i:291;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:56:"In the following script, we redo all the analytic which ";}i:2;i:13660;}i:292;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13716;}i:293;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:1709:"
# count the number of all records 
df.count

# count distinct user 
scala> df.select($"user_id").distinct.count
res5: Long = 358

# get the province which has most users
scala> df.groupBy($"province").count().sort(desc("count"))show(5)


# eliminate duplicate user orders
# we could say two user orders are the same, if the following attributes (user_id, item_id, cat_id, 
# merchant_id, brand_id, month, day, action) of two orders are the same
# the following sql query distinc orders

hive> select count(*) from (select user_id,item_id,cat_id,merchant_id,brand_id,month,day,action from user_log group by user_id,item_id,cat_id,merchant_id,brand_id,month,day,action having count(*)=1)a;

#check how many people buy an item at 11/11.
scala> df.filter($"action"==="2").select($"user_id").distinct().count
res11: Long = 358   

# get the female buyer numbers
scala> df.filter($"action"==="2"&& $"gender"===2).select($"user_id").distinct().count
res12: Long = 228 

# get the product numbers which male buyer bought
scala> df.filter($"action"==="2"&& $"gender"===1).select($"item_id").count
res13: Long = 401

# get user id of users which have bought more than 5 products
scala> df.filter($"action"==="2").groupBy($"user_id").agg(count($"action") as "num").where($"num" > 5).sort(desc("num"))
+-------+---+                                                                   
|user_id|num|
+-------+---+
| 409280| 20|
| 366342| 17|
|  70816| 16|
| 370679| 16|
| 310632| 15|
+-------+---+


# get the top 5 buyer, this request is similar to the privious one, we just remove the where and add show top 5
scala> df.filter($"action"==="2").groupBy($"user_id").agg(count($"action") as "num").sort(desc("num")).show(5)
";i:1;N;i:2;N;}i:2;i:13723;}i:294;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15440;}i:295;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:15440;}}