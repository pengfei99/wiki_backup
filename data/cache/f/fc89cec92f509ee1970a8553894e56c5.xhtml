
<h1 class="sectionedit1" id="flume_installation_and_configuration">Flume installation and configuration</h1>
<div class="level1">

</div>
<!-- EDIT1 SECTION "Flume installation and configuration" [1-52] -->
<h2 class="sectionedit2" id="what_is_flume">What is Flume?</h2>
<div class="level2">

<p>
Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log files, events (etc…) from various sources to a centralized data store.
</p>

<p>
Flume is a highly reliable, distributed, and configurable tool. It is principally designed to copy streaming data (log data) from various web servers to HDFS.
</p>

</div>
<!-- EDIT2 SECTION "What is Flume?" [53-458] -->
<h2 class="sectionedit3" id="application_of_flume">Application of Flume</h2>
<div class="level2">

<p>
Assume an e-commerce web application wants to analyze the customer behavior from a particular region. To do so, they would need to move the available log data in to Hadoop for analysis. Here, Apache Flume comes to our rescue.
</p>

<p>
Flume is used to move the log data generated by application servers into HDFS at a higher speed.
</p>

</div>
<!-- EDIT3 SECTION "Application of Flume" [459-817] -->
<h2 class="sectionedit4" id="advantages_and_features_of_flume">Advantages and features of Flume</h2>
<div class="level2">

</div>
<!-- EDIT4 SECTION "Advantages and features of Flume" [818-863] -->
<h3 class="sectionedit5" id="advantages_of_flume">Advantages of Flume</h3>
<div class="level3">

<p>
Here are the advantages of using Flume:
</p>
<ul>
<li class="level1"><div class="li"> Using Apache Flume we can store the data in to any of the centralized stores (HBase, HDFS).</div>
</li>
<li class="level1"><div class="li"> When the rate of incoming data exceeds the rate at which data can be written to the destination, Flume acts as a mediator between data producers and the centralized stores and provides a steady flow of data between them.</div>
</li>
<li class="level1"><div class="li"> Flume provides the feature of contextual routing.</div>
</li>
<li class="level1"><div class="li"> The transactions in Flume are channel-based where two transactions (one sender and one receiver) are maintained for each message. It guarantees reliable message delivery.</div>
</li>
<li class="level1"><div class="li"> Flume is reliable, fault tolerant, scalable, manageable, and customizable.</div>
</li>
</ul>

</div>
<!-- EDIT5 SECTION "Advantages of Flume" [864-1565] -->
<h3 class="sectionedit6" id="features_of_flume">Features of Flume</h3>
<div class="level3">

<p>
Some of the notable features of Flume are as follows:
</p>
<ul>
<li class="level1"><div class="li"> Flume ingests log data from multiple web servers into a centralized store (HDFS, HBase) efficiently.</div>
</li>
<li class="level1"><div class="li"> Using Flume, we can get the data from multiple servers immediately into Hadoop.</div>
</li>
<li class="level1"><div class="li"> Along with the log files, Flume is also used to import huge volumes of event data produced by social networking sites like Facebook and Twitter, and e-commerce websites like Amazon and Flipkart.</div>
</li>
<li class="level1"><div class="li"> Flume supports a large set of sources and destinations types.</div>
</li>
<li class="level1"><div class="li"> Flume supports multi-hop flows, fan-in fan-out flows, contextual routing, etc.</div>
</li>
<li class="level1"><div class="li"> Flume can be scaled horizontally.</div>
</li>
</ul>

</div>
<!-- EDIT6 SECTION "Features of Flume" [1566-2225] -->
<h2 class="sectionedit7" id="key_concept_of_flume">Key concept of Flume</h2>
<div class="level2">

<p>
Flume has three layer :
</p>
<ul>
<li class="level1"><div class="li"> agent → An agent is an independent process (JVM) in Flume.It collects data from clients or other agents and send them to next destination(sink or agent).</div>
</li>
<li class="level1"><div class="li"> collector → Collector can combine data from one or many agents, and send it to storage</div>
</li>
<li class="level1"><div class="li"> storage → Storage can be a file, hdfs, Hive, HBase, etc.</div>
</li>
</ul>

</div>
<!-- EDIT7 SECTION "Key concept of Flume" [2226-2597] -->
<h3 class="sectionedit8" id="basic_components_of_flume">Basic components of Flume</h3>
<div class="level3">

<p>
In one agent, you have the following key concepts.
</p>
<ul>
<li class="level1"><div class="li"> Event → An event is the basic unit of the data transported inside Flume. It contains a payload of byte array that is to be transported from the source to the destination accompanied by optional headers. A typical Flume event would have the following structure (Header+Byte_Payload)</div>
</li>
<li class="level1"><div class="li"> Source → A source is the component of an Agent which receives data from the data generators and transfers it to one or more channels in the form of Flume events. Apache Flume supports several types of sources and each source receives events from a specified data generator. (e.g. Avro source, Thrift source, twitter 1% source)</div>
</li>
<li class="level1"><div class="li"> Channel → A channel is a transient store which receives the events from the source and buffers them till they are consumed by sinks. It acts as a bridge between the sources and the sinks. These channels are fully transactional and they can work with any number of sources and sinks. (e.g. JDBC channel, File system channel, Memory channel, etc.)</div>
</li>
<li class="level1"><div class="li"> Sink → A sink stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels and delivers it to the destination. The destination of the sink might be another agent or the central stores. (e.g. HDFS sink)</div>
</li>
</ul>

</div>
<!-- EDIT8 SECTION "Basic components of Flume" [2598-3913] -->
<h3 class="sectionedit9" id="additional_components_of_flume_agent">Additional Components of Flume agent</h3>
<div class="level3">

<p>
What we have discussed above are the primitive components of the agent. In addition to this, we have a few more components that play a vital role in transferring the events from the data generator to the centralized stores.
</p>

</div>

<h4 id="interceptor">Interceptor</h4>
<div class="level4">

<p>
Interceptors are used to alter/inspect flume events which are transferred between source and channel.
</p>

</div>

<h4 id="channel_selectors">Channel Selectors</h4>
<div class="level4">

<p>
These are used to determine which channel is to be opted to transfer the data in case of multiple channels. There are two types of channel selectors 
</p>
<ul>
<li class="level1"><div class="li"> <strong>Default channel selectors</strong> − These are also known as replicating channel selectors they replicates all the events in each channel.</div>
</li>
<li class="level1"><div class="li"> <strong>Multiplexing channel selectors</strong> − These decides the channel to send an event based on the address in the header of that event.</div>
</li>
</ul>

</div>

<h4 id="sink_processors">Sink Processors</h4>
<div class="level4">

<p>
These are used to invoke a particular sink from the selected group of sinks. These are used to create failover paths for your sinks or load balance events across multiple sinks from a channel.
</p>

</div>
<!-- EDIT9 SECTION "Additional Components of Flume agent" [3914-4985] -->
<h2 class="sectionedit10" id="install_flume">Install flume</h2>
<div class="level2">

<p>
1. Download the flume tar ball from the flume site (<a href="https://www.apache.org/dist/flume/stable/" class="urlextern" title="https://www.apache.org/dist/flume/stable/" rel="nofollow">https://www.apache.org/dist/flume/stable/</a>).
</p>

<p>
The current stable version is flume 1.8.
</p>

<p>
Put the unziped flume under /opt/flume
</p>
<pre class="code">#flume home path
[hadoop@localhost flume-1.8.0]$ ls
bin        conf      doap_Flume.rdf  lib      NOTICE     RELEASE-NOTES
CHANGELOG  DEVNOTES  docs            LICENSE  README.md  tools
[hadoop@localhost flume-1.8.0]$ pwd
/opt/flume/flume-1.8.0

#add the flume home to the path
vim /etc/profile.d/flume.sh

export FLUME_HOME=/opt/flume/flume-1.8.0
export PATH=$PATH:$FLUME_HOME/bin

# check the flume home
[root@localhost profile.d]# source flume.sh 
[root@localhost profile.d]# echo $FLUME_HOME
/opt/flume/flume-1.8.0
</pre>

</div>
<!-- EDIT10 SECTION "Install flume" [4986-5727] -->
<h2 class="sectionedit11" id="configure_flume">Configure flume</h2>
<div class="level2">
<pre class="code">[hadoop@localhost conf]$ cp flume-env.sh.template flume-env.sh
[hadoop@localhost conf]$ vim flume-env.sh

#add the export java home
export JAVA_HOME=/opt/JAVA/jdk1.8.0_144

# Check the flume version
[hadoop@localhost flume-1.8.0]$ sh $FLUME_HOME/bin/flume-ng version 
Error: Could not find or load main class org.apache.flume.tools.GetJavaProperty
Flume 1.8.0
Source code repository: https://git-wip-us.apache.org/repos/asf/flume.git
Revision: 99f591994468633fc6f8701c5fc53e0214b6da4f
Compiled by denes on Fri Sep 15 14:58:00 CEST 2017
From source with checksum fbb44c8c8fb63a49be0a59e27316833d

# The Error is caused by HBAse, to get rid of it, you need to modify the 
# hbase-env.sh.

#The simple way is to comment the line 
##export HBASE_CLASSPATH 

#now rerun the command
[hadoop@localhost flume-1.8.0]$ sh $FLUME_HOME/bin/flume-ng version 
Flume 1.8.0
Source code repository: https://git-wip-us.apache.org/repos/asf/flume.git
Revision: 99f591994468633fc6f8701c5fc53e0214b6da4f
Compiled by denes on Fri Sep 15 14:58:00 CEST 2017
From source with checksum fbb44c8c8fb63a49be0a59e27316833d</pre>

</div>
<!-- EDIT11 SECTION "Configure flume" [5728-6865] -->
<h2 class="sectionedit12" id="test_your_flume_with_different_scenario">Test your flume with different scenario</h2>
<div class="level2">

<p>
For each data flow in Flume, we need to define a flume agent which contains: sources, channels, sinks. In the following examples, we will demonstrate how to configure different sources, channels and sinks.
</p>

</div>
<!-- EDIT12 SECTION "Test your flume with different scenario" [6866-7126] -->
<h3 class="sectionedit13" id="avro_agent">Avro agent</h3>
<div class="level3">

<p>
Avro is a remote procedure call and data serialization framework developed within Apache&#039;s Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. Its primary use is in Apache Hadoop, where it can provide both a serialization format for persistent data, and a wire format for communication between Hadoop nodes, and from client programs to the Hadoop services.
</p>

<p>
It is similar to Thrift and Protocol Buffers, but does not require running a code-generation program when a schema changes (unless desired for statically-typed languages).
</p>

<p>
Apache Spark SQL can access Avro as a data source.
</p>

</div>

<h4 id="edit_avro_agent_conf_file">Edit avro agent conf file</h4>
<div class="level4">

<p>
You can put the flume agent file anywhere. In this example, I put them under $FLUME_HOME/conf/agent-example
</p>
<pre class="code">vim /opt/flume/flume-1.8.0/conf/agent-example/avro.conf

#Put the following lines
# the name of this avro agent is avroAgent.
# it has one sources localAvroClient, sinks stdLogger, channels memoryChannel.
# Part 1. Define the name of sources, sinks, channels 
  avroAgent.sources = localAvroClient
  avroAgent.sinks = stdLogger
  avroAgent.channels = memoryChannel
 
# Part 2. Describe/configure the source
  avroAgent.sources.localAvroClient.type = avro
  avroAgent.sources.localAvroClient.channels = memoryChannel
# this source bind on every available network interface on this server at port 4141
  avroAgent.sources.localAvroClient.bind = 0.0.0.0
  avroAgent.sources.localAvroClient.port = 4141
 
# Part 3. Describe the sink
  avroAgent.sinks.stdLogger.type = logger
 
# Part 4. Use a channel which buffers events in memory
  avroAgent.channels.memoryChannel.type = memory
  avroAgent.channels.memoryChannel.capacity = 1000
  avroAgent.channels.memoryChannel.transactionCapacity = 100
 
# Bind the source and sink to the channel
  avroAgent.sources.localAvroClient.channels = memoryChannel
  avroAgent.sinks.stdLogger.channel = memoryChannel
</pre>

<p>
You can notice that, one agent must have a comple conf of source, sink and channel
</p>

</div>

<h4 id="run_flume_agent">Run flume agent</h4>
<div class="level4">

<p>
To run the agent,
</p>
<pre class="code">$ bin/flume-ng agent --conf ./conf/ -f /opt/flume/flume-1.8.0/conf/agent-example/avro.conf -n avroAgent -Dflume.root.logger=INFO,console

#agent -&gt; start a flume agent
#--conf/-c &lt;dir path&gt;-&gt; Use configuration file in the conf directory
#-f &lt;file path&gt; -&gt; Specifies a config file path, if missing
#--name, -n &lt;agent name&gt; -&gt; Name of the agent in the conf file 
# for example, if you put -n a1. WARN node.AbstractConfigurationProvider: No configuration found for this host:a1
#-Dproperty = value -&gt; Sets a Java system property value

</pre>

</div>

<h4 id="run_avro_client_to_send_flume_event_to_agent">Run avro client to send flume event to agent</h4>
<div class="level4">

<p>
In this example, we use the flume native avro-client
</p>
<pre class="code">#As the above agent listen to all available network interface, 
#so we need only send the content of text.txt to localhost on port 4141 ( port number defined in avro.conf file)
flume-ng avro-client -H localhost -p 4141 -F test.txt </pre>

<p>
Suppose the contenx of test.txt is 
</p>
<pre class="code">real

toto
titi
tata</pre>

<p>
You should see the following result
</p>
<pre class="code">18/01/22 10:43:44 INFO source.AvroSource: Avro source localAvroClient started.
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 =&gt; /127.0.0.1:4141] OPEN
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 =&gt; /127.0.0.1:4141] BOUND: /127.0.0.1:4141
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 =&gt; /127.0.0.1:4141] CONNECTED: /127.0.0.1:49466
18/01/22 10:43:53 INFO sink.LoggerSink: Event: { headers:{} body: 72 65 61 6C                                     real }
18/01/22 10:43:53 INFO sink.LoggerSink: Event: { headers:{} body: }
18/01/22 10:43:53 INFO sink.LoggerSink: Event: { headers:{} body: 74 6F 74 6F                                     toto }
18/01/22 10:43:53 INFO sink.LoggerSink: Event: { headers:{} body: 74 69 74 69                                     titi }
18/01/22 10:43:53 INFO sink.LoggerSink: Event: { headers:{} body: 74 61 74 61                                     tata }
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 :&gt; /127.0.0.1:4141] DISCONNECTED
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 :&gt; /127.0.0.1:4141] UNBOUND
18/01/22 10:43:53 INFO ipc.NettyServer: [id: 0x408ccd0a, /127.0.0.1:49466 :&gt; /127.0.0.1:4141] CLOSED
18/01/22 10:43:53 INFO ipc.NettyServer: Connection to /127.0.0.1:49466 disconnected.
</pre>

<p>
You can notice that, flume consider one line as a event, even the line is empty.
</p>

</div>
<!-- EDIT13 SECTION "Avro agent" [7127-11705] -->
<h3 class="sectionedit14" id="netcat_source_agent">Netcat source agent</h3>
<div class="level3">

</div>

<h4 id="flume_netcat_agent_conf">Flume netcat agent conf</h4>
<div class="level4">
<pre class="code">#name the components on this agent  
    netcatAgent.sources = localSocket  
    netcatAgent.sinks = stdLogger  
    netcatAgent.channels = memoryChannel  
 
    # Describe/configure the source  
    netcatAgent.sources.localSocket.type = netcat  
    netcatAgent.sources.localSocket.bind = localhost  
    netcatAgent.sources.localSocket.port = 8888 
     
 
    # Describe the sink  
    netcatAgent.sinks.stdLogger.type = logger  
 
    # Use a channel which buffers events in memory  
    netcatAgent.channels.memoryChannel.type = memory  
    netcatAgent.channels.memoryChannel.capacity = 10000  
    netcatAgent.channels.memoryChannel.transactionCapacity = 1000  
 
    # Bind the source and sink to the channel  
    netcatAgent.sources.localSocket.channels = memoryChannel  
    netcatAgent.sinks.stdLogger.channel = memoryChannel

   # set the line length of event 
   netcatAgent.sources.localSocket.deserializer.maxLineLength=10000 </pre>

</div>

<h4 id="run_the_agent">run the agent</h4>
<div class="level4">
<pre class="code">flume-ng agent -f /opt/flume/flume-1.8.0/conf/agent-example/netcat.conf -n netcatAgent -Dflume.root.logger=INFO,console</pre>

</div>

<h4 id="run_the_client">run the client</h4>
<div class="level4">
<pre class="code">[hadoop@localhost log]$ telnet localhost 8888</pre>

</div>
<!-- EDIT14 SECTION "Netcat source agent" [11706-] -->