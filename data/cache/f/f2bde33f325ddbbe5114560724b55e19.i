a:69:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:23:"Data structure overview";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:199:"Generally speaking, Spark provides 3 main abstractions to work with it. First, we will provide you with a holistic view of all of them in one place. Second, we will explore each option with examples.";}i:2;i:40;}i:5;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:240;}i:6;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:240;}i:7;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:240;}i:8;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:240;}i:9;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:244;}i:10;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:245;}i:11;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"RDD";}i:2;i:247;}i:12;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:250;}i:13;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:152:" (Resilient Distributed Dataset). The main approach to work with unstructured data. Pretty similar to a distributed collection that is not always typed.";}i:2;i:252;}i:14;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:404;}i:15;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:404;}i:16;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:404;}i:17;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:404;}i:18;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:408;}i:19;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:409;}i:20;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"Datasets";}i:2;i:411;}i:21;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:419;}i:22;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:163:". The main approach to work with semi-structured and structured data. Typed distributed collection, type-safety at a compile time, strong typing, lambda functions.";}i:2;i:421;}i:23;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:584;}i:24;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:584;}i:25;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:584;}i:26;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:584;}i:27;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:588;}i:28;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:589;}i:29;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"DataFrames";}i:2;i:591;}i:30;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:601;}i:31;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:242:". It is the Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. Think about it as a table in a relational database.";}i:2;i:603;}i:32;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:845;}i:33;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:845;}i:34;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:845;}i:35;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:845;}i:36;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:92:"The more Spark knows about the data initially, the more optimizations are available for you.";}i:2;i:847;}i:37;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:940;}i:38;a:3:{i:0;s:10:"listo_open";i:1;a:0:{}i:2;i:940;}i:39;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:940;}i:40;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:940;}i:41;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:944;}i:42;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:945;}i:43;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"RDD";}i:2;i:947;}i:44;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:950;}i:45;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:202:". Raw data lacking predefined structure forces you to do most of the optimizations by yourself. This results in lower performance out of the box and requires more effort to speed up the data processing.";}i:2;i:952;}i:46;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1154;}i:47;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1154;}i:48;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1154;}i:49;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1154;}i:50;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:1158;}i:51;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:1159;}i:52;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"Datasets";}i:2;i:1161;}i:53;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:1169;}i:54;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:116:". Typed data, possible to apply existing common optimizations, benefits of Spark SQLâ€™s optimized execution engine.";}i:2;i:1171;}i:55;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1287;}i:56;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1287;}i:57;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1287;}i:58;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1287;}i:59;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:1291;}i:60;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:1292;}i:61;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"DataFrames";}i:2;i:1294;}i:62;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:1304;}i:63;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:195:". Share the codebase with the Datasets and have the same basic optimizations. In addition, you have optimized code generation, transparent conversions to column based format and an SQL interface.";}i:2;i:1306;}i:64;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1501;}i:65;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1501;}i:66;a:3:{i:0;s:11:"listo_close";i:1;a:0:{}i:2;i:1501;}i:67;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1503;}i:68;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:1503;}}