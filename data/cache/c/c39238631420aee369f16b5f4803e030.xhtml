
<h1 class="sectionedit1" id="flume_introduction">Flume Introduction</h1>
<div class="level1">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Flume Introduction&quot;,&quot;hid&quot;:&quot;flume_introduction&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:1,&quot;range&quot;:&quot;1-34&quot;} -->
<h2 class="sectionedit2" id="what_is_flume">What is Flume?</h2>
<div class="level2">

<p>
Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log files, events (etc…) from various sources to a centralized data store.
</p>

<p>
Flume is a highly reliable, distributed, and configurable tool. It is principally designed to copy streaming data (log data) from various web servers to HDFS.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;What is Flume?&quot;,&quot;hid&quot;:&quot;what_is_flume&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:2,&quot;range&quot;:&quot;35-440&quot;} -->
<h2 class="sectionedit3" id="application_of_flume">Application of Flume</h2>
<div class="level2">

<p>
Assume an e-commerce web application wants to analyze the customer behavior from a particular region. To do so, they would need to move the available log data into Hadoop for analysis. Here, Apache Flume comes to our rescue.
</p>

<p>
Flume is used to move the log data generated by application servers into HDFS at a higher speed.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Application of Flume&quot;,&quot;hid&quot;:&quot;application_of_flume&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:3,&quot;range&quot;:&quot;441-798&quot;} -->
<h2 class="sectionedit4" id="advantages_and_features_of_flume">Advantages and features of Flume</h2>
<div class="level2">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Advantages and features of Flume&quot;,&quot;hid&quot;:&quot;advantages_and_features_of_flume&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:4,&quot;range&quot;:&quot;799-844&quot;} -->
<h3 class="sectionedit5" id="advantages_of_flume">Advantages of Flume</h3>
<div class="level3">

<p>
Here are the advantages of using Flume:
</p>
<ul>
<li class="level1"><div class="li"> Using Apache Flume we can store the data into any of the centralized stores (HBase, HDFS).</div>
</li>
<li class="level1"><div class="li"> When the rate of incoming data exceeds the rate at which data can be written to the destination, Flume acts as a mediator between data producers and the centralized stores and provides a steady flow of data between them.</div>
</li>
<li class="level1"><div class="li"> Flume provides the feature of contextual routing.</div>
</li>
<li class="level1"><div class="li"> The transactions in Flume are channel-based where two transactions (one sender and one receiver) are maintained for each message. It guarantees reliable message delivery.</div>
</li>
<li class="level1"><div class="li"> Flume is reliable, fault tolerant, scalable, manageable, and customizable.</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Advantages of Flume&quot;,&quot;hid&quot;:&quot;advantages_of_flume&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:5,&quot;range&quot;:&quot;845-1545&quot;} -->
<h3 class="sectionedit6" id="features_of_flume">Features of Flume</h3>
<div class="level3">

<p>
Some of the notable features of Flume are as follows:
</p>
<ul>
<li class="level1"><div class="li"> Flume ingests log data from multiple web servers into a centralized store (HDFS, HBase) efficiently.</div>
</li>
<li class="level1"><div class="li"> Using Flume, we can get the data from multiple servers immediately into Hadoop.</div>
</li>
<li class="level1"><div class="li"> Along with the log files, Flume is also used to import huge volumes of event data produced by social networking sites like Facebook and Twitter, and e-commerce websites like Amazon and Flipkart.</div>
</li>
<li class="level1"><div class="li"> Flume supports a large set of sources and destinations types.</div>
</li>
<li class="level1"><div class="li"> Flume supports multi-hop flows, fan-in fan-out flows, contextual routing, etc.</div>
</li>
<li class="level1"><div class="li"> Flume can be scaled horizontally.</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Features of Flume&quot;,&quot;hid&quot;:&quot;features_of_flume&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:6,&quot;range&quot;:&quot;1546-2205&quot;} -->
<h2 class="sectionedit7" id="why_we_need_flume_in_hadoop_eco-system">Why we need flume in hadoop eco-system?</h2>
<div class="level2">

<p>
As we know, <strong>Hadoop File System Shell/<abbr title="Application Programming Interface">API</abbr></strong> provides put functions to upload data into HDFS. But it suffers from the following drawbacks
</p>
<ul>
<li class="level1"><div class="li">  Using put, we can transfer only one file at a time while the data generators generate data at a much higher rate. Since the analysis made on older data is less accurate, we need to have a solution to transfer data in real time.</div>
</li>
<li class="level1"><div class="li"> If we use put command, the data is needed to be packaged and should be ready for the upload. Since the web servers generate data continuously, it is a very difficult task.</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Why we need flume in hadoop eco-system?&quot;,&quot;hid&quot;:&quot;why_we_need_flume_in_hadoop_eco-system&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:7,&quot;range&quot;:&quot;2206-2806&quot;} -->
<h3 class="sectionedit8" id="problem_with_hdfs">Problem with HDFS</h3>
<div class="level3">

<p>
In HDFS, the file exists as a directory entry and the length of the file will be considered as zero till it is closed. For example, if a source is writing data into HDFS and the network was interrupted in the middle of the operation (without closing the file), then the data written in the file will be lost.
</p>

<p>
Therefore we need a reliable, configurable, and maintainable system to transfer the log data into HDFS.
</p>

<p>
Note − In POSIX file system, whenever we are accessing a file (say performing write operation), other programs can still read this file (at least the saved portion of the file). This is because the file exists on the disc before it is closed.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Problem with HDFS&quot;,&quot;hid&quot;:&quot;problem_with_hdfs&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:8,&quot;range&quot;:&quot;2807-3496&quot;} -->
<h2 class="sectionedit9" id="flume_architecture">Flume Architecture</h2>
<div class="level2">

<p>
Flume has three layers :
</p>
<ul>
<li class="level1"><div class="li"> <strong>Agent</strong> → An agent is an independent process (JVM) in Flume. It collects data from clients or other agents and sends them to the next destination(sink or agent).</div>
</li>
<li class="level1"><div class="li"> <strong>Collector</strong> → Collector can combine data from one or many agents, and send it to storage</div>
</li>
<li class="level1"><div class="li"> <strong>Storage</strong> → Storage can be a file, hdfs, Hive, HBase, etc.</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Flume Architecture&quot;,&quot;hid&quot;:&quot;flume_architecture&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:9,&quot;range&quot;:&quot;3497-3885&quot;} -->
<h3 class="sectionedit10" id="flume-agent">Flume-Agent</h3>
<div class="level3">

<p>
An agent is an independent daemon process (JVM) in Flume. It receives the data (<strong>events</strong>) from clients(aka. <strong>sources</strong>) or other agents and forwards it to its next destination (<strong>sinks</strong> or agent) via <strong>channels</strong>. Flume may have more than one agent. In one agent, you have the following key components:
</p>
<ul>
<li class="level1"><div class="li"> <strong>Source</strong> → A source is the component of an Agent which receives data from the data generators and transfers it to one or more channels in the form of Flume events. Apache Flume supports several types of sources and each source receives events from a specified data generator. (e.g. Avro source, Thrift source, twitter 1% source)</div>
</li>
<li class="level1"><div class="li"> <strong>Channel</strong> → A channel is a transient store which receives the events from the source and buffers them till they are consumed by sinks. It acts as a bridge between the sources and the sinks. These channels are fully transactional and they can work with any number of sources and sinks. (e.g. JDBC channel, File system channel, Memory channel, etc.)</div>
</li>
<li class="level1"><div class="li"> <strong>Sink</strong> → A sink stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels and delivers it to the destination. The destination of the sink might be another agent or the central stores. (e.g. HDFS sink)</div>
</li>
</ul>

<p>
The data in flume is represented as <strong>flume event</strong>, it contains a body and a set of headers. The body of the event is a byte array that usually is the payload that Flume is transporting. The headers are represented as a map with string keys and string values. Headers are not meant to transfer data, but for routing purposes and to keep track of priority, the severity of events being sent, etc. The headers can be used to add event IDs or UUIDs to events as well.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Flume-Agent&quot;,&quot;hid&quot;:&quot;flume-agent&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:10,&quot;range&quot;:&quot;3886-5635&quot;} -->
<h3 class="sectionedit11" id="additional_components_of_flume_agent">Additional Components of Flume agent</h3>
<div class="level3">

<p>
What we have discussed above are the primitive components of the agent. In addition to this, we have a few more components that play a vital role in transferring the events from the data generator to the centralized stores.
</p>

</div>

<h4 id="interceptor">Interceptor</h4>
<div class="level4">

<p>
Interceptors are used to alter/inspect flume events which are transferred between source and channel.
</p>

</div>

<h4 id="channel_selectors">Channel Selectors</h4>
<div class="level4">

<p>
These are used to determine which channel is to be opted to transfer the data in case of multiple channels. There are two types of channel selectors 
</p>
<ul>
<li class="level1"><div class="li"> <strong>Default channel selectors</strong> − These are also known as replicating channel selectors they replicate all the events in each channel.</div>
</li>
<li class="level1"><div class="li"> <strong>Multiplexing channel selectors</strong> − These decides the channel to send an event based on the address in the header of that event.</div>
</li>
</ul>

</div>

<h4 id="sink_processors">Sink Processors</h4>
<div class="level4">

<p>
These are used to invoke a particular sink from the selected group of sinks. These are used to create failover paths for your sinks or load balance events across multiple sinks from a channel.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Additional Components of Flume agent&quot;,&quot;hid&quot;:&quot;additional_components_of_flume_agent&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:11,&quot;range&quot;:&quot;5636-6704&quot;} -->
<h2 class="sectionedit12" id="flume_data_flow">Flume Data flow</h2>
<div class="level2">

<p>
There are three key steps in flume data flow mechanism:
</p>
<ol>
<li class="level1"><div class="li"> Generally, events and log data are generated by the log servers and these servers have Flume agents running on them. These agents receive data from the data generators. Inside an agent, sources write the data into channels using channel processors, interceptors and selectors. Each and every source has its own channel processor, which takes the task given by the source and then passes that task or events to one or more interceptors. Interceptors read the event and modify or drop the event based on some criteria like regex. We can have multiple interceptors which are called in the order in which they are defined. This can be called a chain-of-responsibility design pattern. Then we pass that list of events generated by interceptor chain to channel selector. The selectors decide which channels attached to this source each event be written to.</div>
</li>
<li class="level1"><div class="li"> The data in these agents will be collected by an intermediate node known as Collector. Just like agents, there can be multiple collectors in Flume.</div>
</li>
<li class="level1"><div class="li"> Finally, the data from all these collectors will be aggregated and pushed to a centralized store such as HBase or HDFS. The following diagram explains the data flow in Flume.</div>
</li>
</ol>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Adata_science%3Aflume%3Aintroduction&amp;media=employes:pengfei.liu:data_science:flume:flume_general_dataflow.png" class="media" title="employes:pengfei.liu:data_science:flume:flume_general_dataflow.png"><img src="/lib/exe/fetch.php?w=400&amp;tok=ea4354&amp;media=employes:pengfei.liu:data_science:flume:flume_general_dataflow.png" class="media" alt="" width="400" /></a>
</p>

<p>
For example, suppose we have some web services generate events and log data. To upload these data into HDFS. First, we need to install and configure Flume agents on these servers.
</p>
<ol>
<li class="level1"><div class="li"> Flume agents running on the web services servers receive the data from the data generators. The data generators can be internal like apache log files or external like facebook and Twitter web services.</div>
</li>
<li class="level1"><div class="li"> The data in these agents will be collected by an intermediate node known as Collector. Just like agents, there can be multiple collectors in Flume.</div>
</li>
<li class="level1"><div class="li"> At last data from all collectors will be aggregated and pushed to a centralized store such as HBase or HDFS.</div>
</li>
</ol>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Flume Data flow&quot;,&quot;hid&quot;:&quot;flume_data_flow&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:12,&quot;range&quot;:&quot;6705-8709&quot;} -->
<h3 class="sectionedit13" id="failure_handling">Failure Handling</h3>
<div class="level3">

<p>
In Flume, for each event, two transactions take place: one at the sender and one at the receiver. The sender sends events to the receiver. Soon after receiving the data, the receiver commits its own transaction and sends a “received” signal to the sender. After receiving the signal, the sender commits its transaction. (Sender will not commit its transaction till it receives a signal from the receiver.)
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Failure Handling&quot;,&quot;hid&quot;:&quot;failure_handling&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:13,&quot;range&quot;:&quot;8710-9148&quot;} -->
<h3 class="sectionedit14" id="different_data_flow">Different data flow</h3>
<div class="level3">

</div>

<h4 id="multi-hop_flow">Multi-hop Flow</h4>
<div class="level4">

<p>
An event may travel through more than one agent before reaching final destination is called as Multi-hop Flow. 
</p>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Adata_science%3Aflume%3Aintroduction&amp;media=employes:pengfei.liu:data_science:flume:multi_hop.png" class="media" title="employes:pengfei.liu:data_science:flume:multi_hop.png"><img src="/lib/exe/fetch.php?w=400&amp;tok=e67269&amp;media=employes:pengfei.liu:data_science:flume:multi_hop.png" class="media" alt="" width="400" /></a>
</p>

</div>

<h4 id="fan-out_flow">Fan-out Flow</h4>
<div class="level4">

<p>
The dataflow from one source to multiple channels is known as fan-out flow. It is of two types:
</p>
<ul>
<li class="level1"><div class="li"> Replicating − The data flow where the data will be replicated in all the configured channels.</div>
</li>
<li class="level1"><div class="li"> Multiplexing − The data flow where the data will be sent to a selected channel which is mentioned in the header of the event.</div>
</li>
</ul>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Adata_science%3Aflume%3Aintroduction&amp;media=employes:pengfei.liu:data_science:flume:fan-out.png" class="media" title="employes:pengfei.liu:data_science:flume:fan-out.png"><img src="/lib/exe/fetch.php?w=400&amp;tok=f72a05&amp;media=employes:pengfei.liu:data_science:flume:fan-out.png" class="media" alt="" width="400" /></a>
</p>

</div>

<h4 id="fan-in_flow">Fan-in Flow</h4>
<div class="level4">

<p>
The data flow in which the data will be transferred from many sources to one channel is known as fan-in flow.
</p>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Adata_science%3Aflume%3Aintroduction&amp;media=employes:pengfei.liu:data_science:flume:fan-in.png" class="media" title="employes:pengfei.liu:data_science:flume:fan-in.png"><img src="/lib/exe/fetch.php?w=400&amp;tok=b514da&amp;media=employes:pengfei.liu:data_science:flume:fan-in.png" class="media" alt="" width="400" /></a>
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Different data flow&quot;,&quot;hid&quot;:&quot;different_data_flow&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:14,&quot;range&quot;:&quot;9149-&quot;} -->