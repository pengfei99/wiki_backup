a:87:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:43:"Install a devlopment envirionment for spark";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:60;}i:4;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:37:"Spark dev env with Intellij and Maven";i:1;i:2;i:2;i:60;}i:2;i:60;}i:5;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:60;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:60;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:"1. Pre-requise";}i:2;i:111;}i:8;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:125;}i:9;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:125;}i:10;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:52:"Download a IntelliJ IDEA, install jdk and scala sdk,";}i:2;i:127;}i:11;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:179;}i:12;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:179;}i:13;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:39:"First install scala plugin in IntelliJ.";}i:2;i:181;}i:14;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:220;}i:15;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:220;}i:16;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:45:"Then you add jdk and scala sdk into Intellij.";}i:2;i:222;}i:17;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:267;}i:18;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:267;}i:19;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:67:"To add jdk and scala sdk into IntelliJ, you open IntelliJ click on ";}i:2;i:269;}i:20;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:5:"color";i:1;a:2:{i:0;i:1;i:1;a:2:{i:0;s:14:"color:#ed1c24;";i:1;s:0:"";}}i:2;i:1;i:3;s:15:"<color #ed1c24>";}i:2;i:336;}i:21;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:5:"color";i:1;a:2:{i:0;i:3;i:1;s:10:"Configure ";}i:2;i:3;i:3;s:10:"Configure ";}i:2;i:351;}i:22;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:361;}i:23;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:5:"color";i:1;a:2:{i:0;i:3;i:1;s:17:" Project default ";}i:2;i:3;i:3;s:17:" Project default ";}i:2;i:363;}i:24;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:380;}i:25;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:5:"color";i:1;a:2:{i:0;i:3;i:1;s:18:" Project structure";}i:2;i:3;i:3;s:18:" Project structure";}i:2;i:382;}i:26;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:5:"color";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</color>";}i:2;i:400;}i:27;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:68:" . After this it should open a window, now click on Global libraries";}i:2;i:408;}i:28;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"->";}i:2;i:476;}i:29;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:66:" +, choose the location where you have installed your jdk and sdk.";}i:2;i:478;}i:30;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:544;}i:31;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:544;}i:32;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:70:"After all this, you have a Intellij for scala development environment.";}i:2;i:546;}i:33;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:616;}i:34;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:616;}i:35;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:31:"To create a spark-scala project";}i:2;i:618;}i:36;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:649;}i:37;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:649;}i:38;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"You need";}i:2;i:651;}i:39;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:659;}i:40;a:3:{i:0;s:10:"listo_open";i:1;a:0:{}i:2;i:659;}i:41;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:659;}i:42;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:659;}i:43;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:" Create a Intellij scala project";}i:2;i:664;}i:44;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:696;}i:45;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:696;}i:46;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:696;}i:47;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:696;}i:48;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:43:" add maven framework support to the project";}i:2;i:700;}i:49;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:743;}i:50;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:743;}i:51;a:3:{i:0;s:11:"listo_close";i:1;a:0:{}i:2;i:743;}i:52;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:743;}i:53;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:49:"In the main/java folder create the following file";}i:2;i:745;}i:54;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:800;}i:55;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:520:"
import org.apache.spark.{SparkConf, SparkContext}


object WordCount {

  def main(args: Array[String]): Unit ={
    val inputFile = "file:///tmp/word.txt"
    val conf = new SparkConf().setAppName("WordCount").setMaster("local")
    val sc = new SparkContext(conf)
    val textFile = sc.textFile(inputFile)
    val wordCount = textFile.flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey((a,b)=>a+b)
    wordCount.foreach(println)
    wordCount.saveAsTextFile("file:///tmp/wordCount")
    sc.close()
  }

}
";i:1;s:5:"scala";i:2;s:15:"WordCount.scala";}i:2;i:800;}i:56;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:800;}i:57;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:"Change the pom.xml as following ";}i:2;i:1352;}i:58;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1384;}i:59;a:3:{i:0;s:4:"file";i:1;a:3:{i:0;s:2774:"
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.pengfei</groupId>
    <artifactId>spark-tutor</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <spark.version>2.1.0</spark.version>
        <scala.version>2.11</scala.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>

            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.19</version>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>

        </plugins>
    </build>
</project>
";i:1;s:3:"xml";i:2;s:7:"pom.xml";}i:2;i:1391;}i:60;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1391;}i:61;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:129:"In intelliJ, you can write click on the .scala file and choose Run <file-name>. It will lauch the scala script on spark cluster. ";}i:2;i:4186;}i:62;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4315;}i:63;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4315;}i:64;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:89:"You can also use maven to build a jar file of project WordCount, we name it wordCount.jar";}i:2;i:4317;}i:65;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4406;}i:66;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4406;}i:67;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:"To run it on spark with mode cluster ";}i:2;i:4408;}i:68;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4445;}i:69;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:64:"
spark-submit --class WordCount --master local[*] wordCount.jar
";i:1;N;i:2;N;}i:2;i:4452;}i:70;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:4526;}i:71;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:40:"Launching Applications with spark-submit";i:1;i:2;i:2;i:4526;}i:2;i:4526;}i:72;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:4526;}i:73;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4526;}i:74;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:112:"The jar can be build based on scala or java source, In this section, we exam the spark-submit with more details.";}i:2;i:4580;}i:75;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4692;}i:76;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:200:"
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
";i:1;N;i:2;N;}i:2;i:4701;}i:77;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4701;}i:78;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:38:"Some of the commonly used options are:";}i:2;i:4911;}i:79;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4949;}i:80;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:777:"
  * --class: The entry point for your application (e.g. org.apache.spark.examples.SparkPi)
  * --master: The master URL for the cluster (e.g. ''spark://23.195.26.187:7077'')
  * --deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client)
  * --conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown).
  * application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.
  * application-arguments: Arguments passed to the main method of your main class, if any
";i:1;N;i:2;N;}i:2;i:4956;}i:81;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4956;}i:82;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:127:"The following command uses a jar file came from spark distribution and call a sparkPi class, it prints a pi value in the output";}i:2;i:5744;}i:83;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5871;}i:84;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:141:"
spark-submit --class org.apache.spark.examples.SparkPi --master local[*] /opt/spark/spark-2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar
";i:1;N;i:2;N;}i:2;i:5879;}i:85;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6028;}i:86;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:6028;}}