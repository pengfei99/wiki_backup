a:133:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:21:"Installation of sqoop";i:1;i:1;i:2;i:1;}i:2;i:1;}i:2;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:3;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1;}i:4;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:152:"Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.";}i:2;i:38;}i:5;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:190;}i:6;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:193;}i:7;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Pre-requise";i:1;i:2;i:2;i:193;}i:2;i:193;}i:8;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:193;}i:9;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:193;}i:10;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:97:"Sqoop needs Hadoop as dependencies, so before you install sqoop, you need to have a hadoop infra.";}i:2;i:218;}i:11;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:315;}i:12;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:315;}i:13;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:67:"In our case, we have 2 datanode and 1 name node. For more info see ";}i:2;i:317;}i:14;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:29:"employes:pengfei.liu:big_data";i:1;s:34:"Install hdfs on multi node cluster";}i:2;i:384;}i:15;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:452;}i:16;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:452;}i:17;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:85:"In this tutorial, we will install sqoop-1.4.6 on the name node(hadoop-nn.pengfei.org)";}i:2;i:455;}i:18;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:540;}i:19;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:540;}i:20;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:"The latest version can be found here ";}i:2;i:542;}i:21;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:25:"https://sqoop.apache.org/";i:1;N;}i:2;i:579;}i:22;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:604;}i:23;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:606;}i:24;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"Installation";i:1;i:2;i:2;i:606;}i:2;i:606;}i:25;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:606;}i:26;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:606;}i:27;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:53:"==== untar the folder and copy it to /opt/sqoop
 ====";}i:2;i:632;}i:28;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:685;}i:29;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:147:"
mkdir -p /opt/sqoop

tar -xzvf  sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz

mv  sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /opt/sqoop/sqoop-1.4.6
";i:1;N;i:2;N;}i:2;i:692;}i:30;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:692;}i:31;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:123:"change owner to hadoop, it's not required, but as I install hadoop by using user hadoop, I'd like also run sqoop as hadoop.";}i:2;i:849;}i:32;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:972;}i:33;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:47:"
chown -R hadoop:hadoop /opt/sqoop/sqoop-1.4.6
";i:1;N;i:2;N;}i:2;i:979;}i:34;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:979;}i:35;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:28:"==== 
Add sqoop to path ====";}i:2;i:1037;}i:36;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1065;}i:37;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1065;}i:38;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:167:"I prefer to add the export to the /etc/profil.d/..
So it will be valid for all users. You can also add it to your user .bashrc, but it will bi only valid for the user.";}i:2;i:1067;}i:39;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1234;}i:40;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:238:"
vim /etc/profil.d/sqoop.sh

 ***Append the below at the end of the file***
  export SQOOP_HOME=/opt/sqoop/sqoop-1.4.6
  export PATH=$SQOOP_HOME/bin:$PATH
  ***Save and close the file; return to terminal***

source /etc/profil.d/sqoop.sh
";i:1;N;i:2;N;}i:2;i:1241;}i:41;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1489;}i:42;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Add jdbc driver/connector";i:1;i:3;i:2;i:1489;}i:2;i:1489;}i:43;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1489;}i:44;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1489;}i:45;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:98:"If you want to use sqoop to connect to mysql/mariadb, you need to download the appropriate driver.";}i:2;i:1526;}i:46;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1624;}i:47;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1624;}i:48;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:33:"For example, for mysql,plz go to ";}i:2;i:1626;}i:49;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:41:"https://www.mysql.com/products/connector/";i:1;N;}i:2;i:1659;}i:50;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:1700;}i:51;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1701;}i:52;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1701;}i:53;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:"for postgresql plz go to ";}i:2;i:1703;}i:54;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:41:"https://jdbc.postgresql.org/download.html";i:1;N;}i:2;i:1728;}i:55;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1769;}i:56;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1769;}i:57;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:78:"After download the .jar file, you need to put it in /opt/sqoop/sqoop-1.4.6/lib";}i:2;i:1771;}i:58;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1849;}i:59;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1851;}i:60;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:38:"Verify if sqoop is installed correctly";i:1;i:3;i:2;i:1851;}i:2;i:1851;}i:61;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1851;}i:62;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:819:"

[hadoop@CCLinDataWHD01 lib]$ sqoop version
Warning: /opt/hadoop/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/hadoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/hadoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /opt/hadoop/sqoop/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
17/11/13 16:55:04 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
Sqoop 1.4.6
git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25
Compiled by root on Mon Apr 27 14:38:36 CST 2015

";i:1;N;i:2;N;}i:2;i:1906;}i:63;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1906;}i:64;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:82:"These warning are throwed, because of these tools are not installed on the server.";}i:2;i:2735;}i:65;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2817;}i:66;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2820;}i:67;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:14:"Sqoop Commands";i:1;i:1;i:2;i:2820;}i:2;i:2820;}i:68;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:2820;}i:69;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2820;}i:70;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:2850;}i:71;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:82:"1. To view the mysql files [mysql resides in local system , database name is test]";}i:2;i:2852;}i:72;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:2934;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:2936;}i:74;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2942;}i:75;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:101:"
$sqoop list-tables --connect jdbc:mysql://localhost:3306/test --username root --password password1!
";i:1;N;i:2;N;}i:2;i:2942;}i:76;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2942;}i:77;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3053;}i:78;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:106:"2. To import all tables [database name is hadoopdb, giving -P implies password to be given when prompted]
";}i:2;i:3055;}i:79;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3161;}i:80;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:3163;}i:81;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3169;}i:82;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:92:"
$sqoop import-all-tables --connect jdbc:mysql://localhost:3306/hadoopdb --username root -P
";i:1;N;i:2;N;}i:2;i:3169;}i:83;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3169;}i:84;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3271;}i:85;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:84:"3. To import a mysql table into hdfs [database name is hadoopdb, table name is demo]";}i:2;i:3273;}i:86;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3357;}i:87;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3359;}i:88;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:113:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password password1! --table demo
";i:1;N;i:2;N;}i:2;i:3366;}i:89;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3366;}i:90;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3489;}i:91;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:36:"4. To import a mysql table into hive";}i:2;i:3491;}i:92;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3527;}i:93;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3529;}i:94;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:127:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password password1! --table demo --hive-import
";i:1;N;i:2;N;}i:2;i:3536;}i:95;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3536;}i:96;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3673;}i:97;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:63:"5. To import table based on user defined condition into hive  [";}i:2;i:3675;}i:98;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"--";}i:2;i:3738;}i:99;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"m denotes the mappers]
";}i:2;i:3740;}i:100;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3763;}i:101;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:3765;}i:102;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3771;}i:103;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:161:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root -P --table demo1 --where "state like 'k%'" --m 3 --hive-import --hive-table kstate
";i:1;N;i:2;N;}i:2;i:3771;}i:104;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3771;}i:105;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3943;}i:106;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:182:"6. To import a table using split by option [mappers is decided based on the values in column specified in split by option, if you want to control the mappers then explicitly specify ";}i:2;i:3945;}i:107;a:3:{i:0;s:6:"entity";i:1;a:1:{i:0;s:2:"--";}i:2;i:4127;}i:108;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"m]
";}i:2;i:4129;}i:109;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4132;}i:110;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:4134;}i:111;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4140;}i:112;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:168:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password password1! --table demo1 --split-by state --hive-import --hive-table splittest
";i:1;N;i:2;N;}i:2;i:4140;}i:113;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4140;}i:114;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:4318;}i:115;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:"7. To import a table using query";}i:2;i:4320;}i:116;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4352;}i:117;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4354;}i:118;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:236:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password password1! --query "select * from demo1 where \$CONDITIONS order by name" --split-by state --hive-import --target-dir test --hive-table sorteddata
";i:1;N;i:2;N;}i:2;i:4361;}i:119;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4361;}i:120;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:4607;}i:121;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:86:"8. To import a table without hive delimiters [drops \n, \r and \01 from string fields]";}i:2;i:4609;}i:122;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4695;}i:123;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4697;}i:124;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:151:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password
password1! --table demo hive-import --hive-drop-import-delims
";i:1;N;i:2;N;}i:2;i:4704;}i:125;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4704;}i:126;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:4865;}i:127;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:55:"9. To import a table into hdfs with specific delimiters";}i:2;i:4867;}i:128;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4922;}i:129;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4924;}i:130;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:141:"
$sqoop import --connect jdbc:mysql://localhost:3306/hadoopdb --username root --password password1! --table demo --fields-terminated-by "||"
";i:1;N;i:2;N;}i:2;i:4931;}i:131;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:5080;}i:132;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:5080;}}