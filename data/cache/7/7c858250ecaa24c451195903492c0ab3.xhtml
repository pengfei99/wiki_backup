
<h1 class="sectionedit1" id="install_and_configure_hive">Install and configure Hive</h1>
<div class="level1">

</div>
<!-- EDIT1 SECTION "Install and configure Hive" [1-42] -->
<h2 class="sectionedit2" id="introduction_of_hive">Introduction of Hive</h2>
<div class="level2">

<p>
Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.
</p>

<p>
Initially Hive was developed by Facebook, later the Apache Software Foundation took it up and developed it further as an open source under the name Apache Hive. It is used by different companies. For example, Amazon uses it in Amazon Elastic MapReduce.
</p>

<p>
Hive is not :
</p>
<ol>
<li class="level1"><div class="li"> A relational database</div>
</li>
<li class="level1"><div class="li"> A design for OnLine Transaction Processing (OLTP)</div>
</li>
<li class="level1"><div class="li"> A language for real-time queries and row-level updates</div>
</li>
</ol>

<p>
Features of Hive :
</p>
<ol>
<li class="level1"><div class="li"> It stores schema in a database and processed data into HDFS.</div>
</li>
<li class="level1"><div class="li"> It is designed for OLAP.</div>
</li>
<li class="level1"><div class="li"> It provides SQL type language for querying called HiveQL or HQL.</div>
</li>
<li class="level1"><div class="li"> It is familiar, fast, scalable, and extensible.</div>
</li>
</ol>

</div>
<!-- EDIT2 SECTION "Introduction of Hive" [43-894] -->
<h3 class="sectionedit3" id="architecture_of_hive">Architecture of Hive</h3>
<div class="level3">

<p>
The following component diagram depicts the architecture of Hive:
</p>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Abig_data%3Ahive%3Ainstall_config&amp;media=employes:pengfei.liu:big_data:hive:hive_architecture.jpg" class="media" title="employes:pengfei.liu:big_data:hive:hive_architecture.jpg"><img src="/lib/exe/fetch.php?w=400&amp;tok=a48ad9&amp;media=employes:pengfei.liu:big_data:hive:hive_architecture.jpg" class="media" alt="" width="400" /></a>
</p>

<p>
<strong>User Interface</strong> : → Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server). 
</p>

<p>
Meta Store : → Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping.
</p>

<p>
HiveQL Process Engine : → HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it.
</p>

<p>
Execution Engine : → The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce.
</p>

<p>
HDFS or HBASE : → Hadoop distributed file system or HBASE are the data storage techniques to store data into file system.
</p>

</div>
<!-- EDIT3 SECTION "Architecture of Hive" [895-2103] -->
<h3 class="sectionedit4" id="working_of_hive">Working of Hive</h3>
<div class="level3">

<p>
The following diagram depicts the workflow between Hive and Hadoop
</p>

<p>
<a href="/lib/exe/detail.php?id=employes%3Apengfei.liu%3Abig_data%3Ahive%3Ainstall_config&amp;media=employes:pengfei.liu:big_data:hive:how_hive_works.jpg" class="media" title="employes:pengfei.liu:big_data:hive:how_hive_works.jpg"><img src="/lib/exe/fetch.php?w=400&amp;tok=410318&amp;media=employes:pengfei.liu:big_data:hive:how_hive_works.jpg" class="media" alt="" width="400" /></a>
</p>

<p>
Step 1 → Execute Query
The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.
</p>

<p>
Step 2 → Get Plan
The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.
</p>

<p>
Step 3 → Get Metadata
The compiler sends metadata request to Metastore (any database).
</p>

<p>
Step 4 → Send Metadata
Metastore sends metadata as a response to the compiler.
</p>

<p>
Step 5 → Send Plan
The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.
</p>

<p>
Step 6→ Execute Plan
The driver sends the execute plan to the execution engine.
</p>

<p>
Step 7 → Execute Job
Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.
</p>

<p>
Step 7.1 → Metadata Ops
Meanwhile in execution, the execution engine can execute metadata operations with Metastore.
</p>

<p>
Step 8 → Fetch Result
The execution engine receives the results from Data nodes.
</p>

<p>
Step 9 → Send Results
The execution engine sends those resultant values to the driver.
</p>

<p>
Step 10 → Send Results
The driver sends the results to Hive Interfaces.
</p>

</div>
<!-- EDIT4 SECTION "Working of Hive" [2104-3609] -->
<h2 class="sectionedit5" id="installation">Installation</h2>
<div class="level2">

<p>
As we said before, Hive works on top of Hadoop, so before you install hive, you need to install Hadoop first. <a href="/doku.php?id=employes:pengfei.liu:big_data:hadoop:install_config_hadoop" class="wikilink1" title="employes:pengfei.liu:big_data:hadoop:install_config_hadoop">Install hdfs on multi node cluster</a>
</p>

<p>
After you install java, hadoop, you can follow this guide to download the latest stable version.
</p>

<p>
<a href="https://hive.apache.org/downloads.html" class="urlextern" title="https://hive.apache.org/downloads.html" rel="nofollow">https://hive.apache.org/downloads.html</a>
</p>

<p>
The current stable version is 2.3.2
</p>

</div>
<!-- EDIT5 SECTION "Installation" [3610-4019] -->
<h3 class="sectionedit6" id="install_hive">Install hive</h3>
<div class="level3">
<pre class="code">mkdir -p /opt/hive

#put the download tar.gz and untar it and rename it.
#Suppose we call it hive-2.3.2

#home of hive
/opt/hive/hive-2.3.2 

#create environment variable
vim /etc/profile.d/hive.sh

#put the following lines
#!/bin/bash

export HIVE_HOME=/opt/hive/hive-2.3.2
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:.

# check your hadoop environment, if hadoop home is not set add the following
export HADOOP_HOME=/opt/hadoop/hadoop-2.8.2
export HADOOP_CONF_DIR=/opt/hadoop/hadoop-2.8.2/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.
</pre>

</div>
<!-- EDIT6 SECTION "Install hive" [4020-4698] -->
<h3 class="sectionedit7" id="install_rmdb_for_hive_metastore">Install RMDB for hive metastore</h3>
<div class="level3">

<p>
Now hive is installed, but hive metastore needs a RMDB to store metadata. So we need to install a RMDB. In our case, we installed a postgresql server. Follow this doc to install it <a href="/doku.php?id=employes:pengfei.liu:admin_system:centos7_postgres95" class="wikilink1" title="employes:pengfei.liu:admin_system:centos7_postgres95">Install postgresql on centos 7</a>  .
</p>

<p>
Suppose you already have postgresql server installed
</p>
<pre class="code"># create user hive with password hive in postgres

$ sudo -u postgres psql

postgres=# CREATE USER hive WITH PASSWORD &#039;hive&#039;;

# create database metastore
postgres=# CREATE DATABASE metastore;

# grant all privilege of database metastore to user hive
GRANT ALL PRIVILEGES ON DATABASE metastore TO hive;

# test connectivity
psql -h 127.0.0.1 -p 5432 -U hive -d metastore

# Copy jdbc driver to hive lib, in this example, we use postgresql driver
cp postgresql-42.2.5.jar /opt/hive/hive-2.3.3/lib/.</pre>

</div>
<!-- EDIT7 SECTION "Install RMDB for hive metastore" [4699-5582] -->
<h3 class="sectionedit8" id="configure_hive_to_use_hdfs_and_metastore">Configure hive to use hdfs and metastore</h3>
<div class="level3">

<p>
There are too main conf file you need to modify
</p>

<p>
The hive-env.sh  conf
</p>
<pre class="code">cp hive-env.sh.template hive-env.sh

vim /opt/hive/hive/hive-2.3.2/hive-env.sh

#put the following lines
# The heap size of the jvm stared by hive shell script can be controlled via:
#
export HADOOP_HEAPSIZE=1024
# Set HADOOP_HOME to point to a specific hadoop install directory
HADOOP_HOME=/opt/hadoop/hadoop

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/hive/hive-2.3.2/conf
</pre>

<p>
The hive-site.xml
</p>
<pre class="code">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;

&lt;property&gt;
    &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
    &lt;value&gt;/tmp/hive&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
    &lt;value&gt;hdfs://127.0.0.1:9000/user/hive/warehouse&lt;/value&gt;
    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:postgresql://127.0.0.1:5432/metastore&lt;/value&gt;
&lt;/property&gt;
 
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;org.postgresql.Driver&lt;/value&gt;
&lt;/property&gt;
 
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;hive&lt;/value&gt;
&lt;/property&gt;
 
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;hive&lt;/value&gt;
&lt;/property&gt;


&lt;property&gt;
    &lt;name&gt;javax.jdo.option.Multithreaded&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;


&lt;/configuration&gt;

</pre>

<p>
hive.exec.scratchdir → defines the location of temporal data during the Hive mapr operation
</p>

<p>
hive.metastore.warehouse.dir → defines the location of hive data warehouse
</p>

<p>
The rest is to connect sql server
</p>

</div>
<!-- EDIT8 SECTION "Configure hive to use hdfs and metastore" [5583-7352] -->
<h3 class="sectionedit9" id="create_dir_in_hdfs">Create dir in hdfs</h3>
<div class="level3">
<pre class="code">
# create dir in hdfs
hdfs dfs -mkdir /tmp/hive
hdfs dfs -mkdir /user/hive/warehouse

#grant access in hdfs
hdfs dfs -chmod g+w /tmp
hdfs dfs -chmod g+w /user/hive/warehouse
</pre>

<p>
Now make sure hive can access hdfs, On node where you are running hive service, add below property in your hdfs config core-site.xml
</p>
<pre class="code">&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.$user_name.hosts&lt;/name&gt;
     &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.$user_name.groups&lt;/name&gt;
     &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;</pre>

</div>
<!-- EDIT9 SECTION "Create dir in hdfs" [7353-7918] -->
<h3 class="sectionedit10" id="initial_hive_db_structure">Initial hive DB structure</h3>
<div class="level3">
<pre class="code">cd /opt/hive/hive-2.3.2
/bin/schematool -dbType postgres -initSchema</pre>

</div>
<!-- EDIT10 SECTION "Initial hive DB structure" [7919-8040] -->
<h3 class="sectionedit11" id="run_hive_in_console">Run hive in console</h3>
<div class="level3">

<p>
Before you run hive, you need to make sure your hdfs and yarn is activate
</p>
<pre class="code">[hadoop@localhost hadoop]$ jps
8548 Jps
8329 RunJar
4986 NameNode
6170 ResourceManager
5371 SecondaryNameNode
6285 NodeManager
5119 DataNode</pre>

<p>
You also need to make sure your meta data store is activate. In our case , it&#039;s postgresql
</p>
<pre class="code">[pliu@localhost taobao_data_set]$ sudo systemctl status postgresql-9.5.service 
● postgresql-9.5.service - PostgreSQL 9.5 database server
   Loaded: loaded (/usr/lib/systemd/system/postgresql-9.5.service; disabled; vendor preset: disabled)
   Active: active (running) since Wed 2018-02-14 16:01:19 CET; 5min ago
     Docs: https://www.postgresql.org/docs/9.5/static/</pre>

<p>
If you have added hive home in your path you can just run hive
</p>
<pre class="code">[hadoop@localhost bin]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/hive/hive-2.3.2/lib/hive-common-2.3.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive&gt; show tables;
OK
customercustomerdemo
customerdemographics
......</pre>

</div>
<!-- EDIT11 SECTION "Run hive in console" [8041-9751] -->
<h3 class="sectionedit12" id="run_metastore_and_hive_as_daemons">Run metastore and hive as daemons</h3>
<div class="level3">
<pre class="code">hive --service metastore --hiveconf hive.log.dir=$HIVE_HOME/logs --hiveconf hive.log.file=metastore.log &gt;/dev/null 2&gt;&amp;1 &amp;

hive --service hiveserver2 --hiveconf hive.log.dir=$HIVE_HOME/logs --hiveconf hive.log.file=hs2.log &gt;/dev/null 2&gt;&amp;1 &amp;</pre>

</div>
<!-- EDIT12 SECTION "Run metastore and hive as daemons" [9752-10054] -->
<h3 class="sectionedit13" id="test_hive_operation">Test hive operation</h3>
<div class="level3">
<pre class="code">hive&gt; create database hive;
OK

hive&gt; CREATE TABLE IF NOT EXISTS employees ( eid int, name String,salary String, destination String) COMMENT &#039;Employee details&#039; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#039;\t&#039; LINES TERMINATED BY &#039;\n&#039; STORED AS TEXTFILE;
OK
Time taken: 0.295 seconds
</pre>

<p>
We will use the following sample data to learn some basic operation in hive.
If you want to copy it, make sure the white space is &#039;\t&#039; not other things 
</p>
<pre class="code">1201	Gopal	45000	Technical_manager
1202	Manisha	45000	Proof_reader
1203	Masthanvali	40000	Technical_writer
1204	Kiran	40000	Hr_Admin
1205	Kranthi	30000	Op_Admin</pre>

</div>
<!-- EDIT13 SECTION "Test hive operation" [10055-10714] -->
<h3 class="sectionedit14" id="load_data_into_table">Load data into table</h3>
<div class="level3">
<pre class="code">hive&gt; LOAD DATA LOCAL INPATH &#039;/tmp/hive_test_data/sample.txt&#039; OVERWRITE INTO TABLE employees;
Loading data to table default.employees
OK
Time taken: 3.614 seconds

#Test your loaded data
hive&gt; select * from employees where salary &gt; 40000;
OK
1201	Gopal	45000	Technical_manager
1202	Manisha	45000	Proof_reader
Time taken: 0.361 seconds, Fetched: 2 row(s)
</pre>

</div>
<!-- EDIT14 SECTION "Load data into table" [10715-11117] -->
<h2 class="sectionedit15" id="use_beeline_to_access_hive">Use beeline to access hive</h2>
<div class="level2">
<pre class="code"># open a beeline terminal
beeline
# connect to hive 
!connect jdbc:hive2://localhost:10000/default

# enter username and password, this username will be used by hive to connect to hdfs, in our example, I use hadoop as username, and empty password

# if you see the following error, it means your hive cannot use the input username to connect hdfs
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate  (state=08S01,code=0)

# To solve the above error, you need to reconfig hdfs to accept connection of user hadoop
vim core-site.xml

# add the following line
&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; 
     &lt;value&gt;*&lt;/value&gt; 
&lt;/property&gt; 
&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;
     &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;

#note that the above line only works for user hadoop, if you want to use foobar as username, you need to add another lines, I tried with generic config hadoop.proxyuser.$user_name.hosts, it doesnot work</pre>

</div>
<!-- EDIT15 SECTION "Use beeline to access hive" [11118-12362] -->
<h2 class="sectionedit16" id="remote_access_of_hive">Remote access of hive</h2>
<div class="level2">

<p>
If you want to use hive client to access a remote hive server, you need to active the hive server daemon.
for hive 1.x, the service name is hiveserver. For hive 2.x, the service name is hiveserver2
</p>
<pre class="code">&gt; hive --service hiveserver2

2018-08-17 09:35:44: Starting HiveServer2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
</pre>

</div>
<!-- EDIT16 SECTION "Remote access of hive" [12363-13170] -->
<h2 class="sectionedit17" id="multi_node_hive_setup">Multi node hive setup</h2>
<div class="level2">

<p>
As all metadata of hive is stored in hive metastore(physically it&#039;s a relational database, in our case, it&#039;s postgresql). So if your node2 is not using metastore of node1. node2 can&#039;t read the table of node1.
</p>

<p>
First, you need to start the metastore service in node1 
</p>
<pre class="code"># start the metastore service, the default port is 9083, you can specify the port by using option -p
hive --service metastore

#check the port
netstat -ln | grep 9083</pre>

<p>
Seconde, in node2 we need to setup hive-site.xml for node2 can connect to metastore of node1.
</p>
<pre class="code">&lt;property&gt;
   &lt;name&gt;hive.metastore.uris&lt;/name&gt;
   &lt;value&gt;thrift://node1(or IP Address):9083&lt;/value&gt;
   &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt;
&lt;/property&gt;</pre>

<p>
Now in node 2, with hive-cli, we can access the tables in node1.
</p>

</div>
<!-- EDIT17 SECTION "Multi node hive setup" [13171-14051] -->
<h1 class="sectionedit18" id="faqs">FAQs</h1>
<div class="level1">
<pre class="code"># 1. When you enter hive, if you see this exception
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D

# solution: You need to add the following line at the beginning of hive-site.xml

&lt;property&gt;
    &lt;name&gt;system:java.io.tmpdir&lt;/name&gt;
    &lt;value&gt;/tmp/hive/java&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;system:user.name&lt;/name&gt;
    &lt;value&gt;${user.name}&lt;/value&gt;
  &lt;/property&gt;
# This is caused by the following config in the hive-site.xml
&lt;property&gt;
    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
    &lt;value&gt;${system:java.io.tmpdir}/${system:user.name}&lt;/value&gt;
    &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
</pre>

</div>
<!-- EDIT18 SECTION "FAQs" [14052-14835] -->
<h2 class="sectionedit19" id="access_control">Access control</h2>
<div class="level2">

<p>
To control which user can use hive, add the following line in hive-site.xml
</p>
<pre class="code">&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.$user_name.hosts&lt;/name&gt;
     &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
     &lt;name&gt;hadoop.proxyuser.$user_name.groups&lt;/name&gt;
     &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;</pre>

<p>
$user_name - specify the username which will have access to hive beeline command line
</p>

</div>
<!-- EDIT19 SECTION "Access control" [14836-] -->