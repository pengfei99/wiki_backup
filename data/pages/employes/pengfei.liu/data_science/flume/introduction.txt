====== Flume Introduction ======

===== What is Flume? =====

Apache Flume is a tool/service/data ingestion mechanism for collecting aggregating and transporting large amounts of streaming data such as log files, events (etc...) from various sources to a centralized data store.

Flume is a highly reliable, distributed, and configurable tool. It is principally designed to copy streaming data (log data) from various web servers to HDFS.

===== Application of Flume =====

Assume an e-commerce web application wants to analyze the customer behavior from a particular region. To do so, they would need to move the available log data into Hadoop for analysis. Here, Apache Flume comes to our rescue.

Flume is used to move the log data generated by application servers into HDFS at a higher speed.

===== Advantages and features of Flume =====

==== Advantages of Flume ====

Here are the advantages of using Flume:

  * Using Apache Flume we can store the data into any of the centralized stores (HBase, HDFS).
  * When the rate of incoming data exceeds the rate at which data can be written to the destination, Flume acts as a mediator between data producers and the centralized stores and provides a steady flow of data between them.
  * Flume provides the feature of contextual routing.
  * The transactions in Flume are channel-based where two transactions (one sender and one receiver) are maintained for each message. It guarantees reliable message delivery.
  * Flume is reliable, fault tolerant, scalable, manageable, and customizable.

==== Features of Flume ====

Some of the notable features of Flume are as follows:

  * Flume ingests log data from multiple web servers into a centralized store (HDFS, HBase) efficiently.
  * Using Flume, we can get the data from multiple servers immediately into Hadoop.
  * Along with the log files, Flume is also used to import huge volumes of event data produced by social networking sites like Facebook and Twitter, and e-commerce websites like Amazon and Flipkart.
  * Flume supports a large set of sources and destinations types.
  * Flume supports multi-hop flows, fan-in fan-out flows, contextual routing, etc.
  * Flume can be scaled horizontally.

===== Why we need flume in hadoop eco-system? =====

As we know, **Hadoop File System Shell/API** provides put functions to upload data into HDFS. But it suffers from the following drawbacks
  *  Using put, we can transfer only one file at a time while the data generators generate data at a much higher rate. Since the analysis made on older data is less accurate, we need to have a solution to transfer data in real time.
  * If we use put command, the data is needed to be packaged and should be ready for the upload. Since the web servers generate data continuously, it is a very difficult task.

==== Problem with HDFS ====

In HDFS, the file exists as a directory entry and the length of the file will be considered as zero till it is closed. For example, if a source is writing data into HDFS and the network was interrupted in the middle of the operation (without closing the file), then the data written in the file will be lost.

Therefore we need a reliable, configurable, and maintainable system to transfer the log data into HDFS.

Note − In POSIX file system, whenever we are accessing a file (say performing write operation), other programs can still read this file (at least the saved portion of the file). This is because the file exists on the disc before it is closed.

===== Flume Architecture =====

Flume has three layers :
  * **Agent** -> An agent is an independent process (JVM) in Flume. It collects data from clients or other agents and sends them to the next destination(sink or agent).
  * **Collector** -> Collector can combine data from one or many agents, and send it to storage
  * **Storage** -> Storage can be a file, hdfs, Hive, HBase, etc.

==== Flume-Agent ====

An agent is an independent daemon process (JVM) in Flume. It receives the data (**events**) from clients(aka. **sources**) or other agents and forwards it to its next destination (**sinks** or agent) via **channels**. Flume may have more than one agent. In one agent, you have the following key components:

  * **Source** -> A source is the component of an Agent which receives data from the data generators and transfers it to one or more channels in the form of Flume events. Apache Flume supports several types of sources and each source receives events from a specified data generator. (e.g. Avro source, Thrift source, twitter 1% source)
  * **Channel** -> A channel is a transient store which receives the events from the source and buffers them till they are consumed by sinks. It acts as a bridge between the sources and the sinks. These channels are fully transactional and they can work with any number of sources and sinks. (e.g. JDBC channel, File system channel, Memory channel, etc.)
  * **Sink** -> A sink stores the data into centralized stores like HBase and HDFS. It consumes the data (events) from the channels and delivers it to the destination. The destination of the sink might be another agent or the central stores. (e.g. HDFS sink)

The data in flume is represented as **flume event**, it contains a body and a set of headers. The body of the event is a byte array that usually is the payload that Flume is transporting. The headers are represented as a map with string keys and string values. Headers are not meant to transfer data, but for routing purposes and to keep track of priority, the severity of events being sent, etc. The headers can be used to add event IDs or UUIDs to events as well.


==== Additional Components of Flume agent ====

What we have discussed above are the primitive components of the agent. In addition to this, we have a few more components that play a vital role in transferring the events from the data generator to the centralized stores.

=== Interceptor ===
Interceptors are used to alter/inspect flume events which are transferred between source and channel.

=== Channel Selectors ===

These are used to determine which channel is to be opted to transfer the data in case of multiple channels. There are two types of channel selectors 

  * **Default channel selectors** − These are also known as replicating channel selectors they replicate all the events in each channel.
  * **Multiplexing channel selectors** − These decides the channel to send an event based on the address in the header of that event.

=== Sink Processors ===

These are used to invoke a particular sink from the selected group of sinks. These are used to create failover paths for your sinks or load balance events across multiple sinks from a channel.

===== Flume Data flow =====

There are three key steps in flume data flow mechanism:

  - Generally, events and log data are generated by the log servers and these servers have Flume agents running on them. These agents receive data from the data generators. Inside an agent, sources write the data into channels using channel processors, interceptors and selectors. Each and every source has its own channel processor, which takes the task given by the source and then passes that task or events to one or more interceptors. Interceptors read the event and modify or drop the event based on some criteria like regex. We can have multiple interceptors which are called in the order in which they are defined. This can be called a chain-of-responsibility design pattern. Then we pass that list of events generated by interceptor chain to channel selector. The selectors decide which channels attached to this source each event be written to.
  - The data in these agents will be collected by an intermediate node known as Collector. Just like agents, there can be multiple collectors in Flume.
  - Finally, the data from all these collectors will be aggregated and pushed to a centralized store such as HBase or HDFS. The following diagram explains the data flow in Flume.


{{:employes:pengfei.liu:data_science:flume:flume_general_dataflow.png?400|}}

For example, suppose we have some web services generate events and log data. To upload these data into HDFS. First, we need to install and configure Flume agents on these servers.

  - Flume agents running on the web services servers receive the data from the data generators. The data generators can be internal like apache log files or external like facebook and Twitter web services.
  - The data in these agents will be collected by an intermediate node known as Collector. Just like agents, there can be multiple collectors in Flume.
  - At last data from all collectors will be aggregated and pushed to a centralized store such as HBase or HDFS.

==== Failure Handling ====

In Flume, for each event, two transactions take place: one at the sender and one at the receiver. The sender sends events to the receiver. Soon after receiving the data, the receiver commits its own transaction and sends a “received” signal to the sender. After receiving the signal, the sender commits its transaction. (Sender will not commit its transaction till it receives a signal from the receiver.)

==== Different data flow ====

=== Multi-hop Flow ===

An event may travel through more than one agent before reaching final destination is called as Multi-hop Flow. 

{{:employes:pengfei.liu:data_science:flume:multi_hop.png?400|}}

=== Fan-out Flow ===

The dataflow from one source to multiple channels is known as fan-out flow. It is of two types:
  * Replicating − The data flow where the data will be replicated in all the configured channels.
  * Multiplexing − The data flow where the data will be sent to a selected channel which is mentioned in the header of the event.

{{:employes:pengfei.liu:data_science:flume:fan-out.png?400|}}

=== Fan-in Flow ===

The data flow in which the data will be transferred from many sources to one channel is known as fan-in flow.

{{:employes:pengfei.liu:data_science:flume:fan-in.png?400|}}



