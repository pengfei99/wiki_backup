====== Flume agent fetch data from local file system to hdfs ======

We have some important data which are generated by some sensors, we want to transfer these data into HDFS.

To do this kind of job, we can imagine two kinds of source:
  * **Spool directory**: This source lets you insert data by placing files into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. After a given file has been fully read into the channel, it is renamed to indicate completion (or optionally deleted).
Unlike the Exec source, this source is reliable and will not miss data, even if Flume is restarted or killed. In exchange for this reliability,uniquely-named files must be dropped into the spooling directory
  * **Exec**: Exec source runs a given Unix command on start-up one time and expects that process to continuously produce data on standard output location with no regular interval. If the process exits for any reason, the source also exits and will produce no further data.

We can conclude that the **Spool directory** source is better.

===== Configure Flume =====

==== Full flume agent conf ====

<code>
############### agent ###############
SpoolDirAgent.sinks =  MyHDFS
SpoolDirAgent.sources = SpoolDir
SpoolDirAgent.channels = myFileChannel


############### channel ########################
# Define a file channel called fileChannel on SpoolDirAgent
SpoolDirAgent.channels.myFileChannel.type = file 
SpoolDirAgent.channels.myFileChannel.capacity = 200000
SpoolDirAgent.channels.myFileChannel.transactionCapacity = 1000


###################### source ######################
SpoolDirAgent.sources.SpoolDir.type = spooldir
# on linux FS
#Spooldir in my case is /tmp/flume_spoolDir_test
SpoolDirAgent.sources.SpoolDir.spoolDir = /tmp/flume_spoolDir_test
SpoolDirAgent.sources.SpoolDir.fileHeader = false
SpoolDirAgent.sources.SpoolDir.fileSuffix = .COMPLETED

################ Sink ##########################
SpoolDirAgent.sinks.MyHDFS.type = hdfs
SpoolDirAgent.sinks.MyHDFS.hdfs.path = hdfs://localhost:9000/tmp/flume_spool_dir
SpoolDirAgent.sinks.MyHDFS.hdfs.batchSize = 1000
SpoolDirAgent.sinks.MyHDFS.hdfs.rollSize = 268435456
SpoolDirAgent.sinks.MyHDFS.hdfs.rollInterval = 0
SpoolDirAgent.sinks.MyHDFS.hdfs.rollCount = 50000000
SpoolDirAgent.sinks.MyHDFS.hdfs.writeFormat=Text
SpoolDirAgent.sinks.MyHDFS.hdfs.fileType = DataStream

###### Binding source and sink to the channel ######
SpoolDirAgent.sources.SpoolDir.channels = myFileChannel
SpoolDirAgent.sinks.MyHDFS.channel = myFileChannel

</code>

===== Execution =====

<code>
flume-ng agent -f spoolDir-agent.conf -n SpoolDirAgent -Dflume.root.logger=INFO,console
</code>

After the flume agent is launched, you can start to create files in the spool directory. Based on your hdfs sinks batch size, you can notice, multiple files could be merged into one single file in the hdfs. If the file size is greater than the batch size, it will be divided into multiple files in the hdfs.

