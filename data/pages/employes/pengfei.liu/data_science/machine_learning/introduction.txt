====== Machine learning Introduction ======

===== what is Machine learning? =====


The first definition given by by Arthur Samuel in 1959: “Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.”

And more recently, in 1997, Tom Mitchell gave a “well-posed” definition that has proven more useful to engineering types: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”

“A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” -- Tom Mitchell, Carnegie Mellon University


===== Examples of machine learning problems include, =====
 

  * “Is this cancer?”, 
  * “What is the market value of this house?”, 
  * “Which of these people are good friends with each other?”, 
  * “Will this rocket engine explode on take off?”, 
  * “Will this person like this movie?”, 
  * “How do you fly this thing?”. 

All of these problems are excellent targets for an ML project, and in fact ML has been applied to each of them with great success.

===== Different machine learning types =====


Among the different types of ML tasks, a crucial distinction is drawn between supervised and unsupervised learning:

  * Supervised machine learning: The program is “trained” on a pre-defined set of “training examples”, which then facilitate its ability to reach an accurate conclusion when given new data.
  * Unsupervised machine learning: The program is given a bunch of data and must find patterns and relationships therein.

There are other more complex types of ML, 
  * Reinforcement learning from 
  * Deep learning

===== Different machine learning algo =====

The common used algorithm in ML 
  * Linear regression
  * Logistic regression
  * Decision Tree
  * Support Vector machine (SVM)
  * Naive Bayes classifiers
  * K-NearestNeighbor (KNN)
  * K-means clustering
  * Random decision forests
  * Dimension reduction (Two branch: Feature selection and Feature extraction)
  * Gradient Boost 
  * Adaboost



===== Supervised Machine Learning =====



In the majority of supervised learning applications, the ultimate goal is to develop a finely tuned predictor function h(x) (sometimes called the “hypothesis”). “Learning” consists of using sophisticated mathematical algorithms to optimize this function so that, given input data x about a certain domain (say, square footage of a house), it will accurately predict some interesting value h(x) (say, market price for said house).

In practice, x almost always represents multiple data points. So, for example, a housing price predictor might take not only square-footage (x1) but also number of bedrooms (x2), number of bathrooms (x3), number of floors (x4), year built (x5), zip code (x6), and so forth. Determining which inputs to use is an important part of ML design. However, for the sake of explanation, it is easiest to assume a single input value is used.

So let’s say our simple predictor has this form:

h of x equals theta 0 plus theta 1 times x : h(x)=T0+T1x

where theta 0 (T0) and theta 1 (T1) are constants. Our goal is to find the perfect values of theta 0 and theta 1 to make our predictor work as well as possible.


Optimizing the predictor h(x) is done using training examples. For each training example, we have an input value x_train, for which a corresponding output, y, is known in advance. 
For each example, we find the difference between the known, correct value y, and our predicted value h(x_train). With enough training examples, these differences give us a useful way to measure the “wrongness” of h(x). 
We can then tweak h(x) by tweaking the values of theta 0 and theta 1 to make it “less wrong”. This process is repeated over and over until the system has converged on the best values for theta 0 and theta 1. In this way, 
the predictor becomes trained, and is ready to do some real-world predicting.


Machine Learning Regression: A Note on Complexity

The above example is technically a simple problem of univariate linear regression, which in reality can be solved by deriving a simple normal equation and skipping this “tuning” process altogether. However, consider a predictor that looks like this:

Four dimensional equation example

This function takes input in four dimensions and has a variety of polynomial terms. Deriving a normal equation for this function is a significant challenge. Many modern machine learning problems take thousands or even millions of dimensions of data to build predictions using hundreds of coefficients. Predicting how an organism’s genome will be expressed, or what the climate will be like in fifty years, are examples of such complex problems.

Many modern ML problems take thousands or even millions of dimensions of data to build predictions using hundreds of coefficients.
Fortunately, the iterative approach taken by ML systems is much more resilient in the face of such complexity. Instead of using brute force, a machine learning system “feels its way” to the answer. For big problems, this works much better. While this doesn’t mean that ML can solve all arbitrarily complex problems (it can’t), it does make for an incredibly flexible and powerful tool.



==== Two major subcategories of supervised Machine learning ====



Under supervised ML, two major subcategories are:

- Regression machine learning systems: Systems where the value being predicted falls somewhere on a continuous spectrum. These systems help us with questions of “How much?” or “How many?”.

- Classification machine learning systems: Systems where we seek a yes-or-no prediction, such as “Is this tumer cancerous?”, “Does this cookie meet our quality standards?”, and so on.

=== Classification problems in supervised Machine learning ===

In classification, a regression predictor is not very useful. What we usually want is a predictor that makes a guess somewhere between 0 and 1. In a cookie quality classifier, a prediction of 1 would represent a very confident guess that the cookie is perfect and utterly mouthwatering. A prediction of 0 represents high confidence that the cookie is an embarrassment to the cookie industry. Values falling within this range represent less confidence, so we might design our system such that prediction of 0.6 means “Man, that’s a tough call, but I’m gonna go with yes, you can sell that cookie,” while a value exactly in the middle, at 0.5, might represent complete uncertainty. This isn’t always how confidence is distributed in a classifier but it’s a very common design and works for purposes of our illustration.

It turns out there’s a nice function that captures this behavior well. It’s called the sigmoid function, g(z), and it looks something like this:

h of x equals g of z.

== Classification work flow ==

Identify feature data -> prepare training data -> train the classifier -> test the accuracy of the classifier -> if bad accuracy, add new feature or remove bad feature data and repeat the process. 


== features selection Classification problems ==
 

- informative : The feature data must help classifier to separate different class. Like skin color of appel and orange can be usefull feature. But color of strawberry and cherry are not usefull feature data.

- independent : Each feature data must be independent. For example, if we have weight of orange and appel in kg and g both as feature data. The classifier may take weight more than it is. This can make classifier inaccurate. 

- simple : Simple feature data can make classifier more accurate. for example, the distance of two city is  the feature data to determine how long a mail will arrive. If you replace distance by gps location. It will be harder for classifier to determine how long.

=== Regression problems in supervised machine learning ===



==== Most common used algo in sml: ====

  * Linear regression
  * Logistic regression
  * Decision Tree
  * Support Vector machine (SVM)
  * KNN
  * Random forest


===== Unsupervised Machine Learning =====


Unsupervised machine learning is typically tasked with finding relationships within data. There are no training examples used in this process. Instead, the system is given a set data and tasked with finding patterns and correlations therein. A good example is identifying close-knit groups of friends in social network data.

The Machine Learning algorithms used to do this are very different from those used for supervised learning, and the topic merits its own post. However, for something to chew on in the meantime, take a look at clustering algorithms such as k-means, and also look into dimensionality reduction systems such as principle component analysis. Our prior post on big data discusses a number of these topics in more detail as well.

===== Deep learning =====

===== Reinforced learning =====

