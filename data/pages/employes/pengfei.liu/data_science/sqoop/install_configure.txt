====== Sqoop Introduction Installation ======

===== Introduction =====

Sqoop − “SQL to Hadoop and Hadoop to SQL”

Sqoop is a tool designed to transfer data between Hadoop and relational database servers. It is used to import data from relational databases such as MySQL, Oracle to Hadoop HDFS, and export from Hadoop file system to relational databases. It is provided by the Apache Software Foundation.

Sqoop Import

The import tool imports individual tables from RDBMS to HDFS. Each row in a table is treated as a record in HDFS. All records are stored as text data in text files or as binary data in Avro and Sequence files.

Sqoop Export

The export tool exports a set of files from HDFS back to an RDBMS. The files given as input to Sqoop contain records, which are called as rows in table. Those are read and parsed into a set of records and delimited with user-specified delimiter.

The offical doc can be found : https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html

See this tutorial for more information : https://www.tutorialspoint.com/sqoop/sqoop_introduction.htm

===== Install sqoop =====

Sqoop needs hdfs to run. So you need to install hdfs first. Follow this doc[[employes:pengfei.liu:big_data:hadoop:install_config_hadoop|Install hdfs on multi node cluster]]

Downloud the latest stable version from here : http://sqoop.apache.org/

In our case, the stable version is 1.4.6.


<code>
#Download the sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz.
#put the sqoop bin in your chosen place
mkdir -p /opt/sqoop
mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /opt/sqoop/.
cd /opt/sqoop/
tar -xzvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz 
mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6

#add sqoop bin path in path
vim /etc/profile.d/sqoop.sh

#add the following lines
#!/bin/bash

export SQOOP_HOME=/opt/sqoop/sqoop-1.4.6
export PATH=$PATH:$SQOOP_HOME/bin

[hadoop@localhost sqoop-1.4.6]$ source /etc/profile.d/sqoop.sh 
[hadoop@localhost sqoop-1.4.6]$ echo $SQOOP_HOME
/opt/sqoop/sqoop-1.4.6

</code>

===== Configure sqoop =====

To make sqoop works with hdfs, you need to modify the sqoop-env.sh

<code>
cp sqoop-env-template.sh sqoop-env.sh

vim sqoop-env.sh

# add follwoing line 

#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/opt/hadoop/hadoop

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/opt/hadoop/hadoop

#set the path to where bin/hbase is available
export HBASE_HOME=/opt/hbase/hbase-1.2.6

#Set the path to where bin/hive is available
export HIVE_HOME=/opt/hive/hive-2.3.2

</code>

Only the the HADOOP_COMMON_HOME and HADOOP_MAPRED_HOME is essential. others can be omitted if you don't have them.

Test your sqoop
<code>
[hadoop@localhost lib]$ sqoop-version
Warning: /opt/sqoop/sqoop-1.4.6/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/sqoop/sqoop-1.4.6/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
Warning: /opt/sqoop/sqoop-1.4.6/../zookeeper does not exist! Accumulo imports will fail.
Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hbase/hbase-1.2.6/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
18/01/31 11:56:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
Sqoop 1.4.6
git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25
Compiled by root on Mon Apr 27 14:38:36 CST 2015

</code>


===== Add jdbc connector =====

If you want to use sqoop to connect to mysql/mariadb, you need to download the appropriate driver.

For example, for mysql,plz go to https://www.mysql.com/products/connector/.

for postgresql plz go to https://jdbc.postgresql.org/download.html

for mariadb, plz go to https://mariadb.com/kb/en/library/about-mariadb-connector-j/

After download the .jar file, you need to put it in /opt/sqoop/sqoop-1.4.6/lib



====== Install sqoop via hortonworks ambari yum repo ======


<code>
# 1. check your ambari yum repo
> yum list sqoop
# The output should list at least one Sqoop package similar to the following:
# sqoop.noarch <version>
# If yum responds with "Error: then you need to follow the ambari installation guid to install the ambari yum repo.

# 2. Install the sqoop
> yum install sqoop

# 3. After the install, you can find your sqoop under /usr/hdp/current/sqoop-client and /usr/hdp/current/sqoop-server.
# They are just soft symbolic link to /usr/hdp/3.0.1.0-187/sqoop. Based on different version of hdp you installed, the 3.0.1.0 will be replaced by other versions numbers.

</code>

====== Configure sqoop to adapte hortonworks ambari evironment ======

All the conf file of sqoop for hortonworks distribution is located at /etc/sqoop/conf, you can also find a symbolic link of it in /usr/hdp/current/sqoop-client/conf.

<code>
# content of my sqoop/conf

# atlas server connection properties
atlas-application.properties  

# sqoop reqired enviroment variable properties
sqoop-env.sh            
sqoop-env-template.sh    
sqoop-env-template.cmd

# sqoop component settings
sqoop-site.xml
sqoop-site-template.xml

# Data connector between oracle and hadoop
oraoop-site-template.xml

</code>

===== Configure sqoop-env.sh =====

<code>
# add the following config to sqoop-env.sh, you can notice that I mixed the real file location and soft symbolic file location. The recommended way is to use the symbolic location, after update to a new version, you don't need to change your conf file, you only need to point current/.. to the new version. 

# Set Hadoop-specific environment variables here.

#Set path to where bin/hadoop is available
export HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/3.0.1.0-187/hadoop}

#set the path to where bin/hbase is available
export HBASE_HOME=${HBASE_HOME:-/usr/hdp/current/hbase-client}

#Set the path to where bin/hive is available
export HIVE_HOME=${HIVE_HOME:-/usr/hdp/current/hive-client}

#Set the path for where zookeper config dir is
export ZOOCFGDIR=${ZOOCFGDIR:-/etc/zookeeper/conf}

# add libthrift in hive to sqoop class path first so hive imports work
export SQOOP_USER_CLASSPATH="`ls ${HIVE_HOME}/lib/libthrift-*.jar 2> /dev/null`:${SQOOP_USER_CLASSPATH}"

</code>
==== Configure sqoop-site.xml ====

Put Sqoop-specific properties in this file. In our case, we just put a atlas sqoop hook. This hook will upload all sqoop jobs to atlas for data governance operations.

<code>
<configuration  xmlns:xi="http://www.w3.org/2001/XInclude">
    
    <property>
      <name>sqoop.job.data.publish.class</name>
      <value>org.apache.atlas.sqoop.hook.SqoopHook</value>
    </property>
    
  </configuration>

</code>

==== Configure atlas-application.properties ====

<code>
atlas.authentication.method.kerberos=False
atlas.cluster.name=Pengfei_Data_Warehouse
atlas.jaas.KafkaClient.option.renewTicket=true
atlas.jaas.KafkaClient.option.useTicketCache=true
atlas.kafka.bootstrap.servers=cclindw01.in2p3.fr:6667
atlas.kafka.hook.group.id=atlas
atlas.kafka.zookeeper.connect=cclindw03.in2p3.fr:2181,cclindw02.in2p3.fr:2181,cclindw01.in2p3.fr:2181
atlas.kafka.zookeeper.connection.timeout.ms=30000
atlas.kafka.zookeeper.session.timeout.ms=60000
atlas.kafka.zookeeper.sync.time.ms=20
atlas.notification.create.topics=True
atlas.notification.replicas=1
atlas.notification.topics=ATLAS_HOOK,ATLAS_ENTITIES
atlas.rest.address=http://cclindw03.in2p3.fr:21000

</code>
===== Validate sqoop installation =====
<code>
# check the sqoop version
sqoop version | grep 'Sqoop'

# You should see something like
Sqoop 1.4.7.3.0.1.0-187
</code>

====== Know bugs ======

You can find the full list here: https://docs.hortonworks.com/HDPDocuments/HDP3/HDP-3.0.0/release-notes/content/known_issues.html

The below bugs are the bugs which I encountered.

1. The table is marked as a managed table but is not transactional.

In HDP 3, managed Hive tables must be transactional (hive.strict.managed.tables=true). Transactional tables with Parquet format are not supported by Hive. Hive imports with --as-parquetfile must use external tables by specifying --external-table-dir.

<code>
# The simplest solution is not use --hive-import and --as-parquetfile togethor

# our you can give the table path 

sqoop import-all-tables \
    -m 1 \
    --driver com.mysql.jdbc.Driver \
    -libjars="/usr/hdp/current/sqoop-server/lib/*.jar" \
    --connect jdbc:mysql://localhost:3306/retail_db \
    --username=retail_dba \
    --password=retail_dba_pwd \
    --compression-codec=snappy \
    --as-parquetfile \
    --warehouse-dir=/user/hive/warehouse \
    --hive-import
    --external-table-dir hdfs:///path/to/table
</code>


