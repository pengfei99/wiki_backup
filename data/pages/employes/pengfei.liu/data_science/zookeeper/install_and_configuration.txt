====== Zookeeper Installation and configuration ======

===== Prerequisites =====

Before you begin this installation and configuration guide, you’ll need the following:

  - The standalone installation needs one server with a minimum of 4GB of RAM, including a non-root user with sudo privileges and a firewall. You need two additional servers, set up by following the same steps, for the multi-node cluster.
  - OpenJDK 8 installed on your server, as ZooKeeper requires Java to run. To do this.
  - Because ZooKeeper keeps data in memory to achieve high throughput and low latency, production systems work best with 8GB of RAM. Lower amounts of RAM may lead to JVM swapping, which could cause ZooKeeper server latency. High ZooKeeper server latency could result in issues like client session timeouts that would have an adverse impact on system functionality.

Suppose, we have three servers in the cluster to illustrate this tutorial.

hadoop-nn.pengfei.org

hadoop-dn1.pengfei.org

hadoop-dn2.pengfei.org

===== Step 1. Creating a User for ZooKeeper  =====

<code>
# create a user for zookeeper with a home directory (-m)
sudo useradd zk -m

# Set bash as the default shell for the zk user:
sudo usermod --shell /bin/bash zk

# set a password for this user:
sudo passwd zk

# add user to the wheel(centos default sudo group) group(if you use other distribution, you need to change to the default sudoer group name), so it can run commands in a privileged mode:
usermod -aG sudo zk

# if you don't want zk has ssh access, go to /etc/ssh/sshd_config, and add following line
DenyUsers zk

# switch to the zk user. The -l flag invokes a login shell after switching users. A login shell resets environment variables and provides a clean start for the user.
su -l zk

</code>


===== Step 2. Creating a Data Directory for ZooKeeper =====


ZooKeeper persists all configuration and state data to disk so it can survive a reboot. In this step, you will create a data directory that ZooKeeper will use to read and write data. You can create the data directory on the local filesystem or on a remote storage drive. This tutorial will focus on creating the data directory on your local filesystem.

<code>
# create a directory 
sudo mkir -p /data/zookeeper

# grant zk the ownership to the directory
chown zk:zk /data/zookeeper
</code>

===== Step3 Download zookeeper =====

You can find the download page 
 https://zookeeper.apache.org/releases.html#download

<code>
# download the latest stable
wget https://miroir.univ-lorraine.fr/apache/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz

# extract the tar ball
tar -xzvf apache-zookeeper-3.6.1-bin.tar.gz

# change the ownership
sudo chown zk:zk -R  zookeeper-3.4.13

# Create a symbolic link using the ln command.
sudo ln -s zookeeper-3.4.13 zookeeper
sudo chown -h zk:zk zookeeper
</code>

===== Step 4. Configuring a single node in stanalone mode =====

The configuration file will live in the /opt/zookeeper/zookeeper/conf directory. This directory contains a sample configuration file that comes with the ZooKeeper distribution. This sample file, named zoo_sample.cfg, contains the most common configuration parameter definitions and sample values for these parameters. Some of the common parameters are as follows:

  * **dataDir**: is the directory used by Zookeeper to store data. 
  * **tickTime**: sets the length of a tick in milliseconds. A tick is a time unit used by ZooKeeper to measure the length between heartbeats. Minimum session timeouts are twice the tickTime.
  * **clientPort**: The port used to listen for client connections.
  * **maxClientCnxns**: limits the maximum number of client connections.
 
Create the zoo.cfg file or copy the sample cfg file, then add the following lines

<code>
tickTime=2000
dataDir=/data/zookeeper
clientPort=2181
maxClientCnxns=60
</code>

A tickTime of 2000 milliseconds is the suggested interval between heartbeats. A shorter interval could lead to system overhead with limited benefits. Conventionally, ZooKeeper uses port 2181 to listen for client connections. In most situations, 60 allowed client connections are plenty for development and testing.

===== Step 5 Starting ZooKeeper and Testing the Standalone Installation =====

<code>
#start the zookeeper service daemon
sh /opt/zookeeper/zookeeper/bin/zkServer.sh start

#check the daemon status
sh /opt/zookeeper/zookeeper/bin/zkServer.sh status

#for a follower, the output is like this
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: follower

#for a leader, the output is like this
ZooKeeper JMX enabled by default
Using config: /opt/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfg
Mode: leader

#stop the daemon
sh /opt/zookeeper/zookeeper/bin/zkServer.sh stop
</code>

Now we are all set for the zookeeper service, in order to stop or start the zookeeper daemon, you need to execute the shell command on all nodes.

A CLI is available to connect to the server and issue commands. To start it use the command below

<code>
# this starts a zookeeper client connect to a zookeeper server
sh /opt/zookeeper/zookeeper/bin/zkCli.sh -server 127.0.0.1:2181
</code>


===== Step 6. Creating and Using a Systemd Unit File=====

The systemd, system and service manager, is an init system used to bootstrap the user space and to manage system processes after boot. You can create a daemon for starting and checking the status of ZooKeeper using systemd.

To know more about systems, https://www.digitalocean.com/community/tutorials/systemd-essentials-working-with-services-units-and-the-journal

Use your editor to create a .service file named **zk.service** at **/etc/systemd/system/**.

<code>
[Unit]
Description=Zookeeper Daemon
Documentation=http://zookeeper.apache.org
Requires=network.target
After=network.target

[Service]    
Type=forking
WorkingDirectory=/opt/zookeeper/zookeeper
User=zk
Group=zk
ExecStart=/opt/zookeeper/zookeeper/bin/zkServer.sh start /opt/zookeeper/zookeeper/conf/zoo.cfg
ExecStop=/opt/zookeeper/zookeeper/bin/zkServer.sh stop /opt/zookeeper/zookeeper/conf/zoo.cfg
ExecReload=/opt/zookeeper/zookeeper/bin/zkServer.sh restart /opt/zookeeper/zookeeper/conf/zoo.cfg
TimeoutSec=30
Restart=on-failure

[Install]
WantedBy=default.target
</code>

The Service section in the unit file configuration specifies the working directory, the user under which the service would run, and the executable commands to start, stop, and restart the ZooKeeper service. 

Now that your systemd configuration is in place, you can start the service:

<code>
sudo systemctl start zk

# enable the service to start on boot.
sudo systemctl enable zk

# Check the status of the ZooKeeper service 
sudo systemctl status zk

# Stop the ZooKeeper service using systemctl
sudo systemctl stop zk

# restart the daemon
sudo systemctl restart zk
</code>
===== Step 7. Multi node Zookeeper cluster Configuration =====

Nodes in the ZooKeeper cluster that work together as an application form a quorum. Quorum refers to the minimum number of nodes that need to agree on a transaction before it’s committed. A quorum needs an odd number of nodes so that it can establish a majority. An even number of nodes may result in a tie, which would mean the nodes would not reach a majority or consensus.

==== 7.1 Edit zoo.cfg ====

Suppose we have zookeeper installed under /opt/zookeeper/zookeeper on three servers. 

<code>
vim /opt/zookeeper/zookeeper-3.4.10/conf/zoo.cfg

#put the following config

tickTime=2000
dataDir=/data/zookeeper
clientPort=2181
maxClientCnxns=60
initLimit=10
syncLimit=5
server.1=hadoop-nn.pengfei.org:2888:3888
server.2=hadoop-dn1.pengfei.org:2888:3888
server.3=hadoop-dn2.pengfei.org:2888:3888
</code>


  * **initLimit**: It specifies the time that the initial synchronization phase can take. This is the time within which each of the nodes in the quorum needs to connect to the leader. When you are processing large amounts of data just increase this value as required. 
  * **syncLimit**: It specifies the time that can pass between sending a request and receiving an acknowledgment. It is used to fix the allowable amount of time a server can be out of sync with the leader.   
  * The properties **server.1**, **server.2** and **server.3** specify the servers that form the Zookeeper cluster. Then you specify the hostname of each of the servers followed by the port numbers 2888 and 3888. These ports are used for peer to peer communication. The first (2888) followers use to connect to the leader, and the second (3888) is for leader election.
  *  
==== 7.2 Add myid file ====

To make each server aware of the other servers in the ensemble you use a myid file that is placed in the data directory of each server. This file contains only a unique value between 1 and 255 which corresponds to the value specified in zoo.cfg file. For example for server.1 the **myid** file only contains the value 1.

In our example, the **myid** file should be under **/data/zookeeper**


===== Step 8 Running and Testing the Multi-Node Installation =====

With each node configured to work as a cluster, you are ready to start a quorum. In this step, you will start the quorum on each node and then test your cluster by creating sample data in ZooKeeper.

To start a quorum node, first change to the /opt/zookeeper/zookeeper directory on each node and start the node with the following command:

<code>
java -cp zookeeper-3.4.13.jar:lib/log4j-1.2.17.jar:lib/slf4j-log4j12-1.7.25.jar:lib/slf4j-api-1.7.25.jar:conf org.apache.zookeeper.server.quorum.QuorumPeerMain conf/zoo.cfg
</code>

As nodes start up, you will intermittently see some connection errors followed by a stage where they join the quorum and elect a leader among themselves. After a few seconds of initialization, you can start testing your installation.

Now connect to a zk node with your client and create a znode and delete it.

====== Advance zookeepr configuration ======

===== 1 Storage configuration =====

  * **dataLogDir** : This is the directory where the ZooKeeper transaction logs are stored. **The server flushes the transaction logs using sync writes. Hence, it's very important that a dedicated transaction log device be used** so that transaction logging by the ZooKeeper server is not impacted by I/O activities from other processes in the system. Having a dedicated log device improves the overall throughput and assigns stable latencies to requests.
  * **preAllocSize** : The zookeeper.preAllocSize Java system property is set to preallocate the block size to the transactions log files. The default block size is 64 MB. Preallocating the transaction log minimizes the disk seeks. If snapshots are taken frequently, the transaction logs might not grow to 64 MB. In such cases, we can tune this parameter to optimize the storage usage.
  * **snapCount** : The zookeeper.snapCount Java system property gives us the number of transactions between two consecutive snapshots. After snapCount transactions are written to a logfile, a new snapshot is started, and a new transaction logfile is created. Snapshot is a performance-sensitive operation, and hence, having a smaller value for snapCount might negatively affect ZooKeeper's performance. The default value of snapCount parameter is 100,000.
  * **traceFile** : The requestTraceFile Java system property sets this option to enable the logging of requests to a trace file named traceFile.year.month.day. This option is useful for debugging, but it impacts the overall performance of the ZooKeeper server.
  * **fsync.warningthresholdms** : This is the time measured in milliseconds; it defines a threshold for the maximum amount of time permitted to flush all outstanding writes to the transactional log, write-ahead log (WAL). It issues a warning message to the debug log whenever the sync operation takes longer than this value. The default value is 1,000.
  * **autopurge.snapRetainCount** : This refers to the number of snapshots and corresponding transaction logs to retain in directories, dataDir, and dataLogDir, respectively. The default value is 3.
  * **autopurge.purgeInterval**: This refers to the time interval in hours to purge old snapshots and transaction logs. The default value is 0 , which means auto purging is disabled by default. We can set this option to a positive integer(1 and above) to enable the auto purging. If it is disabled (set to 0), the default, purging doesn't happen automatically. Manual purging can be done by running the zkCleanup.sh script available in the bin directory of the ZooKeeper distribution.
  * **syncEnabled** : This configuration option is newly introduced in 3.4.6 and later versions of ZooKeeper. It is set using the Java system property zookeeper.observer.syncEnabled to enable the "observers" to log transaction and write snapshots to disk, by default, like the "followers". Recall that observers do not participate in the voting process unlike followers, but commit proposals from the leader. Enabling this option reduces the recovery time of the observers on restart. The default value is true 

===== 2. Client server network configuration =====

  * **globalOutstandingLimit**: This parameter defines the maximum number of outstanding requests in ZooKeeper. In real life, clients might submit requests faster than ZooKeeper can process them. This happens if there are a large number of clients. This parameter enables ZooKeeper to do flow control by throttling clients. This is done to prevent ZooKeeper from running out of memory due to the queued requests. ZooKeeper servers will start throttling client requests once the globalOutstandingLimit has been reached. The default limit is 1000 requests. (Java system property: zookeeper.globalOutstandingLimit )
  * **maxClientCnxns**: This is the maximum number of concurrent socket connections between a single client and the ZooKeeper server. The client is identified by its IP address. Setting up a TCP connection is a resource-intensive operation, and this parameter is used to prevent the overloading of the server. It is also used to prevent certain classes of DoS attacks, including file descriptor exhaustion. The default value is 60. Setting this to 0 entirely removes the limit on concurrent connections.
  * **clientPortAddress**: This is the IP address that listens for client connections. By default, ZooKeeper server binds to all the interfaces for accepting client connection.
  * **minSessionTimeout**: This is the minimum session timeout in milliseconds that the server will allow the client to negotiate. The default value is twice the tickTime parameter. If this timeout is set to a very low value, it might result in false positives due to incorrect detection of client failures. Setting this timeout to a higher value will delay the detection of client failures.
  * **maxSessionTimeout**: This is the maximum session timeout in milliseconds that the server will allow the client to negotiate. By default, it is 20 times the tickTime parameter.

===== 3. Configuring a ZooKeeper ensemble =====

A ZooKeeper ensemble or cluster of replicated ZooKeeper servers should be configured optimally to avoid scenarios such as split-brain. A split-brain scenario might happen due to network portioning where two different servers of the same ensemble might pose as leaders and cause inconsistencies.

  * **electionAlg**: This option is used to choose a leader in a ZooKeeper ensemble. A value of 0 corresponds to the original UDP-based version, 1 corresponds to the non-authenticated UDP-based version of fast leader election, 2 corresponds to the authenticated UDP-based version of fast leader election, and 3 corresponds to the TCP-based version of fast leader election. Currently, algorithm 3 is the default.** The implementations of leader election 0, 1, and 2 are now deprecated, and fast leader election is the only one used. Available options are as follows**
  * **initLimit**: This refers to the amount of time, measured in ticks, to allow followers to connect with the leader. initLimit should be set depending on the network speed (and hops) between the leader and follower and based on the amount of data to be transferred between the two. **If the amount of data stored by ZooKeeper is huge due to a large number of znodes and the amount of data stored in them, or if the network bandwidth is low, initLimit should be increased.**
  * **syncLimit**: This is the amount of time measured in ticks to allow followers to sync with a leader. If the followers fall too far behind the leader due to server load or network problems, they are dropped. However, the amount of data stored by ZooKeeper has no effect on the synchronization time between the leader and the follower. Instead, syncLimit depends on network latency and throughput.
  * **leaderServes**: By default, the server in an ensemble that runs in the leader mode also accepts client connections. However, in a loaded and busy ensemble with an update-heavy workload, we can configure the leader server to not accept client connections. This can aid in coordinating write updates at a faster rate and, hence, can lead to increased write throughput.
  * **cnxTimeout**: This refers to the timeout value for opening connections for leader election notifications. This parameter is only applicable with the leader election algorithm 3 – fast leader election. The default value is 5 seconds.
  * **server. x =[hostname]:port1[:port2]** : This parameter is used to define servers in the ZooKeeper ensemble. When the ZooKeeper server process starts up, it determines its identity by looking for the myid file in the data directory. The myid file contains the server number in ASCII; this should be the same as x in server.x of the configuration parameter. The port1 is used to send transaction updates, the port2 is for leader election. **It is very important that all servers use the same server.x configuration.** Also, the list of servers that make up ZooKeeper servers that are used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has.


===== 4. Configuring a quorum =====

ZooKeeper allows for the flexible configuration of quorums within the ensemble. We can form hierarchical quorums by classifying the ZooKeeper servers into groups. This is particularly useful for forming a ZooKeeper ensemble that spans multiple data centers. An ensemble across data centers helps in ensuring the high availability of the service during disaster scenarios. The following options are useful for forming such groups in an ensemble:

  * group.x=nnnnn[:nnnnn] : This enables a hierarchical quorum construction. x is a group identifier and nnnnn corresponds to server identifiers. Groups must be disjoint, and the union of all the groups must be the ZooKeeper ensemble.
  * weight.x=nnnnn : This is used to assign a weight to servers in a group when forming quorums. It corresponds to the weight of a server when voting for leader election and for the atomic broadcast protocol Zookeeper Atomic Broadcast (ZAB). By default, the weight of a server is 1. Assigning more weight to a server allows it to form a quorum with other servers more easily.

===== 5. Quota and authorization =====

==== 5.1 Quota ====

ZooKeeper has configurable quotas associated with its data model. It's possible to set the quota limit on the znodes and the data amount of data stored. If a subtree in the ZooKeeper namespace crosses the quota associated with it, ZooKeeper prints warning messages in the log. **However, the operation is never cancelled if the quota
assigned is exceeded.**

ZooKeeper quotas are stored in the ZooKeeper tree in the /zookeeper/quota path. It is possible to set, list, and delete quotas from the ZooKeeper client APIs and through the ZooKeeper Java shell. The following screenshot shows the results of using the set, list, and del commands:

<code>
# create a znode, ephemeral can't have child znode
create /quota_example

# set a quota of two child
setquota -n 2 /quota_example

# create child, note, the third child is not denied, only a warning messages in the log
create /quota_example/ch1
create /quota_example/ch2
create /quota_example/ch3

# check the quota
[zk: 127.0.0.1:2181(CONNECTED) 6] listquota /quota_example
absolute path is /zookeeper/quota/quota_example/zookeeper_limits
Output quota for /quota_example count=2,bytes=-1
Output stat for /quota_example count=4,bytes=0

# delete quota
[zk: 127.0.0.1:2181(CONNECTED) 7] delquota /quota_example
[zk: 127.0.0.1:2181(CONNECTED) 8] listquota /quota_example
absolute path is /zookeeper/quota/quota_example/zookeeper_limits
quota for /quota_example does not exist.


# the warning message in the log file looks like this
2020-06-08 15:52:23,422 [myid:] - WARN  [SyncThread:0:DataTree@412] - Quota exceeded: /quota_example count=3 limit=2

</code>

==== 5.2 Authorization ====

ZooKeeper also provides configurable options to control authentication and authorization by the service.

A **zookeeper.DigestAuthenticationProvider.superDigest** parameter enables a ZooKeeper ensemble administrator to access the znode hierarchy as a superuser. The ZooKeeper service doesn't do any ACL checking for a user who is authenticated as a superuser. This feature is disabled by default.

To generate a super digest for the superuser, the Java system property called **org.apache.zookeeper.server.auth.DigestAuthenticationProvider** can be used by calling with the parameter **super:<password>**. Once the superDigest is generated, we need to provide **super:<data>** as a system property while starting
the ZooKeeper servers.

A ZooKeeper client needs to pass a scheme of digest and authentication data of **super:<password>** to authenticate with the ZooKeeper server. While using the ZooKeeper shell, we can use the addauth command.