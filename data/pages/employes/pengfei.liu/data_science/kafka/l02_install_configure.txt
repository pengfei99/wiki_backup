
====== Lesson02: Installing and configuring kafka  ======


===== 2.0 Prerequise =====

==== 2.0.1 Install JDK ====

Kafka needs JDK or JRE to run. So the first step is to install jdk.

==== 2.0.2 Install zookeeper ====

**A critical dependency of Apache Kafka is Apache Zookeeper**, which is a distributed configuration and synchronization service. Kafka uses ZooKeeper for:
  * leadership election of Kafka Broker to elect **Controller**: The first arrived broker creates /kafka/controller znode in zk. The other broker will fail because it already exists. 
  * partition replication leader/follower election
  * discover new coming brokers
  * stores basic metadata in Zookeeper such as information about topics(topic-partition pairs), brokers, consumer offsets (queue readers) and so on.
  * ETc.

Since all the critical information is stored in the Zookeeper and it normally replicates this data across its ensemble, failure of Kafka broker / Zookeeper does not affect the state of the Kafka cluster. Kafka will restore the state, once the Zookeeper restarts. This gives zero downtime for Kafka. The leader election between the Kafka broker is also done by using Zookeeper in the event of leader failure.

To learn more about ZooKeeper: [[employes:pengfei.liu:data_science:zookeeper:start|ZooKeeper docs]]

==== 2.0.3 Hardware requirement ====

At least 4GB of RAM on the server. Installations without this amount of RAM may cause the Kafka service to fail, with the Java virtual machine (JVM) throwing an “Out Of Memory” exception during startup.


===== 2.1 Creating a User for Kafka =====
<code>
# create a system user Kafka with a home dir
useradd -r kafka -m

# add Kafka to sudoer group
usermod -aG wheel kafka

# login as kafka
su -l kafka
</code>
===== 2.2 Downloading and Extracting the Kafka Binaries =====
Download the kafka binary from http://kafka.apache.org/downloads.html

Note that Kafka uses Scala, and it's binary is compiled based on certain version of Scala. For example, kafka_2.11-1.0.0 means this Kafka version 1.0.0 is compiled based on scala 2.11; kafka_2.12-2.5.0 means this kafka of version 2.5.0 is compiled based on scala 2.12. So before you chose your kafka binary version, you need to check the scala version which runs on your server.

Then place the binary in your server. I put it under "/opt/kafka" 

<code>
mkdir /opt/kafka

cd /opt/kafka

# extract the binary
tar -xzvf kafka_2.12-2.5.0.tgz 

# change the name
mv kafka_2.12-2.5.0 kafka-2.5.0

# check the result
[root@localhost kafka-2.5.0]# pwd
/opt/kafka/kafka-2.5.0
</code>

===== 2.3 Configuring the Kafka Server =====

The main config file of a Kafka broker is on the **server.properties** file. The default config is enough to run a standalone Kafka broker as a test server. But I still want to highlight some key configuration attributes.

<code>
【broker.id】
每个broker都必须自己设置的一个唯一id，可以在0~255之间. A good guideline is to set this value to something intrinsic to the host. For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.), that is a good choice for the broker.id value.

【log.dirs】
这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。ps：多个目录用英文逗号分隔. If more than one path is specified(e.g. log.dirs=path1,path2), the broker will store partitions on them in a “least-used” fashion with one partition’s log segments stored within the same path. Note that the broker will place a new partition in the path that has the least number of partitions currently stored in it, not the least amount of disk space used in the following situations

【zookeeper.connect】
连接kafka底层的zookeeper集群的. The default configuration is localhost:2181.

【Listeners】
broker监听客户端发起请求的ip,端口号，默认是9092. It will get the value returned from ** java.net.InetAddress.getCanonicalHostName() if not configured**. The port number can be set to any available port by changing the port configuration parameter. Keep in mind that if a port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.

</code>


<code>
# open the config file
cd /opt/kafka/kafka_2.11-1.0.0/config
vim server.properties
</code>

The following is an example of minimum configuration
<file server.properties>
#in a single server with default zookeeper
broker.id=0
log.dirs=/tmp/kafka-logs
zookeeper.connect=localhost:2181

#in a cluster environment with multiple zookeeper instances
zookeeper.connect=hadoop-nn.pengfei.org:2181,hadoop-dn1.pengfei.org:2181,hadoop-dn2.pengfei.org:2181
broker.id=1 (2,3 in other two machines)
log.dirs=fs_on_disk1,fs_on_disk2
</file>




The above config is the minimun config to run kafka, you may need to dig more.






===== 2.4 Creating Systemd Unit Files and Starting the Kafka Server =====

We need to create the systemd service file for Kafka. As Kafka requires ZooKeeper to manage its cluster state and configuration. We need to make sure we have a systemd service file for Zookeeper first. Suppose we have it in /etc/systemd/system/zk.service

Our Kafka systemd file looks like this:
<code>
[Unit]
Requires=zk.service
After=zk.service

[Service]
Type=forking
WorkingDirectory=/opt/kafka/kafka-2.5.0
User=kafka
Group=kafka
ExecStart=/opt/kafka/kafka-2.5.0/bin/kafka-server-start.sh /opt/kafka/kafka-2.5.0/config/server.properties > /var/log/kafka/kafka.log 2>&1
ExecStop=/opt/kafka/kafka-2.5.0/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code>

The Unit part will ensure that zookeeper gets started automatically when the kafa service starts.

Run the daemon
<code>
sudo systemctl start/stop/status kafka

# run daemon at system startup
sudo systemctl enable kafka
</code>
===== 2.5 Testing the Installation =====

To test the kafka server (with one broker). Make sure the zookeeper is running before you run this command.

<code>
sh bin/kafka-server-start.sh config/server.properties
</code>
==== 2.5.1 Create a topic ====

Create a topic with one partition and three replication

<code>
#create a topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper hadoop-nn.bioaster.org --replication-factor 3 --partitions 1 --topic test-topic
Created topic "test-topic".

#check status of a topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
</code>

Create a topic with three partitions and three replication

<code>
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic Hello-Kafka

Created topic "Hello-Kafka".

#get status of the topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic Hello-Kafka
Topic:Hello-Kafka	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: Hello-Kafka	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1
	Topic: Hello-Kafka	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
	Topic: Hello-Kafka	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2

</code>

List all topic in one broker

<code>
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic

</code>

==== 2.5.2 Start producer to send messages====

<code>
# In the broker-list, you could put one or more kafka brokers
# In this example, we use only one, the name of the broker is defined in server.properties  
[hadoop@CCLinDataWHD01 bin]$ sh kafka-console-producer.sh --broker-list hadoop-nn.bioaster.org:9092 --topic Hello-Kafka
#now you are in producer consol, all the text you enter below will be published in topic Hello-Kafka
>hello
>my first message
>my second message
>my third message

</code>

==== 2.5.3 Start two consumers to receive messages ====

<code>
###in server hadoop-dn1.bioaster.org
hadoop@CCLinDataWHD02 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my first message
my second message
my third message
my fouth message
my fifth message


###in server hadoop-dn2.bioaster.org
[hadoop@CCLinDataWHD03-0 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my third message
my first message
my second message
my fouth message
my fifth message


##You could notice, if your consumer connecter after many message has been published
##the order may not be correct. But the latest message will be correct

</code>

==== 2.5.4 Close the test ====

When you are done testing, press CTRL+C to stop the producer and consumer scripts.

To delete the topic use the following command
<code>
[root@localhost kafka-2.5.0]# bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic pengfeiTest
Topic pengfeiTest is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.

</code> 
===== 2.6 Setting Up a Multi-Node Cluster =====

There are three possible scenarios:
  * Multi brokers on single servers
  * Single brokers on multi-servers
  * Multi brokers on multi-servers

==== 2.6.1 Multi brokers on single servers ====

For setting up multiple brokers on a single node, different server property files are required for each broker. Each property file will define unique, different values for the following properties:
  * broker.id
  * port
  * log.dir

For example, if we want to run three brokers on a single server, we need to create three config files server1.properties, server2.properties, server3.properties. 

<file server1.properties>
broker.id=1
listeners = PLAINTEXT://your.host.name:9093
log.dir=/tmp/kafka-logs-1
</file>

<file server2.properties>
broker.id=2
listeners = PLAINTEXT://your.host.name:9094
log.dir=/tmp/kafka-logs-2
</file>

<file server3.properties>
broker.id=3
listeners = PLAINTEXT://your.host.name:9095
log.dir=/tmp/kafka-logs-3
</file>

We can run the three instances of the broker with the following command
<code>
#run broker1
bin/kafka-server-start.sh config/server1.properties

#run broker2
bin/kafka-server-start.sh config/server1.properties

#run broker3
bin/kafka-server-start.sh config/server1.properties
</code>

==== 2.6.2 Single broker on multi nodes ====

This is very simple, all these brokers can have the exact same configuration except the **broker.id**.

==== Zookeeper configuration ====

If you want your Kafka broker to connect to multiple zookeeper servers. In zookeeper.connect, you can use comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.

For example

<code>
//Kafka uses three zk instances
zookeeper.connect=zk1.pengfei.org:2181,zk2.pengfei.org:2181,zk3.pengfei.org:2181
</code>

===== 2.7 Restricting the Kafka User =====

<code>
# Remove the kafka user from the sudo group:
sudo gpasswd -d kafka wheel

# To further improve your Kafka server’s security, lock the kafka user’s password using the passwd command. This makes sure that nobody can directly log into the server using this account:
sudo passwd kafka -l

# At this point, only root or a sudo user can log in as kafka by typing in the following command:
sudo su - Kafka

# In the future, if you want to unlock it, use passwd with the -u option:
sudo passwd kafka -u


</code>

===== 2.8 Basic topic operations (Optional) =====

==== 2.8.1 List all topics and show details of a topic ====


<code>

[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic

#show detail of topic test-topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3

#we know that it has one partition and 3 replicas
</code>

==== 2.8.2 Change the partition number of a topic ====

<code>
#Now we want to change the partition from one to two
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --alter --topic test-topic --partitions 2
WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected
Adding partitions succeeded!

[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:2	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: test-topic	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
</code>

==== 2.8.3 Delet a topic ====


<code>
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --delete --topic test-topic
Topic test-topic is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
</code>
===== 2.9 Kafka runs starts and runs the zookeeper (Optional) =====


if your zookeeper cluster already running or controlled by zookeeper, you don't need to chage the config of /opt/kafka/kafka_2.11-1.0.0/config/zookeeper.properties

if you want to use kafka to control the zookeeper daemon, you need to edit the zookeeper.properties

Here is an example

<code>
# the directory where the snapshot is stored.
dataDir=/opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0

# The number of milliseconds of each tick
tickTime=2000

# The number of ticks that the initial synchronization phase can take
initLimit=10

# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5

#zoo servers
server.1=hadoop-nn.pengfei.org:2888:3888
server.2=hadoop-dn1.pengfei.org:2888:3888
server.3=hadoop-dn2.pengfei.org:2888:3888

</code>


It's the same as the zoo.cfg in zookeeper

<code>
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
#initLimit=10
initLimit=5
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
#syncLimit=5
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

server.1=hadoop-nn.pengfei.org:2888:3888
server.2=hadoop-dn1.pengfei.org:2888:3888
server.3=hadoop-dn2.pengfei.org:2888:3888

</code>

To run the zookeeper daemon in kafak

<code>
cd /opt/kafka/kafka_2.11-1.0.0
sh bin/zookeeper-server-start.sh config/zookeeper.properties
</code>







