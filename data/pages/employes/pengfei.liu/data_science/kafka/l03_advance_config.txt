====== Lesson03: Advance configuration ======

====== 3.1 broker configuration ======

The main config file of a Kafka broker is on the **server.properties** file. The default config is enough to run a standalone Kafka broker as a test server. But I still want to highlight some key configuration attributes.

<code>
【broker.id】
每个broker都必须自己设置的一个唯一id，可以在0~255之间. A good guideline is to set this value 
to something intrinsic to the host. For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.), that is a good choice for the broker.id value.

【log.dirs】
这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，
那么可以把多个目录挂载到不同的物理硬盘上，然后这里可以设置多个目录，这样kafka可以数据分散
到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。ps：多个目录用英文逗号分隔. 
If more than one path is specified(e.g. log.dirs=path1,path2), the broker will store 
partitions on them in a “least-used” fashion with one partition’s log segments stored 
within the same path. Note that the broker will place a new partition in the path that 
has the least number of partitions currently stored in it, not the least amount of disk 
space used in the following situations

【zookeeper.connect】
Kafka utilizes Zookeeper for storing metadata information about the brokers, topics,
and partitions. This configures kafka to connect zookeeper. The default configuration 
is localhost:2181. This will create all required znode of kafka under the root of zk. 
If you want to specify a special path for storing all the kafka data, you can use  
localhost:2181/path, If a chroot path is specified and does not exist, it will be created 
by the broker when it starts up. 

This can be useful when you have one zk cluster for multiple kafka clusters.

【Listeners】
broker监听客户端发起请求的ip,端口号，默认是9092. It will get the value returned from ** java.net.InetAddress.getCanonicalHostName() if not configured**. The port number can be set to any available port by changing the port configuration parameter. Keep in mind that if a port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.

【num.network.threads】The number of threads that the server uses for **receiving requests** from the network and sending responses to the network .默认值为3
【num.io.threads】The number of threads that the server uses for **processing requests**, which may include disk I/O. 默认值为8, 这就是上一篇我们在网络架构上提到的processor和处理线程池的线程数目。这两个参数就是Kafka集群性能的关键参数了

【unclean.leader.election.enable】
默认是false，意思就是只能选举ISR列表里的follower成为新的leader，1.0版本后才设为false，之前都是true，允许非ISR列表的follower选举为新的leader

【delete.topic.enable】
默认true，允许删除topic


【min.insync.replicas】
acks=-1（一条数据必须写入ISR里所有副本才算成功），你写一条数据只要写入leader就算成功了，不需要等待同步到follower才算写成功。但是此时如果一个follower宕机了，你写一条数据到leader之后，leader也宕机，会导致数据的丢失。

【num.recovery.threads.per.data.dir】
Kafka uses a configurable pool of threads for handling log segments. Currently, this thread pool is used: 
1). When starting normally, to open each partition’s log segments. 
2). When starting after a failure, to check and truncate each partition’s log segments. 
3). When shutting down, to cleanly close log segments. 
**This value is recommended to be increased for installations with data dirs located in RAID array.**
** Note that the number configured is per log directory specified with log.dirs. This means that if num.recovery.threads.per.data.dir is set to 8, and there are 3 paths specified in log.dirs, this is a total of 24 threads**

【auto.create.topics.enable】
The default Kafka configuration specifies that the broker should automatically create a topic under the following circumstances:
• When a producer starts writing messages to the topic
• When a consumer starts reading messages from the topic
• When any client requests metadata for the topic
Set the auto.create.topics.enable configuration to false if you don't want auto-creation

</code>

====== 3.2 Topic configuration ======

The following configuration can be specific to one Topic. So we can have multiple topics with different partition numbers, retention policy, and message path max size, etc.. 

<code>
【num.partitions】
The default number(i.e. default value 1) of log partitions per topic. More partitions allow greater parallelism for consumption, but this will also result in more files across the brokers. Note that the number of partitions for a topic can only be increased, never decreased.

【log.retention.hours】
The minimum age (i.e. default value 168hours/7days) of a log file to be eligible for deletion due to age
Note that retention by time is performed by examining the last modified time (mtime) on each log segment file on disk. Under normal cluster operations, this is the time that the log segment was closed, and represents the timestamp of the last message in the file. However, when using administrative tools to move partitions between brokers, this time is not accurate and will result in excess retention for these partitions.

【log.retention.bytes】
Use the total number of bytes of messages (i.e. default value is 1073741824Byte/1GB) to decide when to delete messages. This number is applied per-partition(not the topic). This means that if you have a topic with 8 partitions, and log.retention.bytes is set to 1 GB, the amount of data retained for the topic will be 8 GB at most.


【log.segment.bytes】
The maximum size (i.e. default is 1073741824Bytes/1GB) of a log segment file. When this size is reached a new log segment will be created. So a partition can have multiple segments to store messages.

【log.retention.check.interval.ms】
The interval(i.e. default value is 300000ms/5minutes) at which log segments are checked to see if they can be deleted according to the retention policies

【message.max.bytes】
The largest record batch size (i.e. default value is 1048588/1MB) allowed by Kafka (after compression if compression is enabled). A producer that tries to send a message larger than this will receive an error back from
the broker, and the message will not be accepted.

There are noticeable performance impacts from increasing the allowable message size. Larger messages will mean that the broker threads that deal with processing network connections and requests will be working longer on each request. Larger messages also increase the size of disk writes, which will impact I/O throughput.

</code>
 
===== 3.2.1 How to choose partition numbers? =====

There are several factors to consider when choosing the number of partitions:
  * What is the throughput you expect to achieve for the topic? For example, do you expect to write 100 KB per second or 1 GB per second?
  * What is the maximum throughput you expect to achieve when consuming from a single partition? You will always have, at most, one consumer reading from a partition, so if you know that your slower consumer writes the data to a database and this database never handles more than 50 MB per second from each thread writing to it, then you know you are limited to 60MB throughput when consuming from a partition.
  * You can go through the same exercise to estimate the maximum throughput per producer for a single partition, but since producers are typically much faster than consumers, it is usually safe to skip this.
  * If you are sending messages to partitions based on keys, adding partitions later can be very challenging, so calculate throughput based on your expected future usage, not the current usage.
  * Consider the number of partitions you will place on each broker and available disk space and network bandwidth per broker.
  * Avoid overestimating, as each partition uses memory and other resources on

After you have done some estimation on the goal throughput and consumer throughput of the topic, you can divide the goal throughput by the expected consumer throughput and derive the number of partitions. For example, if I want to be able to write and read 1 GB/sec from a topic, and I know each consumer can only process 50 MB/s, then I know I need at least 20 partitions. This way, I can have 20 consumers reading from the topic and achieve 1 GB/sec.

If you don’t have this detailed information, our experience suggests that limiting the size of the partition on the disk to less than 6 GB per day of retention often gives satisfactory results.

===== 3.2.2 Retention policy conflict management =====

To resolve the retention policy conflict, Kafka uses the first applicable policy. Which means the first applicable retention policy will be used. 

For example, if log.retention.hours is set to 24 (1 day) and log.retention.bytes is set to 1 GB, it is possible for messages that are less than 1 day old to get deleted if the total volume of messages over the course of the day is greater than 1 GB. Conversely, if the volume is less than 1 GB, messages can be deleted after 1 day even if the total size of the partition is less than 1 GB.


===== 3.2.3 Coordinations between message.max.bytes and other configurations   =====

The configuration of **message.max.bytes** must be coordinated with the **fetch.message.max.bytes** configuration on **consumer clients**. If this value is smaller than **message.max.bytes**, then consumers that encounter larger messages will fail to fetch those messages, resulting in a situation where the consumer gets stuck and cannot proceed. The same rule applies to the **replica.fetch.max.bytes** configuration on the brokers when configured in a cluster.


====== 3.3 Benchmarking your kafka cluster ======

After your configuration, you may want to test the limit of your cluster. If it does not reach your expectation, you need to tune your configuration again.

Read this https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing#Performancetesting-CommonStats
Kafka provides a pair of tools to produce and consume test records to test the throughput of the Kafka cluster.

We need to perform a load test for both, i.e Producer and Consumer, to conclude how many messages a producer can produce and a consumer can consume in a given period of time. The key statistics we are looking for are mentioned below:
  * Throughput (messages/second) on the size of data
  * Throughput (messages/second) on the number of messages
  * Total messages
  * Total data

Note zk cluster setup can also impact the Kafka performance, in all below test cases, we suppose we have the same zk cluster setup.

===== 3.3.1 Importatn flags =====

==== Producer ====

For running kafka-producer-perf-test.sh, we have the below important flags.

<code>
 
 --topic  
 --num-records //total record numbers
 --record-size  //record size in bytes
 --throughput  // The maximum throughput for the test
 --producer-props 
     bootstrap.servers=[serverIP]:9092 
     batch.size=1000 
     acks=1 
     linger.ms=100000 
     buffer.memory=4294967296 
     compression.type=text 
     request.timeout.ms=300000 

</code>
==== Consumer ====
For running kafka-consumer-perf-test.sh, we have the below important flags.
<code>
--topic: The topic to consume from.
--broker-list: The list of brokers which the consumer will connect to (in the form host:port. multiple comma-separated URLs can be provided to allow fail-over) 
--messages: The number of messages to consume.
--threads: Number of processing threads.
--fetch-size: The max message size in bytes which the consumer can consume. 
</code>
===== 3.3.2 Test case 1: Single partition, no replication, single producer thread, no compression =====


<code>
# Create a topic with Single partition, no replication, 

bin/kafka-topics.sh --create \
--zookeeper kafka-zk-1:2181,kafka-zk-2:2181,kafka-zk-3:2181 \
--replication-factor 1 \
--partitions 1 \
--topic test1

# run a producer
bin/kafka-producer-perf-test.sh \
--topic test1 \
--num-records 15000000 \
--record-size 100 \
--throughput 15000000 \
--producer-props \
acks=1 \
bootstrap.servers=kf1:9092,kf2:9092,kf3:9092 \
buffer.memory=67108864 \
compression.type=none \
batch.size=8196

# run a consumer
kafka-consumer-perf-test --topic test1 --broker-list kf1:9092,kf2:9092,kf3:9092 --fetch-size 2000 \
--messages 10000000 --threads 2
</code>

After running the conumer, it will output the following message.
<code>
start.time,end.time,data.consumed.in.MB,MB.sec,data.consumed.in.nMsg,nMsg.sec
2019–03–15 06:53:31,1602019–03–15 06:54:06, 14305.1147,399.9417,10000000,279579.5124
</code>

  * **start.time, end.time**: shows test start and end time
  * **data.consumed.in.MB**: shows the size of all messages consumed
  * **MB.sec**: shows how much data transferred in megabytes per second (Throughput on size)
  * **data.consumed.in.nMsg**: shows the count of the total messages consumed during this test
  * **nMsg.sec**: shows how many messages were consumed in a second (Throughput on the count of messages)

===== 3.3.3 More tests =====

There can be multiple scenarios and use cases where we can test and take the performance stats for Kafka Producer and Consumer, some of those cases can be :
  * Change the number of topics
  * Change the async batch size
  * Change message size
  * Change the number of partitions
  * Change the number of Brokers
  * Change the number of Producer/Consumer etc
  * Change the compression Type
