====== Lesson01: Kafka basics ======

===== 1.1 Important concepts and terms =====

====  Publish-subscribe messaging system ====
In the publish-subscribe system, messages are persisted in a topic. Unlike point-to-point system, consumers can subscribe to one or more topic and consume all the messages in that topic. In the Publish-Subscribe system, message producers are called publishers and message consumers are called subscribers. A real-life example is Dish TV, which publishes different channels like sports, movies, music, etc., and anyone can subscribe to their own set of channels and get them whenever their subscribed channels are available.


==== 1.1.1 What is Kafka? ====

**Apache Kafka is a distributed publish-subscribe messaging system and a robust queue** that can handle a high volume of data and enables you to pass messages from one end-point to another. Kafka is suitable for both offline and online message consumption. **Kafka messages are persisted on the disk and replicated within the cluster to prevent data loss**. Kafka is built on top of the ZooKeeper synchronization service. It integrates very well with Apache Storm and Spark for real-time streaming data analysis



=== Kafka use cases ===

  * **stream processing**: Popular frameworks such as Storm and Spark Streaming read data from a topic, processes it, and write processed data to a new topic where it becomes available for users and applications. Kafka’s strong durability is also very useful in the context of stream processing.
  * website activity tracking
  * **metrics collection and monitoring**: Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.
  * **log aggregation**: Kafka can be used across an organization to collect logs from multiple services and make them available in a standard format to multiple consumers.
  * real-time analytics
  * CEP
  * ingesting data into Spark
  * ingesting data into Hadoop
  * CQRS
  * replay messages
  * error recovery
  * guaranteed distributed commit log for in-memory computing (microservices)


=== Summery Kafka key concepts ===

  * Broker -> In a kafka cluster, we may have one or many server. The server is called broker
  * Topic -> Each message in kafka must has a category, we call the category a Topic 
  * Partition -> Each Topic can have one or more Partitions.
  * Producer -> A program or process which can send messages to Broker 
  * Consumer -> A program or process which read messages from Broker
  * Consumer group -> Each consumer belongs to a consumer group. When you create a consumer, you can specify consumer group name, otherwise kafka will assign a default group name.

==== 1.1.2 Message and Batch ====

**The unit of data within Kafka is called a message**. A message is simply an array of bytes as far as Kafka is concerned, so the data contained within it does not have a specific format or meaning to Kafka. 

**A message can have an optional bit of metadata, which is referred to as a key.** The key is also a byte array and, as with the message, has no specific meaning to Kafka. Keys are used when messages are to be written to partitions in a more controlled manner. 

**For efficiency, messages are written into Kafka in batches. A batch is just a collection of messages**, all of which are being produced to the same topic and partition. An individual roundtrip across the network for each message would result in excessive overhead, and collecting messages together into a batch reduces this. Of course, this is **a tradeoff between latency and throughput**: the larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate. Batches are also typically compressed, providing more efficient data transfer and storage at the cost of some processing power.

==== 1.1.3 Sechema ====

**It is recommended that additional structure, or schema, be imposed on the message content**(e.g. json, XML and avro). Many Kafka developers favor the use of Apache Avro, which is a serialization framework originally developed for Hadoop. Avro provides a compact serialization format; schemas that are separate from the message payloads and that do not require code to be generated when they change; and strong data typing and schema evolution, with both backward and forward compatibility.

A consistent data format is important in Kafka, as it allows writing and reading messages to be decoupled. By using well-defined schemas and storing them in a common repository, the messages in Kafka can be understood without coordination.

==== 1.1.4 Topics and partition ====
**Messages in Kafka are categorized into topics**. **Topics are additionally broken down into a number of partitions**. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic.

The following figure shows a topic with four partitions, with writes being appended to the end of each one.
{{:employes:pengfei.liu:data_science:kafka:kafka_topics.png?600|}}

The number here is the partition offset. All records(messages) in partitions are assigned sequential id number(starts from 0) called the offset. The offset identifies each record location within the partition. As a result, we can guarantee the message time-ordering inside of one partition.

**Partitions are also the way that Kafka provides redundancy and scalability.** Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.

To avoid data loss, partition has replications. We will give more details in the partition replication section.

=== Topic retention ===

Retention is the durable storage of messages for some period of time. Kafka brokers are configured with a default retention setting for topics:
  * for some period of time (e.g., 7 days)
  * for a certain size in bytes (e.g., 1 GB)

Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be configured with their own retention settings.

For example
  - a tracking topic might be retained for several days
  - application metrics might be retained for only a few hours. 
  - Topics can also be configured as **log compacted**, which means that Kafka will retain only the last message produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting.
==== 1.1.5 Stream ====

The term stream is often used when discussing data within systems like Kafka. **Most often, a stream is considered to be a single topic of data, regardless of the number of partitions. This represents a single stream of data moving from the producers to the consumers.** This way of referring to messages is most common when discussing
stream processing, which is when frameworks (e.g. Kafka Streams, Apache Samza, and Storm) operate on the messages in real-time. There is also batch processing which is when offline frameworks (e.g. Hadoop, hive) work on bulk data at a later time. 

==== 1.1.6 Producers and Consumers ====

=== Producer ===

**Producers create new messages. In general, a message will be written to a specific topic. By default, the producer does not care what partition** a specific message is written to and will balance messages over all partitions of a topic evenly. 

**In some cases, the producer will direct messages to specific partitions by using the message key.** A partitioner will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition.

=== Consumer ===

**Consumers read messages.** The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. **Each message in a given partition has a unique offset**. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart
without losing its place.

=== Consumer group ===

消费者组就是让多个消费者并行消费信息而存在的，而且它们不会消费到同一个消息. 
(not sure)A single consumer in a group does not track the offset of messages. The group do the track. 

<code>
consumer group:a
    consumerA
    consumerB
    consumerC
</code> 

=== Topic log ===

**A topic is associated with a log** which is a data structure on disk. 

  - **Records from a producer(s)** are appended to the end of a topic log. **A topic log consists of many partitions** that are spread over multiple files which can be spread on multiple brokers. 
  - Consumers read from Kafka topics at their cadence and can pick where they are (offset) in the topic log. **Each consumer group** tracks offset from where they left off reading. 

Kafka distributes topic log partitions on different nodes in a cluster for high performance with horizontal scalability. Spreading partitions aids in writing data quickly. Topic log partitions are Kafka way to shard reads and writes to the topic log. Also, partitions are needed to have multiple consumers in a consumer group work at the same time. Kafka replicates partitions to many nodes to provide failover.

{{:employes:pengfei.liu:data_science:kafka:kafka_consumer_producer.png?600|}}

==== 1.1.7 Broker and cluster====

**A single Kafka server is called a broker**. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second.

=== Controller ===

**Kafka brokers are designed to operate as part of a cluster.** Within a cluster of brokers, **one broker will also function as the cluster controller (first arrived broker is elected automatically).** The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures. 


=== Leader ===

A partition is owned by a single broker in the cluster, and that broker is called the leader broker of the partition. All consumers and producers operating on that partition must use the leader broker. A replication of the partition may be assigned to multiple brokers(as seen in Figure 1-7). This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. 

{{:employes:pengfei.liu:data_science:kafka:kafka_partition_replication.png?600|}}



==== 1.1.8 Multiple Clusters====

As Kafka deployments grow, it is often advantageous to have multiple clusters. There are several reasons why this can be useful:

  * Segregation of types of data
  * Isolation for security requirements
  * Multiple datacenters (disaster recovery)

When working with multiple datacenters in particular, it is often required that messages be copied between them. In this way, online applications can have access to user activity at both sites. **The replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters**.

=== MirrorMaker ===

The Kafka project includes a tool called MirrorMaker, used for this purpose. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another.

Figure 1-8 shows an example of an architecture that uses MirrorMaker, aggregating messages from two local clusters into an aggregate cluster, and then copying that cluster to other datacenters. 

{{:employes:pengfei.liu:data_science:kafka:kafka_mirror_maker.png?600|}}

==== 1.1.9 In-Sync Replica (ISR) ====

Kafka considers that **a record/message is committed when all replicas in the In-Sync Replica set (ISR) have confirmed that they have written the record to disk.** The acks=all setting requests that an ack is sent once all in-sync replicas (ISR) have the record.

=== 1.1.9.1 Apache Kafka ack-value (Producer config) ===


An acknowledgment (ACK) is a signal passed between communicating processes to signify acknowledgment, i.e., receipt of the message sent. **The ack-value is a producer configuration parameter** (So, you will not find it in the server.properties file) in Apache Kafka and can be set to the following values:

  * **acks=0**: The producer never waits for an ack from the broker.** No guarantee can be made that the broker has received the message**. No retry to send the record again since the producer never knows that the record was lost. **This setting provides lower latency and higher throughput at the cost of much higher risk of message loss**.
  * **acks=1**: The producer gets an ack after the leader has received the record. **The leader will write the record to its log but will respond without awaiting a full acknowledgment from all followers**. The message will be lost only if the leader fails immediately after acknowledging the record, but before the followers have replicated it. **This setting is the middle ground for latency, throughput, and durability. It is slower but more durable than acks=0.**
  * **acks=all(or -1)**: The producer gets an ack when all in-sync replicas have received the record. The leader will wait for the full set of in-sync replicas to acknowledge the record. **This means that it takes a longer time to send a message with ack value all, but it gives the strongest message durability**.

=== 1.1.9.2 What is the ISR (In-Sync Replica)? ===

The ISR is simply all the replicas of a partition that are "in-sync" with the leader. The definition of "in-sync" depends on the topic configuration, but by default, it means that a replica is or has been fully caught up with the leader in the last 10 seconds. The setting for this time period is: **replica.lag.time.max.ms** and has a server default which can be overridden on a per topic basis.

PS: In Kafka 2.5, the default value of zookeeper.session.timeout.ms has been increased from 6s to 18s and **replica.lag.time.max.ms from 10s to 30s**.

The ISR consists of **the leader replica and any additional follower replicas that are also considered in-sync**. Followers sends Fetch Requests periodically(by default every 500ms) to leader to sync replicate data. **If a follower fails or lagging behind, after 10 seconds it will be removed from the ISR.** 


=== 1.1.9.3 What is ISR for? ===

The ISR acts as a tradeoff between data safety and latency. 

=== 1.1.9.4 ISR Achilles heel ===

If all followers are going slow, then the ISR might only consist of the leader. So an acks=all message might get acknowledged when only a single replica (the leader) has it. To avoid this, the **min-insync.replicas** in broker/topic configuration needs to be set** to (at least)2** for example. If the ISR does shrink to one replica, then the incoming messages are rejected. It acts as a safety measure for when we care deeply about avoiding message loss.

Note: **The minimum number of in-sync replicas has nothing to do with the throughput**. Setting the minimum number of in-sync replicas to larger than 1 may ensure less or no data loss, but throughput varies depending on the ack value configuration. The default minimum in-sync replicas number is 1.

===== 2. Why Kafka? =====

==== 2.1 Multiple Producers ====

Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. This makes the system ideal for aggregating data from many frontend systems and making it consistent. 

For example, a site that serves content to users via a number of microservices can have a single topic for page views that all services can write to using a common format. Consumer applications can then receive a single stream of page views for all applications on the site without having to coordinate consuming from multiple topics, one for each application. 

==== 2.2 Multiple Consumers ====

In addition to multiple producers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. **This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other.** Multiple Kafka consumers can choose to operate as part of a
group and share a stream, assuring that the entire group processes a given message only once.


==== 2.3 Disk-Based Retention ====

Durable message retention means that **consumers do not always need to work in real-time**. Messages are committed to disk and will be stored with configurable retention rules. If a consumer falls behind or stopped, there is no danger of losing data or blocking the producer. Because the messages will be retained in Kafka. This allows consumers to restart and pick up processing messages where they left off with no data loss.


To avoid large files in the file system which will reduce the read/write performance. Kafka set the maximum size for each topic log file. For example, A topic which has more than 100 million records is stored in three log file.
The name of the log file is the starter offset of the record in each file. 
<code>
00000000000000000000.index
00000000000000000000.log
00000000000000000000.timeindex

00000000000005367851.index
00000000000005367851.log
00000000000005367851.timeindex

00000000000009936472.index
00000000000009936472.log
00000000000009936472.timeindex
</code>

Kafka broker有一个参数，log.segment.bytes，限定了每个日志段文件的大小. You can set any size which is less than 1GB. 一个日志段文件满了，就自动开一个新的日志段文件来写入，避免单个文件过大，影响文件的读写性能，这个过程叫做**log rolling**，正在被写入的那个日志段文件，叫做**active log segment**。


==== 2.4 High performence ====

**Kafka writes to filesystem sequentially** which is fast. On a modern fast drive, Kafka can easily write up to 700 MB or more bytes of data a second. (操作系统每次从磁盘读写数据的时候，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写，如果是机械硬盘，寻址就需要较长的时间。kafka用的是**顺序写**，追加数据是追加到末尾，磁盘顺序写的性能极高，在磁盘个数一定，转数达到一定的情况下，基本和内存速度一致. **随机写**的话寻址就需要较长的时间，性能会较低)

**Kafka scales writes and reads by sharding topic logs into partitions**. Recall topics logs can be split into multiple partitions which can be stored on multiple different servers, and those servers can use multiple disks. Multiple producers can write to different partitions of the same topic. Multiple consumers from multiple consumer groups can read from different partitions efficiently.

==== 2.5 Horizental scalability ====

Kafka’s flexible scalability makes it easy to handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of three brokers, and move into production with a larger cluster of tens or even hundreds of brokers that grows over time as the data scales up.

**Expansions can be performed while the cluster is online**, with no impact on the availability of the system as a whole. This also means that a cluster of multiple brokers can handle the failure of an individual broker, and continue servicing clients. **Clusters that need to tolerate more simultaneous failures can be configured with higher replication factors**.

==== 2.6 Kafka uses zero-copy principal====

先来看看非零拷贝的情况，数据的拷贝从内存拷贝到kafka服务进程那块，又拷贝到socket缓存那块，整个过程耗费的时间比较高。

{{:employes:pengfei.liu:data_science:kafka:kafka_no_zero_copy.png?600|}}



kafka利用了Linux的sendFile技术（NIO），省去了进程切换和一次数据拷贝，让性能变得更好

{{:employes:pengfei.liu:data_science:kafka:kafka_zero_copy.png?600|}}

For more detail on zero-copy principal, plz visit [[employes:pengfei.liu:data_science:kafka:zero_copy|Zero - Copy Principal]]

===== 3. Kafka netwrok architecture =====

kafka的网络设计和Kafka的调优有关，这也是为什么它能支持高并发的原因

{{:employes:pengfei.liu:data_science:kafka:kafka_broker_architecture.png?600|}}


  - 首先客户端发送请求全部会由Acceptor来处理，一个broker里面会存在n个processor线程（默认是3个。Acceptor不会对客户端的请求做任何的处理，直接封装成一个个socketChannel发送给这些processor形成一个队列，发送的方式是轮询（roundrobin），就是先给第一个processor发送，然后再给第二个，第三个，然后又回到第一个。
  - 消费者线程去消费这些socketChannel时，会获取一个个request请求，这些request请求中就会伴随着数据。线程池里面默认有8个线程，这些线程是用来处理request的，解析请求，如果request是写请求，就写到磁盘里。读的话返回结果。
  - processor会从response中读取响应数据，然后再返回给客户端。

这就是Kafka的网络三层架构。所以如果我们需要对kafka进行增强调优，增加processor并增加线程池里面的处理线程，就可以达到效果。request和response那一块部分其实就是起到了一个缓存的效果，是考虑到processor们生成请求太快，线程数不够不能及时处理的问题。

所以这就是一个加强版的reactor网络线程模型。


