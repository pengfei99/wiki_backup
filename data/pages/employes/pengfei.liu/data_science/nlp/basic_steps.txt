====== Basic steps in NLP ======

Nature language processing applications deal with a huge amount of text to perform classification or translation and involve a lot of work on the back end. Transforming text into something an algorithm can digest is a complicated process. In this article, we will discuss the steps involved in text processing.

===== Step 1: Data Preprocessing =====

To make the text readable by the machine learning models, we need to transform them into digits. To avoid noise, we need to clean them by removing stop words, punctuation, etc. The following list shows the key steps in data preprocessing
 
  * Tokenization — convert sentences to words
  * Removing unnecessary punctuation, tags
  * Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic
  * Stemming — words are reduced to root words by removing inflection through dropping unnecessary characters, usually a suffix.
  * Lemmatization — Another approach is to remove inflection by determining the part of speech and utilizing a detailed database of the language.

<code>
The stemmed form of studies is: studi
The stemmed form of studying is: study

The lemmatized form of studies is: study
The lemmatized form of studying is: study
</code>

Thus stemming & lemmatization helps us to reduce words like ‘studies’, ‘studying’ to a common base form or root word ‘study’. For a detailed discussion on Stemming & Lemmatization refer (https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/). Note that not all the steps are mandatory and is based on the application use case. For Spam Filtering, we may follow all the above steps but may not for language translation problem.

===== Step 2: Feature Extraction =====

In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real-valued vectors is called feature extraction. Below is a list of techniques to numerically represent text:
  * **Bag of Words**.
  * 

==== Bag of Words (BOW) ====
: We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary. Another representation is that we count the number of times each word appears in a document. The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.


  * **Term Frequency (TF)** = (Number of times term t appears in a document)/(Number of terms in the document)
  * **Inverse Document Frequency (IDF)** = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.
  * We calculate **TF-IDF** value of a term as = TF * IDF
</code>

Below is an example of text in two documents, let's calculate the tf-idf
{{:employes:pengfei.liu:data_science:nlp:tf-idf.png?400|}}

<code>
TF('beautiful',Document1) = 2/10, IDF('beautiful')=log(2/2) = 0
TF(‘day’,Document1) = 5/10,  IDF(‘day’)=log(2/1) = 0.30

TF-IDF(‘beautiful’, Document1) = (2/10)*0 = 0
TF-IDF(‘day’, Document1) = (5/10)*0.30 = 0.15
</code>

As you can see for Document1, TF-IDF method heavily penalizes the word ‘beautiful’ but assigns greater weight to ‘day’. This is due to the IDF part, which gives more weight to the words that are distinct. In other words, ‘day’ is an important word for Document1 from the context of the entire corpus. 

=== Disadvantages of using BOW ===

One of the major disadvantages of using BOW is that it discards word order thereby ignoring the context and in turn the meaning of words in the document. For natural language processing (NLP) maintaining the context of the words is of utmost importance. To solve this problem we use another approach called **Word Embedding**.

==== Word Embedding ====

Word Embedding: It is a representation of text where words that have the same meaning have a similar representation. In other words, it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together.

Let us discuss some of the well-known models of word embedding:
  * Word2Vec
  * Global Vectors for Word Representation, or GloVe

=== Word2Vec ===

Word2vec takes as its input a large corpus of text and produces a vector space with each unique word being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. Word2Vec is very famous at capturing the meaning and demonstrating it on tasks like calculating analogy questions of the form a is to b as c is to ? 

For example, a man is to woman as an uncle is to? (aunt) using a simple vector offset method based on cosine distance. For example, here are vector offsets for three words pairs illustrating the gender relation:
  * Man -> Woman
  * Uncle -> Aunt
  * King -> Queen

This kind of vector composition also lets us answer “King — Man + Woman = ?” question and arrive at the result “Queen”! All of which is truly remarkable when you think that all of this knowledge simply comes from looking at lots of word in context with no other information provided about their semantics. For more details refer https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/


=== Glove ===

The Global Vectors for Word Representation, or GloVe, the algorithm is an extension to the word2vec method for efficiently learning word vectors. GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may result in generally better word embeddings.

Consider the following example:

{{:employes:pengfei.liu:data_science:nlp:glove.png?400|}}

The **Target words** are ice, steam, and **probe words** are solid, gas, water, fashion.

Let P(k|w) be the probability that the word k appears in the context of word w. Consider a word strongly related to ice, but not to steam, such as solid. P(solid | ice) will be relatively high, and P(solid | steam) will be relatively low. Thus the ratio of P(solid | ice) / P(solid | steam) will be large. If we take a word such as gas that is related to steam but not to ice, the ratio of P(gas | ice) / P(gas | steam) will instead be small. For a word related to both ice and steam, such as water we expect the ratio to be close to one. Refer https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/ for more details.

Word embeddings encode each word into a vector that captures some sort of relation and similarity between words within the text corpus. This means even the variations of words like case, spelling, punctuation, and so on will be automatically learned. In turn, this can mean that some of the text cleaning steps described above may no longer be required.


===== Step 3: Choosing ML Algorithms =====

There are various approaches to building ML models for various text-based applications depending on what is the problem space and data available.

Classical ML approaches like ‘Naive Bayes’ or ‘Support Vector Machines’ for spam filtering has been widely used. Deep learning techniques are giving better results for NLP problems like sentiment analysis and language translation. Deep learning models are very slow to train and it has been seen that for simple text classification problems classical ML approaches as well give similar results with quicker training time.





