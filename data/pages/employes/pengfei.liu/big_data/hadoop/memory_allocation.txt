====== Configure yarn Memory Allocation for hadoop cluster ======

Memory allocation can be tricky on low RAM nodes because default values are not suitable for nodes with less than 8GB of RAM. This section will highlight how memory allocation works for MapReduce jobs, and provide a sample configuration for 2GB RAM nodes.

===== The Memory Allocation Properties =====

A YARN job is executed with two kind of resources:

An Application Master (AM) is responsible for monitoring the application and coordinating distributed executors in the cluster.
Some executors that are created by the AM actually run the job. For a MapReduce jobs, they’ll perform map or reduce operation, in parallel.
Both are run in containers on slave nodes. Each slave node runs a NodeManager daemon that’s responsible for container creation on the node. The whole cluster is managed by a ResourceManager that schedules container allocation on all the slave-nodes, depending on capacity requirements and current charge.

Four types of resource allocations need to be configured properly for the cluster to work. These are:

How much memory can be allocated for YARN containers on a single node. This limit should be higher than all the others; otherwise, container allocation will be rejected and applications will fail. However, it should not be the entire amount of RAM on the node.

This value is configured in yarn-site.xml with yarn.nodemanager.resource.memory-mb.
How much memory a single container can consume and the minimum memory allocation allowed. A container will never be bigger than the maximum, or else allocation will fail and will always be allocated as a multiple of the minimum amount of RAM.

Those values are configured in yarn-site.xml with yarn.scheduler.maximum-allocation-mb and yarn.scheduler.minimum-allocation-mb.
How much memory will be allocated to the ApplicationMaster. This is a constant value that should fit in the container maximum size.

This is configured in mapred-site.xml with yarn.app.mapreduce.am.resource.mb.
How much memory will be allocated to each map or reduce operation. This should be less than the maximum size.

This is configured in mapred-site.xml with properties mapreduce.map.memory.mb and mapreduce.reduce.memory.mb.

The relationship between all those properties can be seen in the following figure:
{{:employes:pengfei.liu:big_data:hadoop:hadoop-2-memory-allocation.png?400|}}


===== Sample Configuration for 2GB Nodes =====

For 2GB nodes, a working configuration may be:

^Property	^Value ^
|yarn.nodemanager.resource.memory-mb	|1536|
|yarn.scheduler.maximum-allocation-mb	|1536|
|yarn.scheduler.minimum-allocation-mb	|128|
|yarn.app.mapreduce.am.resource.mb	|512|
|mapreduce.map.memory.mb	|256|
|mapreduce.reduce.memory.mb	|256|

Edit $HADOOP_HOME/etc/hadoop/yarn-site.xml and add the following lines:
<file xml yarn-site.xml>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
</file>

Edit $HADOOP_HOME/etc/hadoop/mapred-site.xml and add the following lines:

<file xml mapred-site.xml>
<property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
        <name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</file>

====== Yarn memory config on multi node ======

<color #ed1c24>The above config file need to be copied to each node to make it effective.</color>


