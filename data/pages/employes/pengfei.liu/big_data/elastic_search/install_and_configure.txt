====== Install and configure elasticsearch and kibana ======

===== What is ElasticSearch =====


Initially released in 2010, Elasticsearch is a modern search and analytics engine which is based on **Apache Lucene**. Completely open source and built with Java, Elasticsearch is categorized as a **NoSQL database**. That means it stores data in an unstructured way and that you cannot use SQL to query it.

Unlike most NoSQL databases, though, Elasticsearch has a strong focus on search capabilities and features  so much so, in fact, that the easiest way to get data from Elasticsearch is to search for it using its extensive **REST API**.

===== ElasticSearch General feautres =====

  * Elasticsearch is scalable up to petabytes of structured and unstructured data.
  * Elasticsearch can be used as a replacement of document stores like MongoDB and RavenDB.
  * Elasticsearch uses denormalization to improve the search performance.
  * Elasticsearch is one of the popular enterprise search engines, which is currently being used by many big organizations like Wikipedia, The Guardian, StackOverflow, GitHub etc.
  * Elasticsearch is open source and available under the Apache license version 2.0.

===== Key concepts =====

  * **Node** − It refers to a single running instance of Elasticsearch. Single physical and virtual server accommodates multiple nodes depending upon the capabilities of their physical resources like RAM, storage and processing power.
  * **Cluster** − It is a collection of one or more nodes. Cluster provides collective indexing and search capabilities across all the nodes for entire data.
  * **Index** − It is a collection of different type of documents and document properties. Index also uses the concept of shards to improve the performance. For example, a set of document contains data of a social networking application.
  * **Type/Mapping** − It is a collection of documents sharing a set of common fields present in the same index. For example, an Index contains data of a social networking application, and then there can be a specific type for user profile data, another type for messaging data and another for comments data.
  * **Document** − It is a collection of fields in a specific manner defined in JSON format. Every document belongs to a type and resides inside an index. Every document is associated with a unique identifier, called the UID.
  * **Shard** − Indexes are horizontally subdivided into shards. This means each shard contains all the properties of document, but contains less number of JSON objects than index. The horizontal separation makes shard an independent node, which can be store in any node. Primary shard is the original horizontal part of an index and then these primary shards are replicated into replica shards.
  * **Replicas** − Elasticsearch allows a user to create replicas of their indexes and shards. Replication not only helps in increasing the availability of data in case of failure, but also improves the performance of searching by carrying out a parallel search operation in these replicas.

===== Advantage and disadvantage =====

Advantage :

  * Elasticsearch is developed on Java, which makes it compatible on almost every platform.
  * Elasticsearch is real time, in other words after one second the added document is searchable in this engine.
  * Elasticsearch is distributed, which makes it easy to scale and integrate in any big organization.
  * Creating full backups are easy by using the concept of gateway, which is present in Elasticsearch.
  * Handling multi-tenancy is very easy in Elasticsearch when compared to Apache Solr.
  * Elasticsearch uses JSON objects as responses, which makes it possible to invoke the Elasticsearch server with a large number of different programming languages.
  * Elasticsearch supports almost every document type except those that do not support text rendering.

Disadvantage : 

  * Elasticsearch does not have multi-language support in terms of handling request and response data (only possible in JSON) unlike in Apache Solr, where it is possible in CSV, XML and JSON formats.
  * Elasticsearch also have a problem of Split brain situations, but in rare cases.

===== Comparison between ElasticSearch and RDBMS =====

^Elasticsearch	^RDBMS ^
|Index	|Database |
|Shard	|Shard |
|Mapping|Table |
|Field	|Field |
|JSON Object	|Tuple |

===== Install elasticsearch =====

All the following installation guide is tested on centos7.

Pre-requise : JDK 1.8 +. Folllow this tutorial to install JDK : [[employes:pengfei.liu:java:install_jdk|Install oracle jdk on ubuntu 16.04]]

You have many option to install it, Please go see here : https://www.elastic.co/downloads/elasticsearch

In a prod environment, it's better to use rpm to install it.

In our case, we just use the tar ball of the current stable version (elasticsearch-6.1.3.tar.gz). 

Download elasticsearch tar ball and put it in /opt/elasticsearch/ and unzip it.

After you finish this, it should looks like this.

<code>
[root@localhost elasticsearch-6.1.3]# pwd
/opt/elasticsearch/elasticsearch-6.1.3
[root@localhost elasticsearch-6.1.3]# ls
bin  config  lib  LICENSE.txt  logs  modules  NOTICE.txt  plugins  README.textile
</code>

===== Configure ElasticSearch =====


Elasticsearch configurations are done using a configuration file that is located in different locations depending on your operating system. There are too main config files, **elasticsearch.yml** and **jvm.options**. In elasticsearch.yml, you can configure general settings (e.g. node name), as well as network settings (e.g. host and port), where data is stored. In jvm.options you can configure jvm heap memory and more.

For development and testing purposes, the default settings will suffice yet it is recommended you do some research into what settings can be manually defined before going into production.


==== Single node  ElasticSearch config====

For example, and especially if installing Elasticsearch on the cloud, it is a good practice to bind Elasticsearch to either a private IP or localhost:

The following elasticsearch.yml is an example

<code>
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: hdfs_elastic
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: node-1
#
# Add custom attributes to the node:
#
node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: /tmp/elastic/data
#
# Path to log files:
#
path.logs: /tmp/elastic/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.

</code>
==== Run the elasticsearch service ====

If you install it via tar bar as I do. go to elastic home, (/opt/elasticsearch/elasticsearch-6.1.3/), and run
<code>./bin/elasticsearch</code>

=== Common problems of installation ===

== File descriptors limitation ==
After run the elasticsearch, if you see the following errors

<code>
ERROR: [1] bootstrap checks failed
[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]
</code>

you need to reset the process open file limitation at /etc/security/limits.conf. The default value depends on your linux distribution (e.g. centos7 4096, ubuntu 65536)

<code>
# a woring example 
# * means for everyone, if you want to set limit for a user just put uid (e.g. pliu)
# if you want to set limit for a group just put @gid(e.g. @hadoop)
#        - "soft" for enforcing the soft limits
#        - "hard" for enforcing hard limits
# nofile - number of file

* hard nofile 65536
* soft nofile 65536
</code>

Check the maximum open files


<code>
#1. get the elastic process id
ps -u elasticsearch
#output:
PID TTY          TIME CMD
11708 ?        00:00:10 java

#2. get the max open file numbers
cat /proc/11708/limits | grep 'Max open files'

#output: 
Max open files            65535                65535                files
</code>

== max virtual memory areas ==

if you see the following errors 

<code>
[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
</code>


You need to do reset the max virtual memory areas

<code>
sysctl -w vm.max_map_count=262144
</code>

==== Multi node ElasticSearch cluster config ====


Elastic search has build in cluster mode. So it's really easy to build to elastic search cluster

In this example, we suppose we have 3 server cluster

^IP^URI^ elastic_node_name ^
|10.70.3.48 | hadoop-nn.bioaster.org | node-1|
|10.70.3.49 | hadoop-dn1.bioaster.org | node-2 |
|10.70.3.50 | hadoop-dn2.bioaster.org | node-3 |

Install the elasticsearch tar ball as above. Configure the elasticsearch.yml for each node

<code>
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster: 
# It must be the same for the 3 node
cluster.name: hdfs_elastic
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
# node name must be unique inside the cluster
# in our example, it will be node-1, node-2, node-3
node.name: node-1
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: /tmp/elastic/data
#
# Path to log files:
#
path.logs: /tmp/elastic/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
# The network host should be the ip address or uri of the node
# This will force elastic search only answer request from this uri or ip address
# This is also security concern. the elasticsearch is completely
# driven by rest api without any security. The ip address must be protected (private ip or vpn ) 
# which can be accessed only by trusted people and service
# Do not bind Elasticsearch to a public or shared private network IP address!
# Note the addition of "_local_", which configures Elasticsearch to also listen on all loopback devices
# such as localhost or 127.0.0.1
network.host: [10.70.3.48,_local_]
#
# Set a custom port for HTTP:
#
http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
# In this line we put uri or ip address of all the nodes in the elastic cluster.
discovery.zen.ping.unicast.hosts: ["hadoop-nn.bioaster.org", "hadoop-dn1.bioaster.org","hadoop-dn2.bioaster.org"]
#
# Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.

</code>

=== Dedicated Master nodes and data nodes ===

There are two common types of Elasticsearch nodes: master and data. Master nodes perform cluster-wide actions, such as managing indices and determining which data nodes should store particular data shards. Data nodes hold shards of your indexed documents, and handle CRUD, search, and aggregation operations. As a general rule, data nodes consume a significant amount of CPU, memory, and I/O.

By default, every Elasticsearch node is configured to be a "master-eligible" data node, which means they store data (and perform resource-intensive operations) and have the potential to be elected as a master node. For a small cluster, this is usually fine; a large Elasticsearch cluster, however, should be configured with dedicated master nodes so that the master node's stability can't be compromised by intensive data node work.

== How to Configure Dedicated Master Nodes ==

Before configuring dedicated master nodes, ensure that your cluster will have at least 3 master-eligible nodes. This is important to avoid a split-brain situation, which can cause inconsistencies in your data in the event of a network failure.

<code>
#For master node, edit the elasticsearch.yml
node.master: true 
node.data: false

#for data node 
node.master: false 
node.data: true
</code>

== Configure Minimum Master Nodes ==
When running an Elasticsearch cluster, it is important to set the minimum number of master-eligible nodes that need to be running for the cluster to function normally, which is sometimes referred to as quorum. This is to ensure data consistency in the event that one or more nodes lose connectivity to the rest of the cluster, preventing what is known as a "split-brain" situation.

To calculate the number of minimum master nodes your cluster should have, calculate n / 2 + 1, where n is the total number of "master-eligible" nodes in your healthy cluster, then round the result down to the nearest integer. For example, for a 3-node cluster, the quorum is 2.

**Note:** Be sure to include all master-eligible nodes in your quorum calculation, including any data nodes that are master-eligible (default setting).

The minimum master nodes setting can be set dynamically, through the Elasticsearch HTTP API. To do so, run this command on any node (replace the highlighted number with your quorum):

<code>
# use curl to request rest api
curl -XPUT localhost:9200/_cluster/settings?pretty -d '{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}'

#output

{
  "acknowledged" : true,
  "persistent" : {
    "discovery" : {
      "zen" : {
        "minimum_master_nodes" : "2"
      }
    }
  },
  "transient" : { }
}
</code>

=== Enable Memory Locking ===

Elastic recommends to avoid swapping the Elasticsearch process at all costs, due to its negative effects on performance and stability. One way avoid excessive swapping is to configure Elasticsearch to lock the memory that it needs.

Complete this step on all of your Elasticsearch servers.

<code>
sudo vim elasticsearch.yml

bootstrap.mlockall: true

sudo vim jvm.options 

# Xms represents the initial size of total heap space
# Xmx represents the maximum size of total heap space

-Xms2g
-Xmx4g

</code>

=== Verify the memory lock status ===

<code>
curl http://localhost:9200/_nodes/process?pretty

#output
"nodes" : {
    "kQgZZUXATkSpduZxNwHfYQ" : {
      "name" : "node-1",
      "transport_address" : "10.70.3.48:9300",
      "host" : "10.70.3.48",
      "ip" : "10.70.3.48",
      "version" : "2.2.0",
      "build" : "8ff36d1",
      "http_address" : "10.70.3.48:9200",
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 1650,
        "mlockall" : true
      }

</code>
==== Run the cluster ====
You need to start the elastic deamon or do ./bin/elasticsearch on all your nodes in the cluster

==== Test the status of cluster ====

Test the status of the elastic cluster.

<code>
[hadoop@CCLinDataWHD01 pliu]$ curl -XGET 'http://localhost:9200/_cluster/state?pretty'
{
  "cluster_name" : "hdfs_elastic",
  "compressed_size_in_bytes" : 334,
  "version" : 4,
  "state_uuid" : "5HNWvEDMSlyeG2yZ92acUQ",
  "master_node" : "F-aJixNFSIiSYp5N_IAeLQ",
  "blocks" : { },
  "nodes" : {
    "7j1T8NMSTque9lBcE1Pd-Q" : {
      "name" : "node-2",
      "ephemeral_id" : "ai9NiqoRQSaU1qa8UfMufQ",
      "transport_address" : "10.70.3.49:9300",
      "attributes" : { }
    },
    "F-aJixNFSIiSYp5N_IAeLQ" : {
      "name" : "node-1",
      "ephemeral_id" : "qs5yizBvSOaec-SBcSrO7Q",
      "transport_address" : "10.70.3.48:9300",
      "attributes" : { }
    },
    "t6U8Kf_NSQSnsmY4tmNWKA" : {
      "name" : "node-3",
      "ephemeral_id" : "_okE5-fzRyK8VXlhSxwLIQ",
      "transport_address" : "10.70.3.50:9300",
      "attributes" : { }
    }
  },
  "metadata" : {
    "cluster_uuid" : "Sono_FJpRM6Q4T31H6qKDg",
    "templates" : { },
    "indices" : { },
    "index-graveyard" : {
      "tombstones" : [ ]
    }
  },
  "routing_table" : {
    "indices" : { }
  },
  "routing_nodes" : {
    "unassigned" : [ ],
    "nodes" : {
      "t6U8Kf_NSQSnsmY4tmNWKA" : [ ],
      "F-aJixNFSIiSYp5N_IAeLQ" : [ ],
      "7j1T8NMSTque9lBcE1Pd-Q" : [ ]
    }
  },
  "snapshot_deletions" : {
    "snapshot_deletions" : [ ]
  },
  "snapshots" : {
    "snapshots" : [ ]
  },
  "restore" : {
    "snapshots" : [ ]
  }
}

</code>