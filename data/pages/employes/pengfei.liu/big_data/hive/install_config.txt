====== Install and configure Hive ======

===== Introduction of Hive =====

Hive is a data warehouse infrastructure tool to process structured data in Hadoop. It resides on top of Hadoop to summarize Big Data, and makes querying and analyzing easy.

Initially Hive was developed by Facebook, later the Apache Software Foundation took it up and developed it further as an open source under the name Apache Hive. It is used by different companies. For example, Amazon uses it in Amazon Elastic MapReduce.

Hive is not : 
  - A relational database
  - A design for OnLine Transaction Processing (OLTP)
  - A language for real-time queries and row-level updates

Features of Hive :
  - It stores schema in a database and processed data into HDFS.
  - It is designed for OLAP.
  - It provides SQL type language for querying called HiveQL or HQL.
  - It is familiar, fast, scalable, and extensible.

==== Architecture of Hive ====

The following component diagram depicts the architecture of Hive:

{{:employes:pengfei.liu:big_data:hive:hive_architecture.jpg?400|}}

**User Interface** : -> Hive is a data warehouse infrastructure software that can create interaction between user and HDFS. The user interfaces that Hive supports are Hive Web UI, Hive command line, and Hive HD Insight (In Windows server). 

Meta Store : -> Hive chooses respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping.

HiveQL Process Engine : -> HiveQL is similar to SQL for querying on schema info on the Metastore. It is one of the replacements of traditional approach for MapReduce program. Instead of writing MapReduce program in Java, we can write a query for MapReduce job and process it.

Execution Engine : -> The conjunction part of HiveQL process Engine and MapReduce is Hive Execution Engine. Execution engine processes the query and generates results as same as MapReduce results. It uses the flavor of MapReduce.

HDFS or HBASE : -> Hadoop distributed file system or HBASE are the data storage techniques to store data into file system.

==== Working of Hive ====

The following diagram depicts the workflow between Hive and Hadoop

{{:employes:pengfei.liu:big_data:hive:how_hive_works.jpg?400|}}

Step 1 -> Execute Query
The Hive interface such as Command Line or Web UI sends query to Driver (any database driver such as JDBC, ODBC, etc.) to execute.

Step 2 -> Get Plan
The driver takes the help of query compiler that parses the query to check the syntax and query plan or the requirement of query.

Step 3 -> Get Metadata
The compiler sends metadata request to Metastore (any database).


Step 4 -> Send Metadata
Metastore sends metadata as a response to the compiler.


Step 5 -> Send Plan
The compiler checks the requirement and resends the plan to the driver. Up to here, the parsing and compiling of a query is complete.


Step 6-> Execute Plan
The driver sends the execute plan to the execution engine.


Step 7 -> Execute Job
Internally, the process of execution job is a MapReduce job. The execution engine sends the job to JobTracker, which is in Name node and it assigns this job to TaskTracker, which is in Data node. Here, the query executes MapReduce job.


Step 7.1 -> Metadata Ops
Meanwhile in execution, the execution engine can execute metadata operations with Metastore.


Step 8 -> Fetch Result
The execution engine receives the results from Data nodes.


Step 9 -> Send Results
The execution engine sends those resultant values to the driver.


Step 10 -> Send Results
The driver sends the results to Hive Interfaces.


===== Installation =====

As we said before, Hive works on top of Hadoop, so before you install hive, you need to install Hadoop first. [[employes:pengfei.liu:big_data:hadoop:install_config_hadoop|Install hdfs on multi node cluster]]

After you install java, hadoop, you can follow this guide to download the latest stable version.

https://hive.apache.org/downloads.html

The current stable version is 2.3.2

==== Install hive ====

<code>
mkdir -p /opt/hive

#put the download tar.gz and untar it and rename it.
#Suppose we call it hive-2.3.2

#home of hive
/opt/hive/hive-2.3.2 

#create environment variable
vim /etc/profile.d/hive.sh

#put the following lines
#!/bin/bash

export HIVE_HOME=/opt/hive/hive-2.3.2
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:.

# check your hadoop environment, if hadoop home is not set add the following
export HADOOP_HOME=/opt/hadoop/hadoop-2.8.2
export HADOOP_CONF_DIR=/opt/hadoop/hadoop-2.8.2/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.

</code>

==== Install RMDB for hive metastore ====

Now hive is installed, but hive metastore needs a RMDB to store metadata. So we need to install a RMDB. In our case, we installed a postgresql server. Follow this doc to install it [[employes:pengfei.liu:admin_system:centos7_postgres95|Install postgresql on centos 7]]  .

Suppose you already have postgresql server installed

<code>
# create user hive with password hive in postgres

$ sudo -u postgres psql

postgres=# CREATE USER hive WITH PASSWORD 'hive';

# create database metastore
postgres=# CREATE DATABASE metastore;

# grant all privilege of database metastore to user hive
GRANT ALL PRIVILEGES ON DATABASE metastore TO hive;

# test connectivity
psql -h 127.0.0.1 -p 5432 -U hive -d metastore

# Copy jdbc driver to hive lib, in this example, we use postgresql driver
cp postgresql-42.2.5.jar /opt/hive/hive-2.3.3/lib/.
</code>

==== Configure hive to use hdfs and metastore ====

There are too main conf file you need to modify

The hive-env.sh  conf

<code>
cp hive-env.sh.template hive-env.sh

vim /opt/hive/hive/hive-2.3.2/hive-env.sh

#put the following lines
# The heap size of the jvm stared by hive shell script can be controlled via:
#
export HADOOP_HEAPSIZE=1024
# Set HADOOP_HOME to point to a specific hadoop install directory
HADOOP_HOME=/opt/hadoop/hadoop

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/hive/hive-2.3.2/conf

</code>


The hive-site.xml

<code>
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<property>
    <name>hive.exec.scratchdir</name>
    <value>/tmp/hive</value>
</property>

<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://127.0.0.1:9000/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:postgresql://127.0.0.1:5432/metastore</value>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>org.postgresql.Driver</value>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
</property>


<property>
    <name>javax.jdo.option.Multithreaded</name>
    <value>true</value>
</property>


</configuration>


</code>

hive.exec.scratchdir -> defines the location of temporal data during the Hive mapr operation

hive.metastore.warehouse.dir -> defines the location of hive data warehouse

The rest is to connect sql server

==== Create dir in hdfs ====

<code>

# create dir in hdfs
hdfs dfs -mkdir /tmp/hive
hdfs dfs -mkdir /user/hive/warehouse

#grant access in hdfs
hdfs dfs -chmod g+w /tmp
hdfs dfs -chmod g+w /user/hive/warehouse

</code>

Now make sure hive can access hdfs, On node where you are running hive service, add below property in your hdfs config core-site.xml

<code>
<property>
     <name>hadoop.proxyuser.$user_name.hosts</name>
     <value>*</value>
</property>
<property>
     <name>hadoop.proxyuser.$user_name.groups</name>
     <value>*</value>
</property>
</code>

==== Initial hive DB structure ====

<code>
cd /opt/hive/hive-2.3.2
/bin/schematool -dbType postgres -initSchema
</code>

==== Run hive in console ====
Before you run hive, you need to make sure your hdfs and yarn is activate

<code>
[hadoop@localhost hadoop]$ jps
8548 Jps
8329 RunJar
4986 NameNode
6170 ResourceManager
5371 SecondaryNameNode
6285 NodeManager
5119 DataNode
</code>

You also need to make sure your meta data store is activate. In our case , it's postgresql

<code>
[pliu@localhost taobao_data_set]$ sudo systemctl status postgresql-9.5.service 
‚óè postgresql-9.5.service - PostgreSQL 9.5 database server
   Loaded: loaded (/usr/lib/systemd/system/postgresql-9.5.service; disabled; vendor preset: disabled)
   Active: active (running) since Wed 2018-02-14 16:01:19 CET; 5min ago
     Docs: https://www.postgresql.org/docs/9.5/static/
</code>

If you have added hive home in your path you can just run hive
<code>
[hadoop@localhost bin]$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/hive/hive-2.3.2/lib/hive-common-2.3.2.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> show tables;
OK
customercustomerdemo
customerdemographics
......
</code>
==== Run metastore and hive as daemons  ====

To use hive shell, the below daemons must be running, otherwise, you may receive the following errors
<code>
# errors in hive shell, when metastore daemons is not running
FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient

</code>

<code>
hive --service metastore --hiveconf hive.log.dir=$HIVE_HOME/logs --hiveconf hive.log.file=metastore.log >/dev/null 2>&1 &

hive --service hiveserver2 --hiveconf hive.log.dir=$HIVE_HOME/logs --hiveconf hive.log.file=hs2.log >/dev/null 2>&1 &
</code>

==== Test hive operation ====


<code>
hive> create database hive;
OK

hive> CREATE TABLE IF NOT EXISTS employees ( eid int, name String,salary String, destination String) COMMENT 'Employee details' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE;
OK
Time taken: 0.295 seconds

</code>

We will use the following sample data to learn some basic operation in hive.
If you want to copy it, make sure the white space is '\t' not other things 
<code>
1201	Gopal	45000	Technical_manager
1202	Manisha	45000	Proof_reader
1203	Masthanvali	40000	Technical_writer
1204	Kiran	40000	Hr_Admin
1205	Kranthi	30000	Op_Admin
</code>

==== Load data into table ====

<code>
hive> LOAD DATA LOCAL INPATH '/tmp/hive_test_data/sample.txt' OVERWRITE INTO TABLE employees;
Loading data to table default.employees
OK
Time taken: 3.614 seconds

#Test your loaded data
hive> select * from employees where salary > 40000;
OK
1201	Gopal	45000	Technical_manager
1202	Manisha	45000	Proof_reader
Time taken: 0.361 seconds, Fetched: 2 row(s)

</code>

===== Use beeline to access hive =====

<code>
# open a beeline terminal
beeline
# connect to hive 
!connect jdbc:hive2://localhost:10000/default

# enter username and password, this username will be used by hive to connect to hdfs, in our example, I use hadoop as username, and empty password

# if you see the following error, it means your hive cannot use the input username to connect hdfs
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: hadoop is not allowed to impersonate  (state=08S01,code=0)

# To solve the above error, you need to reconfig hdfs to accept connection of user hadoop
vim core-site.xml

# add the following line
<property>
     <name>hadoop.proxyuser.hadoop.hosts</name> 
     <value>*</value> 
</property> 
<property>
     <name>hadoop.proxyuser.hadoop.groups</name>
     <value>*</value>
</property>

#note that the above line only works for user hadoop, if you want to use foobar as username, you need to add another lines, I tried with generic config hadoop.proxyuser.$user_name.hosts, it doesnot work
</code>

===== Remote access of hive  =====

If you want to use hive client to access a remote hive server, you need to active the hive server daemon.
for hive 1.x, the service name is hiveserver. For hive 2.x, the service name is hiveserver2
<code>
> hive --service hiveserver2

2018-08-17 09:35:44: Starting HiveServer2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/hive/hive-2.3.2/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

</code>


===== Multi node hive setup =====


As all metadata of hive is stored in hive metastore(physically it's a relational database, in our case, it's postgresql). So if your node2 is not using metastore of node1. node2 can't read the table of node1.

First, you need to start the metastore service in node1 

<code>
# start the metastore service, the default port is 9083, you can specify the port by using option -p
hive --service metastore

#check the port
netstat -ln | grep 9083
</code>

Seconde, in node2 we need to setup hive-site.xml for node2 can connect to metastore of node1.
<code>
<property>
   <name>hive.metastore.uris</name>
   <value>thrift://node1(or IP Address):9083</value>
   <description>IP address (or fully-qualified domain name) and port of the metastore host</description>
</property>
</code>
 
Now in node 2, with hive-cli, we can access the tables in node1.

====== FAQs ======

<code>
# 1. When you enter hive, if you see this exception
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D

# solution: You need to add the following line at the beginning of hive-site.xml

<property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
  </property>
# This is caused by the following config in the hive-site.xml
<property>
    <name>hive.exec.local.scratchdir</name>
    <value>${system:java.io.tmpdir}/${system:user.name}</value>
    <description>Local scratch space for Hive jobs</description>
  </property>
  <property>

</code>
 
===== Access control =====

To control which user can use hive, add the following line in hive-site.xml

<code>
<property>
     <name>hadoop.proxyuser.$user_name.hosts</name>
     <value>*</value>
</property>
<property>
     <name>hadoop.proxyuser.$user_name.groups</name>
     <value>*</value>
</property>
</code>
$user_name - specify the username which will have access to hive beeline command line