====== Data formats Introduction ======

In this article, we will first introduce the three main data categories (e.g. Structured Data, unstructured Data, semi-structured Data). Then we will present popular file format. At last we will discuss how to choose data formats based on your requirements.

===== 1. Data categories =====

We have many different data formats. They can all be classified into the following three categories.

  * Structured Data
  * Unstructured Data
  * Semi-structured Data

==== 1.1 Structured Data ====

A **data model** is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.

The term **data model** is used in two distinct but closely related senses. Sometimes it refers to an abstract formalization of the objects and relationships found in a particular application domain, for example the customers, products, and orders found in a manufacturing organization. At other times it refers to a set of concepts used in defining such formalizations: for example concepts such as entities, attributes, relations, or tables. So the "data model" of a banking application may be defined using the entity-relationship "data model". We use the term in both senses. 

All data which obey the formal structure of **data models** are considered as structured data.  

For example, data stored in the relational database in the form of tables having multiple rows and columns. The spreadsheet(excel has more controls than csv on the columns) is another good example of structured data.

Check this for more info about data model: https://en.wikipedia.org/wiki/Data_model#Database_model

==== 1.2 Semi-structured Data ====

Semi-structured data is a form of structured data that does not obey the formal structure of **data models** associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. 

JSON(JavaScript Object Notation) files, BibTex files, .csv files, tab-delimited text files, XML and other markup languages are examples of Semi-structured data.

=== Advantages ===
  * Programmers persisting objects from their application to a database do not need to worry about object-relational impedance mismatch, but can often serialize objects via a light-weight library.
  * Support for nested or hierarchical data often simplifies data models representing complex relationships between entities.
  * Support for lists of objects simplifies data models by avoiding messy translations of lists into a relational data model.

=== Disadvantages ===
  * The traditional relational data model has a popular and ready-made query language, SQL.
  * Prone to "garbage in, garbage out"; by removing restraints from the data model, there is less fore-thought that is necessary to operate a data application.

==== 1.3 Unstructured Data =====

Unstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.

For example, e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents are all unstructured data.


===== 2. Basics of data formats =====

==== 2.1 Criteria for evaluating a data format ====

^ Property ^ CSV ^ Json/Xml ^Parquet^ Avro^ ORC^
| Human Readable |YES|YES|NO|NO|NO|
| Compressable |YES|YES|YES|YES|YES|
| Splittable |YES*|YES*|YES|YES|YES|
| Complex data structure |NO|YES|YES|YES|YES|
| Schema evolution |NO|NO|YES|YES|YES|
| Columnar |NO|NO|YES|NO|YES|

Note:
  * CSV is splittable when it is a raw, uncompressed file or using a splittable compression format such as BZIP2 or LZO (note: LZO needs to be indexed to be splittable!)
  * JSON has the same conditions about splittability when compressed as CSV with one extra difference. When “wholeFile” option is set to true (re: SPARK-18352), JSON is NOT splittable.

==== 2.2 Logical and physical====

When we talk about data format, we always have two-layer:
  * The **logical layer** represents how data is seen by the user. For example, the logical layer of tabular data is represented by rows and columns.
  * The physical layer represents how data is stored on a physical device(e.g. hard drive, memory, etc.)

The physical layer has three major solutions:
  * row-wise:
  * columnar:   
  * Hybrid:

Based on how they store data, they will behave differently in different scenarios. So we don't have one best solution which works for all scenarios. 



===== 3. Common data formats in big data environment =====

==== 2.1 CSV ====

CSV files (comma separated values) are commonly used to exchange tabular data between systems using plain text. CSV is a row-based file format, which means that every line of the file is the row in the table. Basically, CSV contains a header row that provides column names for the data, otherwise, files are considered **partially structured**. CSV files cannot initially present hierarchical or relational data. Data connections are typically organized by using multiple CSV files. Foreign keys are stored in columns of one or more files, but the links between these files are not expressed by the format itself. Also, CSV format is not fully standardized, files can use delimiters other than commas, such as tabs or spaces.
One of the other properties of CSV files is that they are only splittable when it is a raw, uncompressed file or when splittable compression format is used such as bzip2 or lzo (note: lzo needs to be indexed to be splittable!).

=== 2.1.1 Advantages: ===

  * CSV is human-readable and easy to edit manually;
  * CSV provides a straightforward information schema;
  * CSV is processed by almost all existing applications;
  * CSV is simple to implement and parse;
  * CSV is compact. For XML you start tag and end tag for each column in each row. In CSV you write the column headers only once.

=== 2.1.2 Disadvantages ===

  * CSV allows working with flat data. Complex data structures need to be handled aside from format;
  * No support for column types. There is no distinction between text and numeric columns;
  * No standard way to represent binary data;
  * Problems with importing CSV (no distinction between NULL and quotes);
  * Poor support of special characters;
  * Lack of universal standard.

Despite the limitations, CSV files are a popular choice for data sharing, as they are supported by a wide range of business applications, consumer and scientific applications. Similarly, most batch and streaming data processing modules (for example, Spark and Hadoop) initially support serialization and deserialization of CSV files and offer ways to add a schema when reading.

=== Application environment ===

Supported by all applications and languages. One of the most popular formats used to exchange data, because of its simplicity, but usually not recommanded for feeding data analytic process.



==== 2.2 JSON ====

JSON data (JavaScript object notation) is represented as key-value pairs in a partially structured format. JSON is often compared to XML because it can store data in a hierarchical format. Child data is presented by parent data. Both formats are self-describing and readable by the user, but JSON documents are usually much smaller. Therefore, they are more often used in network communication, especially with the advent of REST-based web services.


Since a lot of data transferring is already using JSON format, most web languages initially support JSON or use external libraries to serialize and deserialize JSON data. Thanks to this support, JSON is used in logical formats by presenting data structures, interchange formats for hot data, and cold data stores.


Many batches and stream data processing modules natively support JSON serialization and deserialization. Although the data contained in JSON documents can ultimately be stored in more performance-optimized formats, such as Parquet or Avro, they serve as raw data, which is very important for reprocessing data (if necessary).


=== Advantages: ===

  * JSON supports hierarchical structures, simplifying the storage of related data in one document and the presentation of complex relations;
  * Most languages provide simplified JSON serialization libraries or built-in support for JSON serialization/deserialization;
  * JSON supports lists of objects, helping to avoid erratic transformations of lists into a relational data model;
  * JSON is a widely used file format for NoSQL databases such as MongoDB, Couchbase and Azure Cosmos DB;
  * Built-in support in most nowadays tools.


SON is not splittable and lack indexing as many text formats
Ecosystems

It is privileged format for web applications
JSON is a widely used file format for NoSQL databases such as MongoDB, Couchbase and Azure Cosmos DB. or example,in MongoDB, BSON is used instead the standard format. This rchitecture of MongoDB is easier to modify than structured database as Postgres where you’d need to extract the whole document for changing. ndeed, BSON is the binary encoding of JSON-like documents. It’s indexed and allows to be parsed much ore quickly than standard JSON. However, this doesn’t mean you can’t think of MongoDB as a JSON atabase.
JSON is also an used language in GraphQL on which rely a data fully typed. GraphQL is a query language for an API. Instead of working with rigid server-defined endpoints, you can send queries to get exactly the data you are looking for in one request.
==== 2.3 Parquet ====

Launched in 2013, Parquet was developed by Cloudera and Twitter to serve as a column-based storage format, optimized for work with multi-column datasets. Because data is stored by columns, it can be highly compressed (compression algorithms perform better on data with low information entropy which is usually contained in columns) and splittable. The developers of the format claim that this storage format is ideal for Big Data problems.

**Parquet files are binary files that contain metadata about their content**. So, without reading/parsing the content of the file(s), Spark can just rely on the metadata to determine column names, compression/encodings, data types and even some basic statistics. **The column metadata for a Parquet file is stored at the end of the file, which allows for fast, one-pass writing.**

**Parquet is optimized for the Write Once Read Many (WORM) paradigm. It’s slow to write, but incredibly fast to read, especially when you’re only accessing a subset of the total columns. Parquet is a good choice for read-heavy workloads. For use cases requiring operating on entire rows of data, a format like CSV or AVRO should be used.**

=== Advantages ===

  * Parquet is a columnar format. Only required columns would be fetched/read, it reduces the disk I/O. This concept is called projection pushdown;
  * Schema travels with the data so data is self-describing;
  * Despite the fact that it is created for HDFS, data can be stored in other file systems, such as GlusterFs or on top of NFS;
  * Parquet are just files, which means that it is easy to work with them, move, back up and replicate;
  * Native support in Spark out of the box provides the ability to simply take and save the file to your storage;
  * Parquet provides very good compression up to 75% when used even with the compression formats like snappy;
  * As practice shows, this format is the fastest for reading workflows compared to other file formats;
  * Parquet is well suited for data warehouse kind of solutions where aggregations are required on certain column over a huge set of data;
  * Parquet can be read and write using Avro API and Avro Schema(which gives the idea to store all raw data in Avro format but all processed data in Parquet);
  * It also provides predicate pushdown, thus reducing further disk I/O cost.

=== Disadvantages: ===

  * The column-based design makes you think about the schema and data types;
  * Parquet does not always have native support in other tools other than Spark;
  * Does not support data modification and schematic evolution. Of course, Spark knows how to merge the scheme, if you change it over time (you need to specify a special option when reading). But to change something in an already existing file, you can do nothing other than overwriting, except that you can add a new column.


==== 2.4 Avro ====

Apache Avro was released by the Hadoop working group in 2009. It is a row-based format that is highly splittable. It also described as a data serialization system similar to Java Serialization. The schema is stored in JSON format while the data is stored in binary format, minimizing file size and maximizing efficiency. Avro has robust support for schema evolution by managing added fields, missing fields, and fields that have changed. This allows old software to read the new data and new software to read the old data — a critical feature if your data has the potential to change.

With Avro’s capacity to manage schema evolution, it’s possible to update components independently, at different times, with low risk of incompatibility. This saves applications from having to write if-else statements to process different schema versions and saves the developer from having to look at old code to understand old schemas. Because all versions of the schema are stored in a human-readable JSON header, it’s easy to understand all the fields that you have available.

Avro can support many different programming languages. Because the schema is stored in JSON while the data is in binary, Avro is a relatively compact option for both persistent data storage and wire transfer. Avro is typically the format of choice for write-heavy workloads given its easy to append new rows.

=== Advantages: ===

  * Avro is language-neutral data serialization
  * Avro stores the schema in the header of the file so data is self-describing;
  * Avro formatted files are splittable and compressible and hence it’s a good candidate for data storage in Hadoop ecosystem;
  * The schema used to read an Avro file need not be the same as schema which was used to write the files. This makes it possible to add new fields independently.
  * Just as with Sequence Files, Avro files also contains Sync markers to separate the blocks. This makes it highly splittable;
  * These blocks can be compressed using compression formats such as snappy.




==== Sequence files ====
==== ORC ====


===== Appendix: Nested data structure =====

Json, Parquet, Avro, and ORC allows table cells to host nested complex data structure. For example, in the following table, we have a column called name, which is an object which has three properties(first-name, middle-name, last-name).

<code>
val structureData = Seq(
    Row(Row("James ","","Smith"),"36636","M",3100),
    Row(Row("Michael ","Rose",""),"40288","M",4300),
    Row(Row("Robert ","","Williams"),"42114","M",1400),
    Row(Row("Maria ","Anne","Jones"),"39192","F",5500),
    Row(Row("Jen","Mary","Brown"),"","F",-1)
  )

val structureSchema = new StructType()
    .add("name",new StructType()
      .add("firstname",StringType)
      .add("middlename",StringType)
      .add("lastname",StringType))
    .add("id",StringType)
    .add("gender",StringType)
    .add("salary",IntegerType)

  val df2 = spark.createDataFrame(
     spark.sparkContext.parallelize(structureData),structureSchema)
  df2.printSchema()
  df2.show()
</code> 

There is a negative consequence of keeping the nested structure in parquet. In some cases, spark predicate pushdown may not work properly if you have a nested structure in the parquet file.

So even if you are working with few fields in your parquet dataset spark will load and materialize the entire dataset.