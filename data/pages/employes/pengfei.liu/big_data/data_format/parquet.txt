====== Data format: parquet ======


Launched in 2013, Parquet was developed by Cloudera and Twitter to serve as a column-based storage format, optimized for work with multi-column datasets. Because data is stored by columns, it can be highly compressed (compression algorithms perform better on data with low information entropy which is usually contained in columns) and splittable. The developers of the format claim that this storage format is ideal for Big Data problems.

**Parquet files are binary files that contain metadata about their content**. So, without reading/parsing the content of the file(s), Spark can just rely on the metadata to determine column names, compression/encodings, data types and even some basic statistics. **The column metadata for a Parquet file is stored at the end of the file, which allows for fast, one-pass writing.**

**Parquet is optimized for the Write Once Read Many (WORM) paradigm. It’s slow to write, but incredibly fast to read, especially when you’re only accessing a subset of the total columns. Parquet is a good choice for read-heavy workloads. For use cases requiring operating on entire rows of data, a format like CSV or AVRO should be used.**

=== Advantages ===

  * Parquet is a columnar format. Only required columns would be fetched/read, it reduces the disk I/O. This concept is called projection pushdown;
  * Schema travels with the data so data is self-describing;
  * Despite the fact that it is created for HDFS, data can be stored in other file systems, such as GlusterFs or on top of NFS;
  * Parquet are just files, which means that it is easy to work with them, move, back up and replicate;
  * Native support in Spark out of the box provides the ability to simply take and save the file to your storage;
  * Parquet provides very good compression up to 75% when used even with the compression formats like snappy;
  * As practice shows, this format is the fastest for reading workflows compared to other file formats;
  * Parquet is well suited for data warehouse kind of solutions where aggregations are required on certain column over a huge set of data;
  * Parquet can be read and write using Avro API and Avro Schema(which gives the idea to store all raw data in Avro format but all processed data in Parquet);
  * It also provides predicate pushdown, thus reducing further disk I/O cost.

=== Disadvantages: ===

  * The column-based design makes you think about the schema and data types;
  * Parquet does not always have native support in other tools other than Spark;
  * Does not support data modification and schematic evolution. Of course, Spark knows how to merge the scheme, if you change it over time (you need to specify a special option when reading). But to change something in an already existing file, you can do nothing other than overwriting, except that you can add a new column.

==== Accelerating ORC and Parquet Reads of s3 ====

https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_cloud-data-access/content/s3-orc-parquet-reads.html

==== Parquet loses compression ratio after repartitioning ====

Context: I have a data set imported by using Sqoop ImportTool as-parquet-file using codec snappy. I just read the origin parquet file and repartition the data frame and generate a new parquet file. The size of the new parquet file went up to 200GB. But the origin was 80GB.

The data processing logic
<code>
val productDF = spark.read.parquet("/tmp/sample1")

productDF
.repartition(100)
.write.mode(org.apache.spark.sql.SaveMode.Overwrite)
.option("compression", "snappy")
.parquet("/tmp/sample2")

</code>

=== Cause: ===

This happened because when you call repartition(n) on a data frame **you are doing a round-robin partitioning**. Any data locality that existed before the repartitioning is gone. So the compression codecs can't compress data very well, and the run length and dictionary encoders don't really have much to work with.

=== Solution: ===

so when you do your repartition, you need to repartition your data frame base on a good column that preserves data locality.

<code>
repartition (n, col)
</code>

Also, if you are optimizing your sqooped tables for downstream jobs you can sortWithinPartition for faster scans.
<code>
df.repartition(100, $"userId").sortWithinPartitions("userId").write.parquet(...)
</code>


====== Appendix: Parquet tools ======

The "parquet tools" is a part of the parquet project. It comes in form of a jar file. You can use it to explore the parquet file by using a command line.
 
See these pages for more details about Parquet Tools:
  * https://github.com/apache/parquet-mr/tree/master/parquet-tools-deprecated
  * https://mvnrepository.com/artifact/org.apache.parquet/parquet-tools


===== 2. Usage =====

  * (hadoop): hadoop jar parquet-tools-*.jar COMMAND [GENERIC-OPTIONS] [COMMAND-OPTIONS] PARUQET-FILE-PATH
  * (local): java -jar parquet-tools-*.jar COMMAND [GENERIC-OPTIONS] [COMMAND-OPTIONS] PARUQET-FILE-PATH

**Note that the local mode does not work for version 1.11.1.** 
 
Available command:
  * cat :  Prints out content for a given parquet file.
  * head : Prints out the first n records for a given parquet file (default: 5).
  * schema : Prints out the schema for a given parquet file.
  * meta : Prints out metadata for a given parquet file.
  * dump : Prints out row groups and metadata for a given parquet file.
  * merge : Merges multiple Parquet files into one Parquet file.

Available GENERIC-OPTIONS:
  * --debug     |  Enable debug output.
  * -h,--help   |  Show this help string.
  * --no-color  |  Disable color output even if supported.

===== 3. Command example =====

==== 3.1 Cat ====
It prints out content for a given parquet file
<code>
# cat a file in hdfs, -j means output in json format
hadoop jar parquet-tools.jar cat -j hdfs://pengfei.org:9000/toto.parquet

# cat a flie in local fs
hadoop jar parquet-tools.jar cat --json file:///home/pliu/data_set/toto.parquet
</code>

==== 3.2 Head ====

Prints out the first n records(default: 5) for a given parquet file 

<code>
# you can change the default value of n by using -n
hadoop jar parquet-tools.jar head -n 10 file:///home/pliu/data_set/toto.parquet
</code>

==== 3.3 schema ====
Prints the schema of parquet file
<code>
# you can get more details by using -d, it will print the meta data
hadoop jar parquet-tools.jar schema -d file:///home/pliu/data_set/toto.parquet
</code>

==== 3.4 meta ====
Prints the meta data of parquet file
<code>
hadoop jar parquet-tools.jar meta file:///home/pliu/data_set/toto.parquet
</code>

==== 3.5 dump ====
Prints out row groups and metadata for a given parquet file. You have the following option to choose
  * -c,--column <arg>  |  Dump only the given column, can be specified more than once.
  * -d,--disable-data  |  Do not dump column data.
  * -m,--disable-meta  |  Do not dump row group and page metadata.
  * -n,--disable-crop  |  Do not crop the output based on console width. 
<code>
hadoop jar parquet-tools.jar dump file:///home/pliu/data_set/toto.parquet
</code>

==== 3.6 merge ====
Merges multiple Parquet files into one Parquet file.

<code>
hadoop jar parquet-tools.jar merge file:///home/pliu/data_set/toto.parquet file:///home/pliu/data_set/titi.parquet
</code>


===== 4. python version parquet tools =====

The python version does not work well. 
<code>
# install the tool via pypi
pip install parquet-tools

# get help
parquet-tools --help

# show the details of the parquet file
parquet-tools show test.parquet
parquet-tools show s3://bucket-name/prefix/*

# 
parquet-tools inspect /path/to/parquet
</code>
