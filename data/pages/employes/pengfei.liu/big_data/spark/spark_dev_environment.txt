====== Install a devlopment envirionment for spark ======

===== Spark dev env with Intellij and Maven =====

1. Pre-requise

Download a IntelliJ IDEA, install jdk and scala sdk,

First install scala plugin in IntelliJ.

Then you add jdk and scala sdk into Intellij.

To add jdk and scala sdk into IntelliJ, you open IntelliJ click on <color #ed1c24>Configure -> Project default -> Project structure</color> . After this it should open a window, now click on Global libraries-> +, choose the location where you have installed your jdk and sdk.

After all this, you have a Intellij for scala development environment.

To create a spark-scala project

You need 
  - Create a Intellij scala project
  - add maven framework support to the project

In the main/java folder create the following file
<file scala WordCount.scala>
import org.apache.spark.{SparkConf, SparkContext}


object WordCount {

  def main(args: Array[String]): Unit ={
    val inputFile = "file:///tmp/word.txt"
    val conf = new SparkConf().setAppName("WordCount").setMaster("local")
    val sc = new SparkContext(conf)
    val textFile = sc.textFile(inputFile)
    val wordCount = textFile.flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey((a,b)=>a+b)
    wordCount.foreach(println)
    wordCount.saveAsTextFile("file:///tmp/wordCount")
    sc.close()
  }

}
</file>

Change the pom.xml as following 

<file xml pom.xml>
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.pengfei</groupId>
    <artifactId>spark-tutor</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <spark.version>2.1.0</spark.version>
        <scala.version>2.11</scala.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>

            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.19</version>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>

        </plugins>
    </build>
</project>
</file>
In intelliJ, you can write click on the .scala file and choose Run <file-name>. It will lauch the scala script on spark cluster. 

You can also use maven to build a jar file of project WordCount, we name it wordCount.jar

To run it on spark with mode cluster 

<code>
spark-submit --class WordCount --master local[*] wordCount.jar
</code>

===== Launching Applications with spark-submit =====

The jar can be build based on scala or java source, In this section, we exam the spark-submit with more details.

 
<code>
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
</code>

Some of the commonly used options are:

<code>
  * --class: The entry point for your application (e.g. org.apache.spark.examples.SparkPi)
  * --master: The master URL for the cluster (e.g. ''spark://23.195.26.187:7077'')
  * --deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client)
  * --conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown).
  * application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.
  * application-arguments: Arguments passed to the main method of your main class, if any
</code>


The following command uses a jar file came from spark distribution and call a sparkPi class, it prints a pi value in the output
 
<code>
spark-submit --class org.apache.spark.examples.SparkPi --master local[*] /opt/spark/spark-2.2.0/examples/jars/spark-examples_2.11-2.2.0.jar
</code>