====== Spark read and write parquet in file system ======


Apache Parquet is implemented using the record-shredding and assembly algorithm, which accommodates the complex data structures that can be used to store the data. The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:

  * Column-wise compression is efficient and saves storage space
  * Compression techniques specific to a type can be applied as the column values tend to be of the same type
  * Queries that fetch specific column values need not read the entire row data thus improving performance
  * Different encoding techniques can be applied to different columns

Moreover, Apache Parquet is implemented using the Apache Thrift framework which increases its flexibility; it can work with a number of programming languages like C++, Java, Python, PHP, etc.

As of August 2015, Parquet supports the big-data-processing frameworks including Apache Hive, Apache Drill, Apache Impala, Apache Crunch, Apache Pig, Cascading and Apache Spark.


===== Write data to parquet file =====

We re use the data example of [[employes:pengfei.liu:big_data:spark:spark_usecase:sf_client_satisfaction|Happy customers]] 

We suppose the name of dataframe is df.

<code>
import spark.implicits._

# one way to do it
df.write.format("parquet").save("file:///tmp/happyClient.parquet")

#another way to do it
scala> df.write.parquet("file:///tmp/happyClient.parquet")

#the content of happyClient.parquet
cd /tmp/happyClient.parquet
[hadoop@localhost happyClient.parquet]$ ls
part-00000-43ff1107-22f3-4de5-9d07-25f3a38a8cf7-c000.snappy.parquet  _SUCCESS
# part is actual data, _SUCCESS is the output of spark job
</code>

===== Read data from parquet file =====
To read parquet file generated by spark, you only need to give the folder path.

<code>
#open a new spark shell consol

scala> import spark.implicits._
import spark.implicits._

#
scala> val df= spark.read.parquet("file:///tmp/happyClient.parquet")
df: org.apache.spark.sql.DataFrame = [manager_name: string, client_name: string ... 4 more fields]

scala> df.show
+---------------+-------------+-------------+----------+-------------+------------------+
|   manager_name|  client_name|client_gender|client_age|response_time|satisfaction_level|
+---------------+-------------+-------------+----------+-------------+------------------+
|    Arjun Kumar|  Rehan Nigam|         male|        30|          4.0|               0.5|
|     Kabir Vish| Abhinav Neel|         male|        28|         12.0|               0.1|
|    Arjun Kumar|    Sam Mehta|         male|        27|          3.0|               0.7|
|    Arjun Kumar|    Ira Pawan|       female|        40|          2.5|               0.6|
|Rohit Srivastav| Vihaan Sahni|         male|        38|          6.0|               0.5|
|     Kabir Vish|Daivik Saxena|         male|        45|         16.0|               0.2|
|Rohit Srivastav|   Lera Uddin|       female|        20|          8.0|               0.4|
|Rohit Srivastav|   Aaran Puri|         male|        34|          7.5|               0.3|
|     Kabir Vish|   Rudra Mati|         male|        50|         20.0|               0.1|
+---------------+-------------+-------------+----------+-------------+------------------+

</code>


