====== Install and use pyspark  ======

===== 1.Install python dependencies =====

==== 1.1 Install conda ====

To install and use conda, please go check [[employes:pengfei.liu:python:conda:start|Anaconda documents]]

==== 1.2 Create a conda virtual env for pyspark ====

<code>
# list existing env
conda env list

# create an env
conda create --name spark --channel conda-forge python=3.8

# activate env
source activate spark

# deactivate env
conda deactivate
</code>



==== 1.3 install required packages ====

Before we start, we need to check python, pip version
<code>
python --version

pip --version
</code>


<code>
# Install Jupyter 
pip install jupyter

# Install py4j
pip install py4j
</code>


===== 2 Install Java and Scala =====

To install Java, please check [[employes:pengfei.liu:java:install_jdk|Install oracle jdk on ubuntu 16.04]]

To install scala, please check [[employes:pengfei.liu:java:scala|Install scala on centos]]

===== 3. Install spark =====

Install Apache Spark; go to the Spark download page (http://spark.apache.org/downloads.html) and choose the latest (default) version. I am using Spark 3.0.1 with Hadoop 2.7. After downloading, unpack it in the location you want to use it.

===== 4. Edit the .bashrc =====

<code>
# Create a new env setup file in /etc/profile.d/spark.sh
touch /etc/profile.d/spark.sh
</code>

We need to add the following var env. Here I suppose you already set up your JAVA_HOME. If not, you need to set up JAVA_HOME.

If you want to use pyspark inside jupyter, you can use version 1
<code>
#### spark env version 1
export SPARK_HOME="/opt/Spark/spark-3.0.1"
# pyspark config
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
# if you put jupyter, the pyspark will try to use jupyter to run your code.
# if you don't want to use jupyter. you can put python in the place of juypter.
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
export PYSPARK_PYTHON=python3
export PATH=$SPARK_HOME:$PATH:~/.local/bin:$JAVA_HOME/bin:$JAVA_HOME/jre/bin
</code>

If you don't want to use pyspark via jupyter. you can use version 2

<code>
#### spark env version 2
export SPARK_HOME="/opt/Spark/spark-3.0.1"
# pyspark config
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="python"
export PYSPARK_PYTHON=python3
export PATH=$SPARK_HOME:$PATH:~/.local/bin:$JAVA_HOME/bin:$JAVA_HOME/jre/bin
</code>
===== 5. Test your pyspark via Jupyter =====

<code>
# start your jupyter
jupyter notebook

# choose the python3 kernel, and run the following code
%%time


from pyspark.sql import SparkSession
spark=SparkSession.builder.appName("MyApp").getOrCreate()
df = spark.read.csv('/home/pliu/data_set/flight.csv',inferSchema=True,header=True)

df.show(5)
print(df.count())
</code>

===== 6. Jupyeter Kernel for spark  =====

==== 6.1 Toree ====

Note since Toree 0.3.0, pyspark interpreter has been removed. So we can't use it anymore

<code>
pip install --upgrade toree

jupyter toree install --spark_home /home/pliu/Tools/spark/spark-3.0.1 --interpreters=Scala,SQL --user
</code>

For each interpreter, a kernel spec will be generated. For example, the following spec folder is for scala interpreter

<code>
# scala interpreter spec folder,  
Installed kernelspec apache_toree_scala in /home/pliu/.local/share/jupyter/kernels/apache_toree_scala

# You can find the following contents in it

(base) [pliu@localhost spark-3.0.1]$ ls -lah /home/pliu/.local/share/jupyter/kernels/apache_toree_scala
total 68K
drwxrwxr-x 5 pliu pliu  221 Mar 17 14:50 .
drwxrwxr-x 4 pliu pliu   56 Mar 17 14:40 ..
drwxrwxr-x 2 pliu pliu   20 Mar 17 14:27 bin
-rw-rw-r-- 1 pliu pliu  537 Mar 17 14:27 DISCLAIMER
-rw-rw-r-- 1 pliu pliu  812 Mar 17 14:27 __init__.py
-rw-rw-r-- 1 pliu pliu  415 Mar 17 14:40 kernel.json
drwxrwxr-x 2 pliu pliu   49 Mar 17 14:27 lib
-rw-rw-r-- 1 pliu pliu  31K Mar 17 14:27 logo-64x64.png
-rw-rw-r-- 1 pliu pliu  898 Mar 17 14:27 __main__.py
drwxrwxr-x 2 pliu pliu  130 Mar 17 14:27 __pycache__
-rw-rw-r-- 1 pliu pliu 2.6K Mar 17 14:27 RELEASE_NOTES.md
-rw-rw-r-- 1 pliu pliu 6.3K Mar 17 14:27 toreeapp.py
-rw-rw-r-- 1 pliu pliu   47 Mar 17 14:27 VERSION
-rw-rw-r-- 1 pliu pliu  920 Mar 17 14:27 _version.py

</code>

The most important is kernel.json. This is the default conf 

<file json kernel.json>
{
  "argv": [
    "/home/pliu/.local/share/jupyter/kernels/apache_toree_scala/bin/run.sh",
    "--profile",
    "{connection_file}"
  ],
  "env": {
    "DEFAULT_INTERPRETER": "Scala",
    "__TOREE_SPARK_OPTS__": "",
    "__TOREE_OPTS__": "",
    "SPARK_HOME": "/home/pliu/Tools/spark/spark-3.0.1"
  },
  "display_name": "Apache Toree - Scala",
  "language": "scala",
  "interrupt_mode": "signal",
  "metadata": {}
}

</file>

If you want to change the spec of the **spark context/session, you need to modify the "TOREE_SPARK_OPTS"**

For example, if you want to use a remote spark cluster, you can add the following code

<file json remote.json>
{
  "argv": [
    "/home/pliu/.local/share/jupyter/kernels/apache_toree_scala/bin/run.sh",
    "--profile",
    "{connection_file}"
  ],
  "env": {
    "DEFAULT_INTERPRETER": "Scala",
    "__TOREE_SPARK_OPTS__": "",
    "__TOREE_OPTS__": "",
    "__TOREE_SPARK_OPTS__": "--master spark://192.168.0.255:7077 --deploy-mode client --num-executors 4 --executor-memory 4g --executor-cores 8 --packages org.apache.spark:spark-avro_2.12:3.0.1"
  },
  "display_name": "Apache Toree - Scala",
  "language": "scala",
  "interrupt_mode": "signal",
  "metadata": {}
}

</file>

**Note that we will create a spark session which connects to a Spark cluster at (192.168.0.255:7077) with client deploy mode, with 4 executors(8 core,4g memore), and a external avro package**

So if you want to have two scala interpreter, one for local, one for remote, you can do the following steps:
<code>
# 1. copy the local one
cp -pr /home/pliu/.local/share/jupyter/kernels/apache_toree_scala /home/pliu/.local/share/jupyter/kernels/scala_remote

# 2. Edit the kernel.json file, put the option of your spark session in __TOREE_SPARK_OPTS__

# 3. restart your jupyter
</code>


==== 6.2 Add spylon kernel into Jupyter for scala support ====

<code>
# Install package in your virtual env
pip install spylon-kernel

# Add kernel to the jupyter
python -m spylon_kernel install

# start the jupyter notebook
ipython notebook

# in the web gui, click on kernel-> New-> spylon-kernel
# This will connect your note book to the spylon-kernel

</code>

Note the spylon-kernel will use SPARK_HOME env var to locate the spark executable. So make sure you set it right. 

<code>
# Put the following code in your notebook to test the kernel
val data = Seq((1,2,3), (4,5,6), (6,7,8), (9,19,10))
val ds = spark.createDataset(data)
ds.show()

# If everything goes well, you should see the following output
Intitializing Scala interpreter ...
Spark Web UI available at http://172.22.0.33:4040
SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1601362749037)
SparkSession available as 'spark'

</code>
====== Use pyspark in pycharm ======






===== 1. pycharm embedded interpreter=====

By default, when you create a pycharm project, it will automatically create a virtual env. You can install pyspark in the dedicated virtual env with the following steps(Only works for Spark 2.2.0 and later).


Go to File -> Settings -> Project Interpreter -> Click on the + button (top right) and search for PySpark


===== 2. Manually add user-provided Spark installation to pycharm ====

Create Run configuration:

Go to Run -> Edit configurations
Add new Python configuration
Set Script path so it points to the script you want to execute
Edit Environment variables field so it contains at least:

SPARK_HOME - it should point to the directory with Spark installation. It should contain directories such as bin (with spark-submit, spark-shell, etc.) and conf (with spark-defaults.conf, spark-env.sh, etc.)
PYTHONPATH - it should contain $SPARK_HOME/python and optionally $SPARK_HOME/python/lib/py4j-some-version.src.zip if not available otherwise. some-version should match Py4J version used by a given Spark installation (0.8.2.1 - 1.5, 0.9 - 1.6, 0.10.3 - 2.0, 0.10.4 - 2.1, 0.10.4 - 2.2, 0.10.6 - 2.3)

Apply the settings

Add PySpark library to the interpreter path (required for code completion):

Go to File -> Settings -> Project Interpreter
Open settings for an interpreter you want to use with Spark
Edit interpreter paths so it contains path to $SPARK_HOME/python (an Py4J if required)
Save the settings
Optionally
Install or add to path type annotations matching installed Spark version to get better completion and static error detection (Disclaimer - I am an author of the project).

