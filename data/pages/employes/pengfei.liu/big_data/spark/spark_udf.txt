====== Spark user define function ======

In spark dataframe/dataset dsl or sql, if user define function is not registered, we cant not use it

For example, we have  a following dataset, the seconde column is in Byte

<code>
+-----+-----------+
|  Uid|      SizeH|
+-----+-----------+
|42029|    5279390| 
|42557|       9602| 
|42968|59175620627|
|42566|42226912277| 
|42556|  146878220|
+-----+-----------+
</code>

We want transform the Byte into a humain readable way (e.g. KB,MB,GB,PB,etc)

So we have the following scala object who can do the job

<file scala getSize.scala>
 def getSize(rawSize:Long): String ={
    val unit:Array[String]=Array("B","KB","MB","GB","TB")
    var index=0
    var tmpSize:Long=rawSize
    while(tmpSize>=1024){
      tmpSize=tmpSize/1024
      index+=1
    }
   return tmpSize+unit(index)
  }
</file>

But when we call it in a dataset pipeline , 
<code>
#suppose df is the dataframe of above
val finaldf=df.withColumn("Size", expr("getSize(SizeH)"))
</code>
it shows error Undefined function: 'getSize'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 0
The following example registers a Scala closure as UDF:


   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
 
The following example registers a UDF in Java:


   sparkSession.udf().register("myUDF",
       new UDF2<Integer, String, String>() {
           @Override
           public String call(Integer arg1, String arg2) {
               return arg2 + arg1;
           }
      }, DataTypes.StringType);
 
Or, to use Java 8 lambda syntax:


   sparkSession.udf().register("myUDF",
       (Integer arg1, String arg2) -> arg2 + arg1,
       DataTypes.StringType);