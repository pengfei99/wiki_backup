====== DataFrame in SPARK (vs RDD) ======

===== Data frame introduction =====

DataFrame appeared in Spark Release 1.3.0. We can term DataFrame as Dataset organized into named columns. DataFrames are similar to the table in a relational database or data frame in R /Python. It can be said as a relational table with good optimization technique.

===== Difference between dataframe and RDD =====

RDD is a distributed storage of java object. As a result, RDD is not able to know the parameters inside person. Data frame is based on RDD, but it contains schema of the data, so you can have all information about object person. Since Spark 2.0, DataFrame is implemented as a special case of Dataset.

{{:employes:pengfei.liu:big_data:spark:dataframe-rdd.jpg?400|}}

DataFrame has two main advantages over RDD:

  * Optimized execution plans via Catalyst Optimizer.
  * Custom Memory management via Project Tungsten.
===== create simple dataFrame in scala spark shell =====
There are many ways to create data frame in spark  

  * Use spark sqlContext
  * Use Spark sql session

<code>
#import dependencies of sql context
scala> import org.apache.spark.sql.SQLContext

#create sql context
scala> val sqlContext = new SQLContext(sc)

#create data frame
scala> val sentenceDataFrame = sqlContext.createDataFrame(Seq(
     |       (0, "Hi I heard about Spark"),
     |       (1, "I wish Java could use case classes"),
     |       (2, "Logistic,regression,models,are,neat")
     |     )).toDF("label", "sentence")
sentenceDataFrame: org.apache.spark.sql.DataFrame = [label: int, sentence: string]

#create tokenizer based on data frame
scala> val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")

#show the label and words of first 3 row in tokenizer
scala> tokenized.select("words", "label").take(3).foreach(println)

</code>

Now, let's see how we create data frame with spark session

<code>
# import dependencies
scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> val spark=SparkSession.builder().getOrCreate()
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@75d0cac6

scala> import spark.implicits._
import spark.implicits._

scala> val df = spark.read.json("file:///tmp/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.collect
res0: Array[org.apache.spark.sql.Row] = Array([29,Michael], [30,Andy], [19,Justin])

scala> df.show
+---+-------+
|age|   name|
+---+-------+
| 29|Michael|
| 30|   Andy|
| 19| Justin|
+---+-------+

</code>

===== Basic dataframe command =====

  * select
  * filter
  * groupBy
  * sort

<code>
scala> df.printSchema()
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
 
// select and change column value
scala> df.select(df("name"),df("age")+1).show()
+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+
 
// filter with condition
scala> df.filter(df("age") > 20 ).show()
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+
 
// 
scala> df.groupBy("age").count().show()
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+
 
// sort one column
scala> df.sort(df("age").desc).show()
+----+-------+
| age|   name|
+----+-------+
|  30|   Andy|
|  19| Justin|
|null|Michael|
+----+-------+
 
//sort mutli column
scala> df.sort(df("age").desc, df("name").asc).show()
+----+-------+
| age|   name|
+----+-------+
|  30|   Andy|
|  19| Justin|
|null|Michael|
+----+-------+
 
//change column name
scala> df.select(df("name").as("username"),df("age")).show()
+--------+----+
|username| age|
+--------+----+
| Michael|null|
|    Andy|  30|
|  Justin|  19|
+--------+----+
</code>


===== Data frame real life application =====
[[employes:pengfei.liu:big_data:spark:spark_usecase:sf_client_satisfaction|Happy customers]]
