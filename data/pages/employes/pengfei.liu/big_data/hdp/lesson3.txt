====== Lesson3: Handle unstructured data ======

In this tutorial, we will learn how to use spark to read unstructured data and convert them to structure data.

===== 3.1 Data source =====

The source data is generated by an apache web server, which is the access log of a web site. The following shows the first five lines of the file.

<code>
79.133.215.123 - - [14/Jun/2014:10:30:13 -0400] "GET /home HTTP/1.1" 200 1671 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36"                                                                                                            

162.235.161.200 - - [14/Jun/2014:10:30:13 -0400] "GET /department/apparel/category/featured%20shops/product/adidas%20Kids'%20RG%20III%20Mid%20Football%20Cleat HTTP/1.1" 200 1175 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.76.4 (KHTML, like Gecko) Version/7.0.4 Safari/537.76.4"|

39.244.91.133 - - [14/Jun/2014:10:30:14 -0400] "GET /department/fitness HTTP/1.1" 200 1435 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36"                                                                                    

150.47.54.136 - - [14/Jun/2014:10:30:14 -0400] "GET /department/fan%20shop/category/water%20sports/product/Pelican%20Sunstream%20100%20Kayak/add_to_cart HTTP/1.1" 200 1932 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36"

217.89.36.129 - - [14/Jun/2014:10:30:14 -0400] "GET /view_cart HTTP/1.1" 200 1401 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0" 
</code>

===== 3.2 Read the log file and give it a schema =====

<code scala>
// Read raw data
val filePath = s"${path}/access.log.2"
val rawDf = spark.read.text(filePath)

// split raw data into a token list
val splitDf = rawDf.select(split(col("value"), " ").as("tokenList"))

// transform token list into dataframe
val tokenizedDf = splitDf.withColumn("host", $"tokenList".getItem(0))
      .withColumn("rfc931", $"tokenList".getItem(1))
      .withColumn("authuser", $"tokenList".getItem(2))
      .withColumn("date", concat($"tokenList".getItem(3), $"tokenList".getItem(4)))
      .withColumn("request", $"tokenList".getItem(6))
      .withColumn("status", $"tokenList".getItem(8))
      .withColumn("bytes", $"tokenList".getItem(9))
      .drop("tokenList")

tokenizedDf.show(5, false)

</code>

===== 3.3 Analyze the data =====

<code scala>
//get all the response status of the request
tokenizedDf.select("status").distinct()

// get the top ten most visit product
 val mostViewedPage = tokenizedDf.filter($"request".contains("product")).groupBy($"request").count().orderBy($"count".desc)
</code>

===== 3.4 Clean the data =====

In the previous step, we have the top 10 most visit product, but they are not clean, we want to keep only 
the product name and the visit count

<code scala>
 /* If we want to replace the 20% by space in the request, we can use the regexp_replace*/
    val betterView = mostViewedPage.select(regexp_replace($"request", "%20", " ").alias("request"), $"count")
    
     /* refine data frame, only keep product name, and rename column name*/

    /*Here we use a interesting spark sql function substring_index to get the product name
    * substring_index(str, delim, count) : Returns the substring from str before count occurrences of the delimiter
    *          delim. If count is positive, everything to the left of the final delimiter (counting from the left) is
    *          returned. If count is negative, everything to the right of the final delimiter (counting from the right)
    *          is returned.
    *
    * For example, if we want to keep the head of the string(www), then we do the following
    * SELECT substring_index('www.apache.org', '.', 1);
    * If we want to keep the tail of the string(org), then we do the following
    * SELECT substring_index('www.apache.org', '.', -1);
    * */
   /* After analysis, we found we have false data in access log, so we want to remove all lines which has "add_to_cart"
    * as product_name, we use filter() which takes boolean expression as argument, notice we can't use ! for negation
    * here, we need to use not()
    * */
    val productVisitNumber = betterView
        .withColumn("product_name",substring_index(col("request"),"/",-1))
        .withColumnRenamed("count","view_number")
        .drop("request")
        .filter(not($"product_name".contains("add_to_cart")))
        .select("product_name","view_number")
</code>

===== 3.5 Save the result to hive =====

In HDP 3.0 and later, Spark and Hive use independent catalogs for accessing SparkSQL or Hive tables on the same or different platforms. A table created by Spark resides in the Spark catalog. A table created by Hive resides in the Hive catalog. Databases fall under the catalog namespace, similar to how tables belong to a database namespace. Although independent, these tables interoperate and you can see Spark tables in the Hive catalog, but only when using the Hive Warehouse Connector.


You can use the Hive Warehouse Connector (HWC) API to access any type of table in the Hive catalog from Spark. When you use SparkSQL, standard Spark APIs access tables in the Spark catalog.

Using HWC, you can export tables and extracts from the Spark catalog to Hive and from the Hive catalog to Spark. You export tables and extracts from the Spark catalog to Hive by reading them using Spark APIs and writing them to the Hive catalog using the HWC. You must use low-latency analytical processing (LLAP) in HiveServer Interactive to read ACID, or other Hive-managed tables, from Spark. You do not need LLAP to write to ACID, or other managed tables, from Spark. You do not need HWC to access external tables from Spark.

Using the HWC, you can read and write Apache Spark DataFrames and Streaming DataFrames. Apache Ranger and the HiveWarehouseConnector library provide row and column, fine-grained access to the data.

Limitations
  * HWC supports tables in ORC format only.
  * The spark thrift server is not supported.

Check https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/integrating-hive/content/hive_hivewarehouseconnector_for_handling_apache_spark_data.html
==== Write data frame as hive table in spark catalog ====

<code scala>
// list data base and table in spark catalog
spark.catalog.listDatabases().show(false)
spark.catalog.listTables().show(false)

//create table 
spark.sql("CREATE TABLE product_visit_number (product_name String, view_number Int)")

//write data frame to table
productVisitNumber.write.mode(SaveMode.Overwrite).saveAsTable("product_visit_number")

//show data in the table
spark.sql("select * from product_visit_number limit 10").show()
</code>

==== Export csv and load to hive catalog ====

The other workaround is export data frame to CSV and loads CSV with the following process

Step1: Export data as csv
<code scala>
// function to write data in hdfs
def WriteDataToDisk(df:DataFrame,outputPath:String,fileName:String): Unit ={
    df.coalesce(1).write.mode(SaveMode.Overwrite)
      .option("header","true")
      .option("mapreduce.fileoutputcommitter.marksuccessfuljobs","false") //Avoid creating of crc files
      .option("encoding", "UTF-8")
      .option("delimiter", ",") 
      .csv(outputPath+"/"+fileName)
  }
  
// call function
WriteDataToDisk(productVisitNumber,"/tmp/demo_data","product_visit_number")  
</code>

Step2: Create a table inside the hive
<code>
# The table must use STORED AS TEXTFILE, by default in hive table is stored as orc.
# But we can't load csv data into orc table 
CREATE TABLE product_visit_number(product_name STRING, view_number INT) row format delimited fields terminated BY ',' lines terminated BY '\n' STORED AS TEXTFILE tblproperties("skip.header.line.count"="1"); 

</code> 

Step3: load data into the table
<code>
LOAD DATA INPATH "/tmp/demo_data/product_visit_number/data.csv" OVERWRITE INTO TABLE product_visit_number;

</code>

===== 3.6 Full code for creating the product visit number dataframe=====

<file scala AccessLog.scala>
import com.typesafe.config.ConfigFactory
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object AccessLog {

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    val spark = SparkSession.builder().master("local[2]").appName("Lesson4_Exec04_Parse_Apache_Access_Log").getOrCreate()

    import spark.implicits._
    val sparkConfig = ConfigFactory.load("application.conf").getConfig("spark")
    val path = sparkConfig.getString("sourceDataPath")
    val filePath = s"${path}/spark_lessons/Lesson04_Spark_SQL/access.log.2"
    // Read raw data
    val rawDf = spark.read.text(filePath)

    rawDf.cache()
    rawDf.show(5, false)
    rawDf.count()

    // split raw data into token list
    val splitDf = rawDf.select(split(col("value"), " ").as("tokenList"))
    splitDf.show(5, false)

    // transform token list into dataframe
    val tokenizedDf = splitDf.withColumn("host", $"tokenList".getItem(0))
      .withColumn("rfc931", $"tokenList".getItem(1))
      .withColumn("authuser", $"tokenList".getItem(2))
      .withColumn("date", concat($"tokenList".getItem(3), $"tokenList".getItem(4)))
      .withColumn("request", $"tokenList".getItem(6))
      .withColumn("status", $"tokenList".getItem(8))
      .withColumn("bytes", $"tokenList".getItem(9))
      .drop("tokenList")

    tokenizedDf.show(5, false)

    tokenizedDf.select("status").distinct()

    /* The following request give us the top ten most visited page. We could noticed that the second most viewed item is not in the top 10 sell list */
    val mostViewedPage = tokenizedDf.filter($"request".contains("product")).groupBy($"request").count().orderBy($"count".desc)

    mostViewedPage.show(10, false)

    /* If we want to replace the 20% by space in the request, we can use the regexp_replace*/
    val betterView = mostViewedPage.select(regexp_replace($"request", "%20", " ").alias("request"), $"count")
    betterView.show(10, false)

    /* refine data frame, only keep product name, and rename column name*/

    /*Here we use a interesting spark sql function substring_index to get the product name
    * substring_index(str, delim, count) : Returns the substring from str before count occurrences of the delimiter
    *          delim. If count is positive, everything to the left of the final delimiter (counting from the left) is
    *          returned. If count is negative, everything to the right of the final delimiter (counting from the right)
    *          is returned.
    *
    * For example, if we want to keep the head of the string(www), then we do the following
    * SELECT substring_index('www.apache.org', '.', 1);
    * If we want to keep the tail of the string(org), then we do the following
    * SELECT substring_index('www.apache.org', '.', -1);
    * */
   /* After analysis, we found we have false data in access log, so we want to remove all lines which has "add_to_cart"
    * as product_name, we use filter() which takes boolean expression as argument, notice we can't use ! for negation
    * here, we need to use not()
    * */
    val productVisitNumber = betterView
        .withColumn("product_name",substring_index(col("request"),"/",-1))
        .withColumnRenamed("count","view_number")
        .drop("request")
        .filter(not($"product_name".contains("add_to_cart")))
        .select("product_name","view_number")

    productVisitNumber.show(10,false)

  }


}

</file>

===== 3.7 Join with the product_revenu to find the anomaly =====

Here we will reuse the table products_revenue which we created in Lesson1. Normally, there is a positive between product visit number and product revenue. If not, we could say there is an anomaly.
<code>
select pn.product_name as product_name, pn.view_number as view_number, pr.revenue as revenue 
from product_visit_number pn 
left join products_revenue pr on pn.product_name=pr.product_name
order by pn.view_number desc;

</code>

We can also save the result as a table

<code>
CREATE TABLE retail_db.product_anomaly STORED AS ORC AS select pn.product_name as product_name, pn.view_number as view_number, pr.revenue as revenue 
from product_visit_number pn 
left join products_revenue pr on pn.product_name=pr.product_name
order by pn.view_number desc;
</code>

====== Common problems ======

===== 1.File format does not match when load data into hive table =====

==== Symptom ====

After creating a table, a user imports data to the table by running the Load command but encounters the following error:
<code>
.......
> LOAD DATA INPATH '/user/tester1/hive-data/data.txt' INTO TABLE employees_info;
Error: Error while compiling statement: FAILED: SemanticException Unable to load data to destination table. Error: The file that you are trying to load does not match the file format of the destination table. (state=42000,code=40000)
..........
</code>

==== Fault locating ====

  - The customer has not specified the storage format of data when creating the table. The default format for the hive table is RCFile.
  - However, the loaded data is in TEXTFILE format.

==== Solution ====

Two solutions are available and they both focus on the format consistency between data stored and data imported.

Method 1:

Specify the storage format at the time of table creation with **STORED AS TEXTFILE**. This allows data in the TEXTFILE format to be imported.

Method 2:

Import data in RCFile format.


