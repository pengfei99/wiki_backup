====== Lesson1: Handling structured data ======

In this Lesson, we will use sqoop to extract a database which is stored in a mysql server. Sqoop will inject this database into hive data warehouse. Then we will do some simple analysis.


===== 1. Connect to the mysql database =====

<code>
# View database (retail_db) in the mysql database (pwd:hadoop)
mysql -u root -p retail_db

# check database tables 
show tables;

# show 10 distinct customers
select distinct * from customers limit 10;
</code>

===== 2. Use sqoop to import database =====

For more information about sqoop on import data, see [[employes:pengfei.liu:data_science:sqoop:import_data|Sqoop import data from DB]]


<code>
# 1. view the data in mysql server

# 2. create db in hive
$ create database retail_db

# 3. import data via sqoop
$ sqoop import-all-tables --connect jdbc:mysql://127.0.0.1:3306/retail_db --username=root -P --compression-codec=snappy --hive-overwrite --hive-import --hive-database retail_db 

# In our case, we can't use --direct, becasue mysqldump is not installed on the server. Note that with different DB, the --direct means different things. In mysql/mariadb, postgres, it means use mysqldump/pgdump to bypass jdbc. In oracle

# 4. login to beeline and view the imported data

</code>

===== 3. Use hive to analyse data =====

<code>
# 1. Top ten best seller
select c.category_name as category_name, count(order_item_quantity) as product_number
from order_items oi
inner join products p on oi.order_item_product_id = p.product_id
inner join categories c on c.category_id = p.product_category_id
group by c.category_name
order by product_number desc
limit 10;

# 2. top 10 revenue generating products, if you want more precision, you can change int to float
select p.product_name as product_name, r.revenue as revenue
from products p inner join
(select oi.order_item_product_id, sum(cast(oi.order_item_subtotal as int)) as revenue
from order_items oi inner join orders o
on oi.order_item_order_id = o.order_id
where o.order_status <> 'CANCELED'
and o.order_status <> 'SUSPECTED_FRAUD'
group by order_item_product_id) r
on p.product_id = r.order_item_product_id
order by r.revenue desc
limit 10;

</code>

===== 4. Save result to hive  =====

<code>
# save the best seller result to a new table
CREATE TABLE retail_db.best_seller STORED AS ORC AS select c.category_name as category_name, count(order_item_quantity) as product_number
from order_items oi
inner join products p on oi.order_item_product_id = p.product_id
inner join categories c on c.category_id = p.product_category_id
group by c.category_name
order by product_number desc;

# save the revenue value generated by each products
CREATE TABLE retail_db.products_revenue STORED AS ORC AS select p.product_name as product_name, r.revenue as revenue
from products p inner join
(select oi.order_item_product_id, sum(cast(oi.order_item_subtotal as float)) as revenue
from order_items oi inner join orders o
on oi.order_item_order_id = o.order_id
where o.order_status <> 'CANCELED'
and o.order_status <> 'SUSPECTED_FRAUD'
group by order_item_product_id) r
on p.product_id = r.order_item_product_id
order by r.revenue desc
</code>


===== Common problems =====

==== 1. Transactional tables with Parquet format are not supported by Hive ====

In HDP 3, managed Hive tables must be transactional (hive.strict.managed.tables=true). Transactional tables with Parquet format are not supported by Hive. Hive imports with --as-parquetfile must use external tables by specifying --external-table-dir.

Associated error message

Table db.table failed strict managed table checks due to the
following reason: Table is marked as a managed table but is not
transactional. 
Workaround

When using --hive-import with --as-parquetfile, users must also provide --external-table-dir with a fully qualified location of the table:
<code>
sqoop import ... --hive-import
                 --as-parquetfile 
                 --external-table-dir hdfs:///path/to/table
</code>
