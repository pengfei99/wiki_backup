====== Building, Installing and configuring Apache Atlas ======

The Apache Atlas requires a database which supports graph storage, it supports
  * JanusGraph berkeleyje(default)
  * HBASE (preferred in big data)
  * cassandra (if you don't want to install hdfs)

It also requires a search engine, it supports
  * Apache Solr
  * Elastic search

Gremlin is JanusGraphâ€™s query language. For more info, please visit https://docs.janusgraph.org/basics/gremlin/

atlas uses Kafka to receives metadata generated by the various hooks.  

===== 1. Build atlas from source =====

You can get the atlas source from https://atlas.apache.org/#/Downloads

Based on your envrionment, you can build two types of atlas
  * dev instance (use embedded hbase-solr, or cassandra-solr)
  * prod instance (use external hbase, cassandra, solr, elasticsearch cluster)

To build atlas, you need following dependencies
  * maven 
  * wget
  * git
  * python2.7
  * java1.8
  * patch
  * unzip

In the following example, I use the 2.1.0 source to build
<code>
# get source
VERSION=2.1.0 
cd /tmp 
wget http://mirror.linux-ia64.org/apache/atlas/${VERSION}/apache-atlas-${VERSION}-sources.tar.gz
mkdir -p /opt/gremlin 
mkdir -p /tmp/atlas-src 

# extract the source
tar --strip 1 -xzvf apache-atlas-${VERSION}-sources.tar.gz -C /tmp/atlas-src 
rm apache-atlas-${VERSION}-sources.tar.gz

cd /tmp/atlas-src 
</code>

Set up maven build env
<code>
export MAVEN_OPTS="-Xms2g -Xmx2g"
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
</code>

==== 1.1 Build for dev with embedded hbase and solr ====
You can official doc here https://atlas.apache.org/#/BuildInstallation

<code>
mvn clean -Dmaven.repo.local=/tmp/.mvn-repo -Dhttps.protocols=TLSv1.2 -DskipTests package -Pdist,embedded-hbase-solr
</code>

==== 1.2  Build for dev with embedded hbase and solr ====

<code>
mvn clean -Dmaven.repo.local=/tmp/.mvn-repo -Dhttps.protocols=TLSv1.2 -DskipTests package -Pdist,embedded-cassandra-solr
</code>

==== 1.3 Build for prod  ====

<code>
mvn clean -DskipTests package -Pdist
</code>

Note:
  * Remove option '-DskipTests' to run unit and integration tests
  * To build a distribution without minified js,css file, build with skipMinify profile. By default js and css files are minified.

The above command will build Apache Atlas for an environment having functional HBase and Solr instances. Apache Atlas needs to be setup with the following to run in this environment:

  * Configure atlas.graph.storage.hostname (see "Graph persistence engine - HBase" in the Configuration section).
  * Configure atlas.graph.index.search.solr.zookeeper-url (see "Graph Search Index - Solr" in the Configuration section).
  * Set HBASE_CONF_DIR to point to a valid Apache HBase config directory (see "Graph persistence engine - HBase" in the Configuration section).
  * Create indices in Apache Solr (see "Graph Search Index - Solr" in the Configuration section).

==== 1.4 Run the atlas server ====
After the execution of the above build command, you can find all generated packages in 
<code>
cd /tmp/atlas-src/distro/target/
tar -xzvf apache-atlas-${VERSION}-server.tar.gz -C /opt

# clean the source file
rm -Rf /tmp/atlas-src 
rm -Rf /tmp/.mvn-repo 

# apply two patch files
cd /opt/apache-atlas-${VERSION}/bin 
patch -b -f < atlas_start.py.patch
patch -b -f < atlas_config.py.patch

# run the setup process
cd /opt/apache-atlas-${VERSION} 
./bin/atlas_start.py -setup || true

# start atlas, note the first start will generate error of zk and kafka, which is normal. Just wait all finish
# then stop atlas, restat it. The error message should go away. Need to find out why
cd /opt/apache-atlas-${VERSION} 
./bin/atlas_start.py 

# then visit atlas via localhost:21000 with admin:admin
</code>



<file .patch atlas_config.py.patch>
--- atlas_config.py.orig	2019-05-03 08:22:00.000000000 +0300
+++ atlas_config.py	2020-01-16 02:14:31.660601445 +0300
@@ -500,15 +500,18 @@
 
 def wait_for_startup(confdir, wait):
     count = 0
+    started = False
     host = get_atlas_url_host(confdir)
     port = get_atlas_url_port(confdir)
-    while True:
+    pid_file = pidFile(atlasDir())
+
+    while not started:
         try:
             s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
             s.settimeout(1)
             s.connect((host, int(port)))
             s.close()
-            break
+            started = True
         except Exception as e:
             # Wait for 1 sec before next ping
             sys.stdout.write('.')
@@ -516,11 +519,20 @@
             sleep(1)
 
         if count > wait:
-            s.close()
-            break
+	    s.close()
+            sys.stdout.write('\nAtlas Web-UI startup timed out! But, wait for it...')
+            sys.stdout.flush()
+	    break
+
+        if not os.path.exists(pid_file):
+            sys.stdout.write('\nApache Atlas startup failed!\nCheck logs: /opt/apache-atlas-2.1.0/logs/application.log')
+            sys.stdout.flush()
+	    exit()
+	    break
 
         count = count + 1
 
+
     sys.stdout.write('\n')
 
 def run_zookeeper(dir, action, logdir = None, wait=True):
@@ -555,14 +567,14 @@
 
     if zk_url is None:
         if port is None:
-            cmd = [os.path.join(dir, solrScript), action]
+            cmd = [os.path.join(dir, solrScript), action, '-force']
         else:
-            cmd = [os.path.join(dir, solrScript), action, '-p', str(port)]
+            cmd = [os.path.join(dir, solrScript), action, '-force', '-p', str(port)]
     else:
         if port is None:
-            cmd = [os.path.join(dir, solrScript), action, '-z', zk_url]
+            cmd = [os.path.join(dir, solrScript), action, '-force', '-z', zk_url]
         else:
-            cmd = [os.path.join(dir, solrScript), action, '-z', zk_url, '-p', port]
+            cmd = [os.path.join(dir, solrScript), action, '-force', '-z', zk_url, '-p', port]
 
     return runProcess(cmd, logdir, False, wait)
 
@@ -572,7 +584,7 @@
     if IS_WINDOWS:
         solrScript = "solr.cmd"
 
-    cmd = [os.path.join(dir, solrScript), 'create', '-c', index, '-d', confdir,  '-shards',  solrShards(),  '-replicationFactor', solrReplicationFactor()]
+    cmd = [os.path.join(dir, solrScript), 'create', '-c', index, '-d', confdir,  '-shards',  solrShards(),  '-replicationFactor', solrReplicationFactor(), '-force']
 
     return runProcess(cmd, logdir, False, wait)
 
@@ -691,3 +703,4 @@
     windowsPath = subprocess.Popen(cygpathArgs, stdout=subprocess.PIPE).communicate()[0]
     windowsPath = windowsPath.strip()
     return windowsPath
+

</file>

<file .patch atlas_start.py.patch>
--- atlas_start.py.orig	2019-05-03 08:22:00.000000000 +0300
+++ atlas_start.py	2020-01-16 01:37:16.147611498 +0300
@@ -18,6 +18,9 @@
 import os
 import sys
 import traceback
+import os.path
+import time
+from time import sleep
 
 import atlas_config as mc
 
@@ -114,6 +117,9 @@
         mc.configure_hbase(atlas_home)
         mc.run_hbase_action(mc.hbaseBinDir(atlas_home), "start", hbase_conf_dir, logdir)
         print "hbase started."
+        if is_setup:
+            print "Sleeping 60s due too setup (init run)..."
+            sleep(60)
 
     #solr setup
     if mc.is_solr_local(confdir):
@@ -128,6 +134,9 @@
 
         mc.run_solr(mc.solrBinDir(atlas_home), "start", mc.get_solr_zk_url(confdir), mc.solrPort(), logdir)
         print "solr started."
+        if is_setup:
+            print "Sleeping 60s due too setup (init run)..."
+            sleep(60)
 
         print "setting up solr collections..."
         mc.create_solr_collection(mc.solrBinDir(atlas_home), mc.solrConfDir(atlas_home), "vertex_index", logdir)
@@ -145,8 +154,27 @@
         web_app_path = mc.convertCygwinPath(web_app_path)
     if not is_setup:
         start_atlas_server(atlas_classpath, atlas_pid_file, jvm_logdir, jvm_opts_list, web_app_path)
-        mc.wait_for_startup(confdir, 300)
-        print "Apache Atlas Server started!!!\n"
+        mc.wait_for_startup(confdir, 600)
+        print "Apache Atlas Server process started!\n"
+
+        atlas_pid_file = mc.pidFile(atlas_home)
+        try:
+            pf = file(atlas_pid_file, 'r')
+            pid = int(pf.read().strip())
+            pf.close()
+            print("Running Apache Atlas with PID " + str(pid) + "...\n")
+        except:
+            pid = None
+        if not pid:
+            sys.stderr.write("No PID file found! Server is not running?\nCheck logs: /opt/apache-atlas-2.1.0/logs/application.log\n\n")
+            return
+
+
+        while os.path.exists(atlas_pid_file):
+            time.sleep(1)
+
+        print "Apache Atlas stopped!\n"
+
     else:
         process = mc.java("org.apache.atlas.web.setup.AtlasSetup", [], atlas_classpath, jvm_opts_list, jvm_logdir)
         return process.wait()

</file>

==== 1.5 Configuration ====

You can find the official config page here https://atlas.apache.org/#/Configuration

<code>
cd /opt/apache-atlas-${VERSION}/conf/atlas-application.properties 


#Add the following line to enable web gui to show import entity forms
#########  UI Configuration ########

atlas.ui.default.version=v1
atlas.ui.editable.entity.types=*

</code>

=== 1.5.1  Graph Configs ===

== Graph Persistence engine - HBase ==

Set the following properties to configure JanusGraph to use HBase as the persistence engine. 
<code>
atlas.graph.storage.backend=hbase
atlas.graph.storage.hostname=lin02.udl.org,lin03.udl.org,lin04.udl.org
atlas.graph.storage.hbase.table=atlas_janus

# for the audit data
atlas.audit.hbase.tablename=ATLAS_ENTITY_AUDIT_EVENTS
atlas.audit.hbase.zookeeper.quorum=lin02.udl.org,lin03.udl.org,lin04.udl.org
atlas.audit.zookeeper.session.timeout.ms=60000
</code>

== Graph Index Search Engine ==


An index search engine is required for ATLAS. This search engine runs separately from the ATLAS server and from the storage backend. Only two search engines are currently supported: Solr and Elasticsearch. 

**Graph Search Index - Solr**
Solr installation in Cloud mode is a prerequisite for Apache Atlas use.
<code>

atlas.graph.index.search.backend=solr5
atlas.graph.index.search.solr.mode=cloud
atlas.graph.index.search.solr.wait-searcher=true
# ZK quorum setup for solr as comma separated value. Example: 10.1.6.4:2181,10.1.6.5:2181
atlas.graph.index.search.solr.zookeeper-url=lin03.udl.org:2181/infra-solr,lin04.udl.org:2181/infra-solr,lin02.udl.org:2181/infra-solr
# SolrCloud Zookeeper Connection Timeout. Default value is 60000 ms
atlas.graph.index.search.solr.zookeeper-connect-timeout=60000
# SolrCloud Zookeeper Session Timeout. Default value is 60000 ms
atlas.graph.index.search.solr.zookeeper-session-timeout=60000

</code>

**Graph Search Index - Elasticsearch (Tech Preview)**

<code>
atlas.graph.index.search.backend=elasticsearch
atlas.graph.index.search.hostname=<hostname(s) of the Elasticsearch master nodes comma separated>
atlas.graph.index.search.elasticsearch.client-only=true
</code>

=== 1.5.2 Search Configs ===
Search APIs (DSL, basic search, full-text search) support pagination and have optional limit and offset arguments. Following configs are related to search pagination

<code>
# Default limit used when limit is not specified in API
atlas.search.defaultlimit=100
# Maximum limit allowed in API. Limits maximum results that can be fetched to make sure the atlas server doesn't run out of memory
atlas.search.maxlimit=10000
</code>

=== 1.5.3 Notification configs ===



<code>

#Setup the following configurations only in test deployments where Kafka is started within Atlas in embedded mode
#atlas.notification.embedded=true
#atlas.kafka.data={sys:atlas.home}/data/kafka
#Setup the following two properties if Kafka is running in Kerberized mode.
#atlas.notification.kafka.service.principal=kafka/_HOST@EXAMPLE.COM
#atlas.notification.kafka.keytab.location=/etc/security/keytabs/kafka.service.keytab


atlas.kafka.auto.commit.enable=false
#Kafka servers. Example: localhost:6667
atlas.kafka.bootstrap.servers=lin02.udl.org:6667
atlas.kafka.hook.group.id=atlas
#Zookeeper connect URL for Kafka. Example: localhost:2181
atlas.kafka.zookeeper.connect=lin02.udl.org:2181,lin03.udl.org:2181,lin04.udl.org:2181
atlas.kafka.zookeeper.connection.timeout.ms=30000
atlas.kafka.zookeeper.session.timeout.ms=60000
atlas.kafka.zookeeper.sync.time.ms=20

atlas.notification.create.topics=true
atlas.notification.embedded=false
atlas.notification.replicas=1
atlas.notification.topics=ATLAS_HOOK,ATLAS_ENTITIES
</code>
=== 1.5.4 other configs ===

<code>
atlas.lineage.schema.query.hive_table=hive_table where __guid='%s'\, columns
atlas.lineage.schema.query.Table=Table where __guid='%s'\, columns

atlas.proxyusers=knox
atlas.rest.address=http://lin03.udl.org:21000
atlas.server.address.id1=lin03.udl.org:21000
atlas.server.bind.address=0.0.0.0
atlas.server.ha.enabled=false
atlas.server.http.port=21000
atlas.server.https.port=21443
atlas.server.ids=id1
atlas.simple.authz.policy.file=/usr/hdp/current/atlas-server/conf/atlas-simple-authz-policy.json
atlas.solr.kerberos.enable=false
atlas.ssl.exclude.protocols=TLSv1.2

</code>
===== 2. Install atlas via ambari =====

If you are using ambari to install atlas, the default installation path is 

<code>
/usr/hdp/current/atlas-server
/usr/hdp/current/atlas-client
</code>


