====== HDFS and mapreduce basic cli ======


====== start the hdfs service ======

<code>
cd $HADOOP_HOME/sbin/
#start the hdfs
./start-dfs.sh
#start the yarn, which is essential for run mapreduce job
#yarn will decide which node will run map and reduce job
./start-yarn.sh


#check status after starting the service
jps
</code>


#check status via web interface
http://name-node-ip:8042/

http://name-node-ip:50070/dfshealth.html#tab-overview

http://name-node-ip:8088/cluster

====== Hdfs data management command ======


<code>

#Upload data to hdfs

hdfs dfs -mkdir /test_data

hdfs dfs -put test.txt /test_data

# test mapreduce 

cd /opt/hadoop/hadoop/share/hadoop/mapreduce

ls 

hadoop-mapreduce-client-app-2.8.1.jar              hadoop-mapreduce-client-shuffle-2.8.1.jar
hadoop-mapreduce-client-common-2.8.1.jar           hadoop-mapreduce-examples-2.8.1.jar
hadoop-mapreduce-client-core-2.8.1.jar             jdiff
hadoop-mapreduce-client-hs-2.8.1.jar               lib
hadoop-mapreduce-client-hs-plugins-2.8.1.jar       lib-examples
hadoop-mapreduce-client-jobclient-2.8.1.jar        sources
hadoop-mapreduce-client-jobclient-2.8.1-tests.jar


#use the hadoop-mapreduce-example-2.8.1.jar to calculate the pi
#$1 is the name of the method, $2 is the number of thread in hdfs $3 is the example we use to calculate the pi, more examples will make the pi more accurate, but it will long time.

hadoop jar hadoop-mapreduce-examples-2.8.1.jar pi 5 5 

#calculate the word number
#$1 is the method name, #2 is the input file dir, $3 is the output file dir
hadoop jar hadoop-mapreduce-examples-2.8.1.jar wordcount /test_data/ /test_data/output

hdfs -ls /test_data/output
Found 2 items
-rw-r--r--   1 hadoop supergroup          0 2017-10-03 12:29 /test_data/output/_SUCCESS
-rw-r--r--   1 hadoop supergroup         89 2017-10-03 12:29 /test_data/output/part-r-00000

#view the result
hadoop dfs -cat /test_data/output/part-r-00000

</code>

====== Hdfs root user and clean the trash ======

<code>
# When you remove a file by using hdfs dfs -rm, hadoop will put the removed file into hdfs trash
# To clean the trash you need to type the following command and have root privilege.

hdfs dfs -expunge

# The trash location is /user/hdfs/.Trash

# unlike linux file system, in hdfs, the root user is hdfs. So you can run the command by becoming hdfs
sudo -u hdfs hdfs dfs -ls /user/hive

hdfs dfs -rm -skipTrash /path/to/file/you/want/to/remove/permanently

</code>


