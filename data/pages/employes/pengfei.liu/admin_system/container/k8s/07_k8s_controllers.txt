====== 07: k8s controllers ======

In chapter [[employes:pengfei.liu:admin_system:container:k8s:06_deploy_k8s_objects|06: Deploy k8s objects]], we have seen how to deploy a pod manually. But most of the time, we will use a controller to deploy a pod for us. 


K8s has four types of controllers to manage pods:
  * ReplicationController, ReplicaSet, Deployment (For managing stateless pods)
  * StatefullSet (For managing stateful pods)
  * DaemonSet (For managing pods need to run on every single working node)
  * Job, CronJob


The most common controller to deploy pod is Deployment


===== 7.1 Deployment =====

Below is an example of a simple deployment

<file yaml ngix-deployment.yaml>
apiVersion: apps/v1
kind: Deployment
metadata:
  # define the name of this deployment
  name: nginx-deployment
  # define a label of this deployment, usefull to group a set of deployment
  labels: 
    app: pengfei-deployment-test
# spec of the deployment
spec:
  # defines which template this deployment uses, it must match the template(i.e. pod) label
  selector:
    matchLabels:
      app: pengfei-test
  # number of the replicas of the pod    
  replicas: 2
  # template(i.e. description) of the pod
  template:
    metadata:
      # label of the pod
      labels:
        app: pengfei-test
    spec:
      containers:
        # no 1 container of the pod
      - name: nginx-container
        image: nginx:1.7.9
        # set the limit of the pod resource, by default there is no limit
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
        imagePullPolicy: Always

</file>

To apply the above deployment

<code>
# the record option will save the command and the config details of the deployment
kubectl apply -f nginx-deployment.yaml --record

# To get all the registered history of a deployment
kubectl rollout history deployment.v1.apps/nginx-deployment

deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=nginx-deployment.yaml --record=true
2         kubectl apply --filename=nginx-deployment.yaml --record=true

# To list the details of a particular
kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
</code>

You can check the deployment you just created

<code>
# check all deployment
kubectl get deployment --show-labels

# check all pods
kubectl get pods --show-labels

# get details of a deployment
kubectl describe deployment.v1.apps/nginx-deployment 
</code>


Note Do not change the **Pod-template-hash label**

The **pod-template-hash** label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have.


==== 7.1.1 Change deployment config via command ====

You can modify the deployment.yaml file to change the config. 
<code>
# you can modify the config directly from the server side
$ kubectl edit deployment.v1.apps/nginx-deployment

# you can modify your local config and re-apply it. 
</code>

And you can also use kubectl directly. Check the below examples

<code>
# update the container image, here nginx-deployment is the name of deployment, nginx-container is the name of container, nginx:1.16.1 is the new image name
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.16.1 --record


# re-scale the deployment, note we did not set option record, so this command will not be registered 
$ kubectl scale deployment.v1.apps/nginx-deployment --replicas=6

deployment.apps/nginx-deployment scaled

# we can add an auto-scale to this deployment, note autoscale does not create a new RS.
$ kubectl autoscale deployment nginx-deployment --min=6 --max=10 --cpu-percent=80 --record

</code>


After the change, you can check out the rollout status of the deployment

<code>
# check out the rollout status of the deployment 
$ kubectl rollout status deployment/nginx-deployment

deployment "nginx-deployment" successfully rolled out

# You can noticed that for each modification, a new ReplicaSet is created
$ kubectl get rs --show-labels
NAME                                   DESIRED   CURRENT   READY   AGE     LABELS
nginx-deployment-59d95dccd7            0         0         0       52m     app=pengfei-test,pod-template-hash=59d95dccd7
nginx-deployment-5b45544d45            2         2         2       7m40s   app=pengfei-test,pod-template-hash=5b45544d45
nginx-deployment-5fc6c48565            0         0         0       35m     app=pengfei-test,pod-template-hash=5fc6c48565
</code>

==== 7.1.2 Undo changes ====

If you want to undo the changes, you can run this command

<code>
# this will rollback to previous config, and we can't record a undo
$ kubectl rollout undo deployment/nginx-deployment
</code>

You can set **.spec.revisionHistoryLimit** field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.

**Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be able to roll back.**


===== 7.2 Jobs =====

A Job creates one or more Pods and ensures that **a specified number of them successfully terminate**. As pods successfully completed, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.

A simple case is to create one Job object in order to reliably run one Pod to completion. The Job object will start a new Pod if the first Pod fails or is deleted (for example due to a node hardware failure or a node reboot).

You can also use a Job to run multiple Pods in parallel.

==== 7.2.1 A simple job ====

The following job create a pod with image perl and use perl to calculate the value of pi. Then Print the first 2000 number of pi value

<file yaml job.yaml>
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
  labels: 
   app: pengfei-job-test    
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  # A job's spec are immutable, we can't change them after job creation. We need to delete old job, and submit new job.
  # completions default value is 1, it means if 1 pod complete and return 0. The job is considered finished succesfully.
  completions: 4
  # parallelism default value is 1, it means 1 pod can run in parallel. 
  parallelism: 2
  backoffLimit: 4
</file>

<code>
# Run the job
kubectl apply -f job.yaml 

# get job list
kubectl get jobs --show-labels

# decribe the job
kubectl describe jobs/pi

# get all pods related to a job
pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}')
echo $pods

# get the log of a job
kubectl logs $pods
</code>
==== 7.2.2 Writing a Job spec ====


As with all other Kubernetes config, a Job needs 
  * apiVersion
  * kind
  * metadata
  * spec.template : is a **pod template**. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind. In addition to required fields for a Pod, a pod template in a Job must specify appropriate **labels** (see pod selector) and **an appropriate restart policy**.

Some rules on Job spec
  * **Its name must be a valid DNS subdomain name.**
  * **Pod RestartPolicy** can only equal to **Never** or **OnFailure**.


===== 7.3 CronJob =====

A CronJob creates Jobs on a repeating schedule.

One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format.