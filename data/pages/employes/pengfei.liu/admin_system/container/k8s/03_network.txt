====== 03: k8s networking model ======

The complete guide can be found below. But it's too complex to read

https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model

https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/


====== 3.1 Communication between containers in the same pod ======
**Containers of the same pod communicate by using lo(the loopback device)**. This is because all containers of the same pod share the same network namespace. Kubernetes create a default container called "pause" on every pod. This container’s number 1 job is to keep the namespace open in case all the other containers on the pod die. It means two containers can not use the same port number. For example, if we have two containers run a web app on 8080, a third container calls 127.0.0.1:8080 will generate conflict.

**So, each pod gets its own network namespace. Containers in the same pod are in the same network namespace.**

Containers of different pods can't communicate directly.

===== 3.1.1 Network namespace =====

A **Network namespace** a collection of network interfaces (connections between two pieces of equipment on a network) and routing tables (instructions for where to send network packets).

As a Linux user, network namespaces can be created using the ip command. For example, the following command will create a new network namespace called ns1.

<code>
$ ip netns add ns1
# When the namespace is created, a mount point for it is created under /var/run/netns, 
# allowing the namespace to persist even if there is no process attached to it.
</code>

You can list available namespaces by listing all the mount points under /var/run/netns, or by using the ip command.

<code>
$ ls /var/run/netns
ns1
$ ip netns
ns1
</code>


====== 3.2 Communication between pods on the same node ======
Each pod on a node has its own network namespace. Each pod has its own IP address.

And each pod thinks it has a totally normal ethernet device called eth0 to make network requests through. But Kubernetes is faking it – it’s just a virtual ethernet connection.

Each pod’s eth0 device is actually connected to a virtual ethernet device in the node. A virtual ethernet device is a tunnel that connects the pod’s network with the node. This connection has two sides – on the pod’s side, it’s named eth0, and on the node’s side, it’s named veth{X}. Why the X? There’s a vethX connection for every pod on the node. (So they’d be veth1, veth2, veth3, etc.)

{{:employes:pengfei.liu:admin_system:container:k8s:k8s_pods_on_same_node.gif?400|}}


When a pod makes a request to the IP address of another pod, it makes that request through its own eth0 interface. This tunnels to the node’s respective virtual veth{X} interface. Every pod veth{X} interface on a node is part of a network bridge. In Kubernetes, this bridge is called cbr0. 

===== 3.2.1 What is a Network Bridge? =====

A network bridge connects two networks together. When a request hits the bridge, the bridge asks all the connected devices (i.e. pods) if they have the right IP address to handle the original request.

(Remember that each pod has its own IP address and it knows its own IP address.)

If one of the devices does, the bridge will store this information and also forward data to the original back so that its network request is completed.

====== 3.3 Communication between pods on different nodes ======

If pods are on different nodes, the network bridge asks all the connected devices (i.e. pods) if they have the right IP address, none of them will say yes. After that, the bridge falls back to the default gateway. This goes up to the cluster level and looks for the IP address.

At the cluster level, there’s a table that maps IP address ranges to various nodes. Pods on those nodes will have been assigned IP addresses from those ranges.

For example, Kubernetes might give pods on node 1 addresses like 100.96.1.1, 100.96.1.2, etc. And Kubernetes gives pods on node 2 addresses like 100.96.2.1, 100.96.2.2, and so on.

Then this table will store the fact that IP addresses that look like 100.96.1.xxx should go to node 1, and addresses like 100.96.2.xxx need to go to node 2.

After we’ve figured out which node to send the request to, the process proceeds the roughly same as if the pods had been on the same node all along.

{{ :employes:pengfei.liu:admin_system:container:k8s:k8s_node_to_node.gif?400 |}}
====== 3.4 Pod to service communication ======

In Kubernetes, a **service** lets you map a single IP address to a set of pods. You make requests to one endpoint (domain name/IP address) and the service proxies requests to a pod in that service.

This happens via **kube-proxy a small process that Kubernetes runs inside every node**.

This process maps virtual IP addresses to a group of actual pod IP addresses.

Once kube-proxy has mapped the service virtual IP to an actual pod IP, the request proceeds as in the above sections.


====== 4. How does DNS work? How do we discover IP addresses? ======

DNS is the system for converting domain names to IP addresses.

Kubernetes clusters have a service responsible for DNS resolution.

Every service in a cluster is assigned a domain name like my-service.my-namespace.svc.cluster.local.

Pods are automatically given a DNS name, and can also specify their own using the hostname and subdomain properties in their YAML config.

So when a request is made to a service via its domain name, the DNS service resolves it to the IP address of the service.

Then kube-proxy converts that service's IP address into a pod IP address. After that, based on whether the pods are on the same node or on different nodes, the request follows one of the paths explained above.