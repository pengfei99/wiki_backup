a:2:{s:7:"current";a:9:{s:4:"date";a:2:{s:7:"created";i:1515064117;s:8:"modified";i:1515068590;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";a:8:{s:4:"date";i:1515068590;s:2:"ip";s:14:"134.158.37.239";s:4:"type";s:1:"E";s:2:"id";s:51:"employes:pengfei.liu:big_data:spark:spark_mapreduce";s:4:"user";s:4:"pliu";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:7;}s:5:"title";s:29:"Use spark to do mapreduce job";s:11:"description";a:2:{s:15:"tableofcontents";a:1:{i:0;a:4:{s:3:"hid";s:29:"use_spark_to_do_mapreduce_job";s:5:"title";s:29:"Use spark to do mapreduce job";s:4:"type";s:2:"ul";s:5:"level";i:1;}}s:8:"abstract";s:503:"Use spark to do mapreduce job


#read the file
scala> val textFile = sc.textFile("file:///tmp/word.txt")
textFile: org.apache.spark.rdd.RDD[String] = file:///tmp/word.txt MapPartitionsRDD[1] at textFile at <console>:24

#lazy transformation
scala> val wordCount = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:26

#action, active spark job
scala> wordCountâ€¦";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}s:8:"relation";a:1:{s:10:"firstimage";s:0:"";}s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}}s:10:"persistent";a:5:{s:4:"date";a:2:{s:7:"created";i:1515064117;s:8:"modified";i:1515068590;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";a:8:{s:4:"date";i:1515068590;s:2:"ip";s:14:"134.158.37.239";s:4:"type";s:1:"E";s:2:"id";s:51:"employes:pengfei.liu:big_data:spark:spark_mapreduce";s:4:"user";s:4:"pliu";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:7;}s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}}}