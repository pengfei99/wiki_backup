a:2:{s:7:"current";a:9:{s:4:"date";a:2:{s:7:"created";i:1516004983;s:8:"modified";i:1516008274;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";b:0;s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}s:5:"title";s:43:"Spark read and write parquet in file system";s:11:"description";a:2:{s:15:"tableofcontents";a:3:{i:0;a:4:{s:3:"hid";s:43:"spark_read_and_write_parquet_in_file_system";s:5:"title";s:43:"Spark read and write parquet in file system";s:4:"type";s:2:"ul";s:5:"level";i:1;}i:1;a:4:{s:3:"hid";s:26:"write_data_to_parquet_file";s:5:"title";s:26:"Write data to parquet file";s:4:"type";s:2:"ul";s:5:"level";i:2;}i:2;a:4:{s:3:"hid";s:27:"read_data_from_parquet_file";s:5:"title";s:27:"Read data from parquet file";s:4:"type";s:2:"ul";s:5:"level";i:2;}}s:8:"abstract";s:346:"Spark read and write parquet in file system

Apache Parquet is implemented using the record-shredding and assembly algorithm, which accommodates the complex data structures that can be used to store the data. The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:";}s:8:"relation";a:2:{s:10:"references";a:1:{s:72:"employes:pengfei.liu:big_data:spark:spark_usecase:sf_client_satisfaction";b:1;}s:10:"firstimage";s:0:"";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}}s:10:"persistent";a:5:{s:4:"date";a:2:{s:7:"created";i:1516004983;s:8:"modified";i:1516008274;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";b:0;s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}}}