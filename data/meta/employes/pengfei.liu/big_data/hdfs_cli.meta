a:2:{s:7:"current";a:9:{s:4:"date";a:2:{s:7:"created";i:1510215641;s:8:"modified";i:1548082573;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";a:8:{s:4:"date";i:1548082573;s:2:"ip";s:13:"46.218.31.186";s:4:"type";s:1:"E";s:2:"id";s:38:"employes:pengfei.liu:big_data:hdfs_cli";s:4:"user";s:4:"pliu";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:567;}s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}s:5:"title";s:28:"HDFS and mapreduce basic cli";s:11:"description";a:2:{s:15:"tableofcontents";a:4:{i:0;a:4:{s:3:"hid";s:28:"hdfs_and_mapreduce_basic_cli";s:5:"title";s:28:"HDFS and mapreduce basic cli";s:4:"type";s:2:"ul";s:5:"level";i:1;}i:1;a:4:{s:3:"hid";s:22:"start_the_hdfs_service";s:5:"title";s:22:"start the hdfs service";s:4:"type";s:2:"ul";s:5:"level";i:1;}i:2;a:4:{s:3:"hid";s:28:"hdfs_data_management_command";s:5:"title";s:28:"Hdfs data management command";s:4:"type";s:2:"ul";s:5:"level";i:1;}i:3;a:4:{s:3:"hid";s:34:"hdfs_root_user_and_clean_the_trash";s:5:"title";s:34:"Hdfs root user and clean the trash";s:4:"type";s:2:"ul";s:5:"level";i:1;}}s:8:"abstract";s:285:"HDFS and mapreduce basic cli

start the hdfs service


cd $HADOOP_HOME/sbin/
#start the hdfs
./start-dfs.sh
#start the yarn, which is essential for run mapreduce job
#yarn will decide which node will run map and reduce job
./start-yarn.sh


#check status after starting the service
jps";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}s:8:"relation";a:1:{s:10:"firstimage";s:0:"";}}s:10:"persistent";a:5:{s:4:"date";a:2:{s:7:"created";i:1510215641;s:8:"modified";i:1548082573;}s:7:"creator";s:11:"pengfei liu";s:4:"user";s:4:"pliu";s:11:"last_change";a:8:{s:4:"date";i:1548082573;s:2:"ip";s:13:"46.218.31.186";s:4:"type";s:1:"E";s:2:"id";s:38:"employes:pengfei.liu:big_data:hdfs_cli";s:4:"user";s:4:"pliu";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:567;}s:11:"contributor";a:1:{s:4:"pliu";s:11:"pengfei liu";}}}